<!DOCTYPE HTML>
<html lang="en" class="navy sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>DeltaForge CDC Framework - User Guide</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        <!-- SEO Meta Tags -->
        <meta name="author" content="Vahid Negahdari">
        <meta name="keywords" content="CDC, Change Data Capture, Rust, Kafka, Redis, MySQL, Postgres, streaming, data pipeline, real-time, Debezium alternative, ETL">
        <meta name="robots" content="index, follow">
        <link rel="canonical" href="https://vnvo.github.io/deltaforge/">
        
        <!-- Open Graph / Facebook -->
        <meta property="og:type" content="website">
        <meta property="og:url" content="https://vnvo.github.io/deltaforge/">
        <meta property="og:title" content="DeltaForge - CDC Framework">
        <meta property="og:description" content="A modular, config-driven Change Data Capture (CDC) micro-framework built in Rust. Stream database changes to Kafka and Redis.">
        <meta property="og:image" content="https://vnvo.github.io/deltaforge/assets/deltaforge-blc.png">
        <meta property="og:site_name" content="DeltaForge Documentation">
        
        <!-- Twitter Card -->
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:url" content="https://vnvo.github.io/deltaforge/">
        <meta name="twitter:title" content="DeltaForge - CDC Framework">
        <meta name="twitter:description" content="A modular, config-driven Change Data Capture (CDC) micro-framework built in Rust. Stream database changes to Kafka and Redis.">
        <meta name="twitter:image" content="https://vnvo.github.io/deltaforge/assets/deltaforge-blc.png">
        
        <!-- Structured Data (JSON-LD) -->
        <script type="application/ld+json">
        {
          "@context": "https://schema.org",
          "@type": "SoftwareSourceCode",
          "name": "DeltaForge",
          "description": "A modular, config-driven Change Data Capture (CDC) micro-framework built in Rust",
          "url": "https://github.com/vnvo/deltaforge",
          "codeRepository": "https://github.com/vnvo/deltaforge",
          "programmingLanguage": "Rust",
          "license": "https://opensource.org/licenses/MIT",
          "author": {
            "@type": "Person",
            "name": "Vahid Negahdari",
            "url": "https://github.com/vnvo"
          },
          "keywords": ["CDC", "Change Data Capture", "Rust", "Kafka", "Redis", "MySQL", "Postgres", "streaming"]
        }
        </script>
        <meta name="description" content="A modular, config-driven Change Data Capture (CDC) micro-framework built in Rust. Stream database changes to Kafka, Redis and etc.">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon-de23e50b.svg">
        <link rel="shortcut icon" href="favicon-8114d1fc.png">
        <link rel="stylesheet" href="css/variables-8adf115d.css">
        <link rel="stylesheet" href="css/general-2459343d.css">
        <link rel="stylesheet" href="css/chrome-ae938929.css">
        <link rel="stylesheet" href="css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="css/custom-55d982ab.css">


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "navy";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex-46e7f876.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc-d434cb00.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>â†</kbd> or <kbd>â†’</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('navy')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div class="sidebar-footer">
                <a href="https://github.com/vnvo/deltaforge/releases">
                    <img src="https://img.shields.io/github/v/release/vnvo/deltaforge?label=version" alt="Version">
                </a>
                <a href="https://github.com/vnvo/deltaforge/blob/main/LICENSE-MIT">
                    <img src="https://img.shields.io/badge/license-MIT%20OR%20Apache--2.0-blue" alt="License">
                </a>
                <img src="https://img.shields.io/badge/arch-amd64|arm64-blue" alt="Arch">
                <img src="https://img.shields.io/badge/rustc-1.89+-blue.svg" alt="MSRV">
            </div>            
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">DeltaForge CDC Framework - User Guide</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>
                        <a href="https://github.com/vnvo/deltaforge" title="Git repository" aria-label="Git repository">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <p align="center">
  <img src="assets/deltaforge-blc.png" width="450" alt="DeltaForge">
  <br><br>
  <a href="https://github.com/vnvo/deltaforge/releases">
    <img src="https://img.shields.io/github/v/release/vnvo/deltaforge?label=version" alt="Version">
  </a>
  <img src="https://img.shields.io/badge/arch-amd64|arm64-blue" alt="Arch">
  <img src="https://img.shields.io/badge/license-MIT%20OR%20Apache--2.0-blue" alt="License">
</p>

<h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>DeltaForge is a versatile, high-performance <a href="#change-data-capture-cdc">Change Data Capture</a> (CDC) engine built in Rust. It streams database changes into downstream systems like Kafka, Redis, and NATS - giving you full control over how events are routed, transformed, and delivered. Built-in schema discovery automatically infers and tracks the shape of your data as it flows through, including deep inspection of nested JSON structures.</p>
<p>Pipelines are defined declaratively in YAML, making it straightforward to onboard new use cases without custom code.</p>
<table>
  
<tr>
    <td align="center" width="140">
      <b>Built with</b>
    </td>
    <td align="center" width="140">
      <b>Sources</b>
    </td>
    <td align="center" width="140">
      <b>Processors</b>
    </td>
    <td align="center" width="140">
      <b>Sinks</b>
    </td>
    <td align="center" width="140">
      <b>Output Formats</b>
    </td>
  </tr>

  
<tr>
    <td align="center">
      <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/rust/rust-original.svg" width="40" height="40" alt="Rust">
      <br><sub>Rust</sub>
    </td>
    <td align="center">
      <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/mysql/mysql-original.svg" width="40" height="40" alt="MySQL">
      <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/postgresql/postgresql-original.svg" width="40" height="40" alt="PostgreSQL">
      <br><sub>MySQL Â· PostgreSQL</sub>
    </td>
    <td align="center">
      <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/javascript/javascript-original.svg" width="40" height="40" alt="JavaScript">
      <br><sub>JavaScript Â· Outbox</sub>
    </td>
    <td align="center">
      <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/apachekafka/apachekafka-original.svg" width="40" height="40" alt="Kafka">
      <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/redis/redis-original.svg" width="40" height="40" alt="Redis">
      <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/nats/nats-original.svg" width="40" height="40" alt="NATS">
      <br><sub>Kafka Â· Redis Â· NATS</sub>
    </td>
    <td align="center">
      <img src="https://img.shields.io/badge/Native-red?style=flat-square" alt="Native">
      <img src="https://img.shields.io/badge/Debezium-green?style=flat-square" alt="Debezium">
      <img src="https://img.shields.io/badge/CloudEvents-blue?style=flat-square" alt="CloudEvents">
    </td>
  </tr>

</table>

<h2 id="why-deltaforge"><a class="header" href="#why-deltaforge">Why DeltaForge?</a></h2>
<h3 id="core-capabilities"><a class="header" href="#core-capabilities">Core Capabilities</a></h3>
<ul>
<li>âš¡ <strong>Powered by Rust</strong> : Predictable performance, memory safety, and minimal resource footprint.</li>
<li>ğŸ”Œ <strong>Pluggable architecture</strong> : Sources, processors, and sinks are modular and independently extensible.</li>
<li>ğŸ§© <strong>Declarative pipelines</strong> : Define sources, transforms, sinks, and commit policies in version-controlled YAML with environment variable expansion for secrets.</li>
<li>ğŸ“¦ <strong>Reliable checkpointing</strong> : Resume safely after restarts with at-least-once delivery guarantees.</li>
<li>ğŸ”€ <strong>Dynamic routing</strong> : Route events to per-table topics, streams, or subjects using templates or JavaScript logic.</li>
<li>ğŸ“¤ <strong>Transactional outbox</strong> : Publish domain events atomically with database writes. Per-aggregate routing, raw payload delivery, zero polling.</li>
<li>ğŸ› ï¸ <strong>Cloud-native ready</strong> : Single binary, Docker images, JSON logs, Prometheus metrics, and liveness/readiness probes for Kubernetes.</li>
</ul>
<h3 id="schema-intelligence"><a class="header" href="#schema-intelligence">Schema Intelligence</a></h3>
<ul>
<li>ğŸ” <strong>Schema sensing</strong> : Automatically infer and track schema from event payloads, including deep inspection of nested JSON structures.</li>
<li>ğŸ—ºï¸ <strong>High-cardinality handling</strong> : Detect and normalize dynamic map keys (session IDs, trace IDs) to prevent false schema evolution events.</li>
<li>ğŸ·ï¸ <strong>Schema fingerprinting</strong> : SHA-256 based change detection with schema-to-checkpoint correlation for reliable replay.</li>
<li>ğŸ—ƒï¸ <strong>Source-owned semantics</strong> : Preserves native database types (PostgreSQL arrays, MySQL JSON, etc.) instead of normalizing to a universal type system.</li>
</ul>
<h3 id="operational-features"><a class="header" href="#operational-features">Operational Features</a></h3>
<ul>
<li>ğŸ”„ <strong>Graceful failover</strong> : Handles source failover with automatic schema revalidation - no manual intervention needed.</li>
<li>ğŸ§¬ <strong>Zero-downtime schema evolution</strong> : Detects DDL changes and reloads schemas automatically, no pipeline restart needed.</li>
<li>ğŸ¯ <strong>Flexible table selection</strong> : Wildcard patterns (<code>db.*</code>, <code>schema.prefix%</code>) for easy onboarding.</li>
<li>ğŸ“€ <strong>Transaction boundaries</strong> : Optionally keep source transactions intact across batches.</li>
<li>âš™ï¸ <strong>Commit policies</strong> : Control checkpoint behavior with <code>all</code>, <code>required</code>, or <code>quorum</code> modes across multiple sinks.</li>
<li>ğŸ”§ <strong>Live pipeline management</strong> : Pause, resume, patch, and inspect running pipelines via the REST API.</li>
</ul>
<h2 id="use-cases"><a class="header" href="#use-cases">Use Cases</a></h2>
<p>DeltaForge is designed for:</p>
<ul>
<li><strong>Real-time data synchronization</strong> : Keep caches, search indexes, and analytics systems in sync with your primary database.</li>
<li><strong>Event-driven architectures</strong> : Stream database changes to Kafka or NATS for downstream microservices.</li>
<li><strong>Transactional messaging</strong> : Use the <a href="#outbox-pattern">outbox pattern</a> to publish domain events atomically with database writes - no distributed transactions needed.</li>
<li><strong>Audit trails and compliance</strong> : Capture every mutation with full before/after images for SOC2, HIPAA, or GDPR requirements.</li>
<li><strong>Lightweight ETL</strong> : Transform, filter, and route data in-flight with JavaScript processors - no Spark or Flink cluster needed.</li>
</ul>
<blockquote>
<p><strong>DeltaForge is <em>not</em> a DAG-based stream processor.</strong> It is a focused CDC engine meant to replace tools like Debezium when you need a lighter, cloud-native, and more customizable runtime.</p>
</blockquote>
<h2 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Guide</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><a href="#quickstart">Quickstart</a></td><td>Get DeltaForge running in minutes</td></tr>
<tr><td><a href="#change-data-capture-cdc">CDC Overview</a></td><td>Understand Change Data Capture concepts</td></tr>
<tr><td><a href="#configuration">Configuration</a></td><td>Pipeline spec reference</td></tr>
<tr><td><a href="#development-guide">Development</a></td><td>Build from source, contribute</td></tr>
</tbody>
</table>
</div>
<h2 id="quick-example"><a class="header" href="#quick-example">Quick Example</a></h2>
<pre><code class="language-yaml">metadata:
  name: orders-to-kafka
  tenant: acme

spec:
  source:
    type: mysql
    config:
      dsn: ${MYSQL_DSN}
      tables: [shop.orders]

  processors:
    - type: javascript
      inline: |
        (event) =&gt; {
          event.processed_at = Date.now();
          return [event];
        }

  sinks:
    - type: kafka
      config:
        brokers: ${KAFKA_BROKERS}
        topic: order-events
</code></pre>
<h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<p><strong>Docker (recommended):</strong></p>
<pre><code class="language-bash">docker pull ghcr.io/vnvo/deltaforge:latest
</code></pre>
<p><strong>From source:</strong></p>
<pre><code class="language-bash">git clone https://github.com/vnvo/deltaforge.git
cd deltaforge
cargo build --release
</code></pre>
<p>See the <a href="#development-guide">Development Guide</a> for detailed build instructions and available Docker image variants.</p>
<h2 id="license"><a class="header" href="#license">License</a></h2>
<p>DeltaForge is dual-licensed under <a href="https://github.com/vnvo/deltaforge/blob/main/LICENSE-MIT">MIT</a> and <a href="https://github.com/vnvo/deltaforge/blob/main/LICENSE-APACHE">Apache 2.0</a>.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="change-data-capture-cdc"><a class="header" href="#change-data-capture-cdc">Change Data Capture (CDC)</a></h1>
<p>Change Data Capture (CDC) is the practice of streaming database mutations as ordered events so downstream systems can stay in sync without periodic full loads. Rather than asking â€œwhat does my data look like now?â€, CDC answers â€œwhat changed, and when?â€</p>
<p>DeltaForge is a CDC engine built to make log-based change streams reliable, observable, and operationally simple in modern, containerized environments. It focuses on <strong>row-level CDC</strong> from MySQL binlog and Postgres logical replication to keep consumers accurate and latency-aware while minimizing impact on source databases.</p>
<blockquote>
<p><strong>In short</strong>: DeltaForge tails committed transactions from MySQL and Postgres logs, preserves ordering and transaction boundaries, and delivers events to Kafka and Redis with checkpointed delivery, configurable batching, and Prometheus metrics - without requiring a JVM or distributed coordinator.</p>
</blockquote>
<hr>
<h2 id="why-cdc-matters"><a class="header" href="#why-cdc-matters">Why CDC matters</a></h2>
<p>Traditional data integration relies on periodic batch jobs that query source systems, compare snapshots, and push differences downstream. This approach worked for decades, but modern architectures demand something better.</p>
<p><strong>The batch ETL problem</strong>: A nightly sync means your analytics are always a day stale. An hourly sync still leaves gaps and hammers your production database with expensive <code>SELECT *</code> queries during business hours. As data volumes grow, these jobs take longer, fail more often, and compete for the same resources your customers need.</p>
<p>CDC flips the model. Instead of pulling data on a schedule, you subscribe to changes as they happen.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Aspect</th><th>Batch ETL</th><th>CDC</th></tr>
</thead>
<tbody>
<tr><td>Latency</td><td>Minutes to hours</td><td>Seconds to milliseconds</td></tr>
<tr><td>Source load</td><td>High (repeated scans)</td><td>Minimal (log tailing)</td></tr>
<tr><td>Data freshness</td><td>Stale between runs</td><td>Near real-time</td></tr>
<tr><td>Failure recovery</td><td>Re-run entire job</td><td>Resume from checkpoint</td></tr>
<tr><td>Change detection</td><td>Diff comparison</td><td>Native from source</td></tr>
</tbody>
</table>
</div>
<p>These benefits compound as systems scale and teams decentralize:</p>
<ul>
<li><strong>Deterministic replay</strong>: Ordered events allow consumers to reconstruct state or power exactly-once delivery with checkpointing.</li>
<li><strong>Polyglot delivery</strong>: The same change feed can serve caches, queues, warehouses, and search indexes simultaneously without additional source queries.</li>
</ul>
<hr>
<h2 id="how-cdc-works"><a class="header" href="#how-cdc-works">How CDC works</a></h2>
<p>All CDC implementations share a common goal: detect changes and emit them as events. The approaches differ in how they detect those changes.</p>
<h3 id="log-based-cdc"><a class="header" href="#log-based-cdc">Log-based CDC</a></h3>
<p>Databases maintain transaction logs (MySQL binlog, Postgres WAL) that record every committed change for durability and replication. Log-based CDC reads these logs directly, capturing changes without touching application tables.</p>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    commits    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    tails     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Database   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ CDC Engine  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚   + WAL     â”‚              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
                                                                  â–¼
                                                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                           â”‚   Kafka /   â”‚
                                                           â”‚   Redis     â”‚
                                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Zero impact on source table performance</li>
<li>Captures all changes including those from triggers and stored procedures</li>
<li>Preserves transaction boundaries and ordering</li>
<li>Can capture deletes without soft-delete columns</li>
</ul>
<p><strong>Trade-offs</strong>:</p>
<ul>
<li>Requires database configuration (replication slots, binlog retention)</li>
<li>Schema changes need careful handling</li>
<li>Log retention limits how far back you can replay</li>
</ul>
<p><strong>DeltaForge uses log-based CDC exclusively.</strong> This allows DeltaForge to provide stronger ordering guarantees, lower source impact, and simpler operational semantics than hybrid approaches that mix log tailing with polling or triggers.</p>
<h3 id="trigger-based-cdc"><a class="header" href="#trigger-based-cdc">Trigger-based CDC</a></h3>
<p>Database triggers fire on INSERT, UPDATE, and DELETE operations, writing change records to a shadow table that a separate process polls.</p>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Works on databases without accessible transaction logs</li>
<li>Can capture application-level context unavailable in logs</li>
</ul>
<p><strong>Trade-offs</strong>:</p>
<ul>
<li>Adds write overhead to every transaction</li>
<li>Triggers can be disabled or forgotten during schema migrations</li>
<li>Shadow tables require maintenance and can grow unbounded</li>
</ul>
<h3 id="polling-based-cdc"><a class="header" href="#polling-based-cdc">Polling-based CDC</a></h3>
<p>A process periodically queries tables for rows modified since the last check, typically using an <code>updated_at</code> timestamp or incrementing ID.</p>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Simple to implement</li>
<li>No special database configuration required</li>
</ul>
<p><strong>Trade-offs</strong>:</p>
<ul>
<li>Cannot reliably detect deletes</li>
<li>Requires <code>updated_at</code> columns on every table</li>
<li>Polling frequency trades off latency against database load</li>
<li>Clock skew and transaction visibility can cause missed or duplicate events</li>
</ul>
<hr>
<h2 id="anatomy-of-a-cdc-event"><a class="header" href="#anatomy-of-a-cdc-event">Anatomy of a CDC event</a></h2>
<p>A well-designed CDC event contains everything downstream consumers need to process the change correctly.</p>
<pre><code class="language-json">{
  "id": "evt_01J7K9X2M3N4P5Q6R7S8T9U0V1",
  "source": {
    "database": "shop",
    "table": "orders",
    "server_id": "mysql-prod-1"
  },
  "operation": "update",
  "timestamp": "2025-01-15T14:32:01.847Z",
  "transaction": {
    "id": "gtid:3E11FA47-71CA-11E1-9E33-C80AA9429562:42",
    "position": 15847293
  },
  "before": {
    "id": 12345,
    "status": "pending",
    "total": 99.99,
    "updated_at": "2025-01-15T14:30:00.000Z"
  },
  "after": {
    "id": 12345,
    "status": "shipped",
    "total": 99.99,
    "updated_at": "2025-01-15T14:32:01.000Z"
  },
  "schema_version": "v3"
}
</code></pre>
<p>Key components:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Purpose</th></tr>
</thead>
<tbody>
<tr><td><code>operation</code></td><td>INSERT, UPDATE, DELETE, or DDL</td></tr>
<tr><td><code>before</code> / <code>after</code></td><td>Row state before and after the change (enables diff logic)</td></tr>
<tr><td><code>transaction</code></td><td>Groups changes from the same database transaction</td></tr>
<tr><td><code>timestamp</code></td><td>When the change was committed at the source</td></tr>
<tr><td><code>schema_version</code></td><td>Helps consumers handle schema evolution</td></tr>
</tbody>
</table>
</div>
<p>Not all fields are present for every operation-<code>before</code> is omitted for INSERTs, <code>after</code> is omitted for DELETEs - but the event envelope and metadata fields are consistent across MySQL and Postgres sources.</p>
<hr>
<h2 id="real-world-use-cases"><a class="header" href="#real-world-use-cases">Real-world use cases</a></h2>
<h3 id="cache-invalidation-and-population"><a class="header" href="#cache-invalidation-and-population">Cache invalidation and population</a></h3>
<p><strong>Problem</strong>: Your Redis cache serves product catalog data, but cache invalidation logic is scattered across dozens of services. Some forget to invalidate, others invalidate too aggressively, and debugging staleness issues takes hours.</p>
<p><strong>CDC solution</strong>: Stream changes from the <code>products</code> table to a message queue. A dedicated consumer reads the stream and updates or deletes cache keys based on the operation type. This centralizes invalidation logic, provides replay capability for cache rebuilds, and removes cache concerns from application code entirely.</p>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     CDC      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   consumer   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MySQL   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Stream    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Cache Keys  â”‚
â”‚ products â”‚              â”‚  (Kafka/    â”‚              â”‚ product:123 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚   Redis)    â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h3 id="event-driven-microservices"><a class="header" href="#event-driven-microservices">Event-driven microservices</a></h3>
<p><strong>Problem</strong>: Your order service needs to notify inventory, shipping, billing, and analytics whenever an order changes state. Direct service-to-service calls create tight coupling and cascade failures.</p>
<p><strong>CDC solution</strong>: Publish order changes to a durable message queue. Each downstream service subscribes independently, processes at its own pace, and can replay events after failures or during onboarding. The order service doesnâ€™t need to know about its consumers.</p>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     CDC      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Orders    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Message   â”‚
â”‚   Database  â”‚              â”‚   Queue     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼               â–¼               â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚ Inventory â”‚   â”‚ Shipping  â”‚   â”‚ Analytics â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h3 id="search-index-synchronization"><a class="header" href="#search-index-synchronization">Search index synchronization</a></h3>
<p><strong>Problem</strong>: Your Elasticsearch index drifts from the source of truth. Full reindexing takes hours and blocks search updates during the process.</p>
<p><strong>CDC solution</strong>: Stream changes continuously to keep the index synchronized. Use the <code>before</code> image to remove old documents and the <code>after</code> image to index new content.</p>
<h3 id="data-warehouse-incremental-loads"><a class="header" href="#data-warehouse-incremental-loads">Data warehouse incremental loads</a></h3>
<p><strong>Problem</strong>: Nightly full loads into your warehouse take 6 hours and block analysts until noon. Business users complain that dashboards show yesterdayâ€™s numbers.</p>
<p><strong>CDC solution</strong>: Stream changes to a staging topic, then micro-batch into your warehouse every few minutes. Analysts get near real-time data without impacting source systems.</p>
<h3 id="audit-logging-and-compliance"><a class="header" href="#audit-logging-and-compliance">Audit logging and compliance</a></h3>
<p><strong>Problem</strong>: Regulations require you to maintain a complete history of changes to sensitive data. Application-level audit logging is inconsistent and can be bypassed.</p>
<p><strong>CDC solution</strong>: The transaction log captures every committed change regardless of how it was made - application code, admin scripts, or direct SQL. Stream these events to immutable storage for compliance.</p>
<h3 id="cross-region-replication"><a class="header" href="#cross-region-replication">Cross-region replication</a></h3>
<p><strong>Problem</strong>: You need to replicate data to a secondary region for disaster recovery, but built-in replication doesnâ€™t support the transformations you need.</p>
<p><strong>CDC solution</strong>: Stream changes through a CDC pipeline that filters, transforms, and routes events to the target regionâ€™s databases or message queues.</p>
<hr>
<h2 id="architecture-patterns"><a class="header" href="#architecture-patterns">Architecture patterns</a></h2>
<h3 id="the-outbox-pattern"><a class="header" href="#the-outbox-pattern">The outbox pattern</a></h3>
<p>When you need to update a database and publish an event atomically, the outbox pattern provides exactly-once semantics without distributed transactions.</p>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Single Transaction         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ UPDATE      â”‚    â”‚ INSERT INTO     â”‚ â”‚
â”‚  â”‚ orders SET  â”‚ +  â”‚ outbox (event)  â”‚ â”‚
â”‚  â”‚ status=...  â”‚    â”‚                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼ CDC
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚   Kafka     â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<ol>
<li>The application writes business data and an event record in the same transaction.</li>
<li>CDC tails the outbox table and publishes events to Kafka.</li>
<li>A cleanup process (or CDC itself) removes processed outbox rows.</li>
</ol>
<p>This guarantees that events are published if and only if the transaction commits.</p>
<p><strong>When to skip the outbox</strong>: If your only requirement is to react to committed database state (not application intent), direct CDC from business tables is often simpler than introducing an outbox. The outbox pattern adds value when you need custom event payloads, explicit event versioning, or when the event schema differs significantly from table structure.</p>
<h3 id="event-sourcing-integration"><a class="header" href="#event-sourcing-integration">Event sourcing integration</a></h3>
<p>CDC complements event sourcing by bridging legacy systems that werenâ€™t built event-first. Stream changes from existing tables into event stores, then gradually migrate to native event sourcing.</p>
<h3 id="cqrs-command-query-responsibility-segregation"><a class="header" href="#cqrs-command-query-responsibility-segregation">CQRS (Command Query Responsibility Segregation)</a></h3>
<p>CDC naturally supports CQRS by populating read-optimized projections from the write model. Changes flow through the CDC pipeline to update denormalized views, search indexes, or cache layers.</p>
<hr>
<h2 id="challenges-and-solutions"><a class="header" href="#challenges-and-solutions">Challenges and solutions</a></h2>
<h3 id="schema-evolution"><a class="header" href="#schema-evolution">Schema evolution</a></h3>
<p>Databases change. Columns get added, renamed, or removed. Types change. CDC pipelines need to handle this gracefully.</p>
<p><strong>Strategies</strong>:</p>
<ul>
<li><strong>Schema registry</strong>: Store and version schemas centrally (e.g., Confluent Schema Registry with Avro/Protobuf).</li>
<li><strong>Forward compatibility</strong>: Add columns as nullable; avoid removing columns that consumers depend on.</li>
<li><strong>Consumer tolerance</strong>: Design consumers to ignore unknown fields and handle missing optional fields.</li>
<li><strong>Processor transforms</strong>: Use DeltaForgeâ€™s JavaScript processors to normalize schemas before sinks.</li>
</ul>
<h3 id="ordering-guarantees"><a class="header" href="#ordering-guarantees">Ordering guarantees</a></h3>
<p>Events must arrive in the correct order for consumers to reconstruct state accurately. A DELETE arriving before its corresponding INSERT would be catastrophic.</p>
<p><strong>DeltaForge guarantees</strong>:</p>
<ul>
<li>Source-order preservation per table partition (always enabled)</li>
<li>Transaction boundary preservation when <code>respect_source_tx: true</code> is configured in batch settings</li>
</ul>
<p>Kafka sink uses consistent partitioning by primary key to maintain ordering within a partition at the consumer.</p>
<h3 id="exactly-once-delivery"><a class="header" href="#exactly-once-delivery">Exactly-once delivery</a></h3>
<p>Network failures, process crashes, and consumer restarts can cause duplicates or gaps. True exactly-once semantics require coordination between source, pipeline, and sink.</p>
<p><strong>DeltaForge approach</strong>:</p>
<ul>
<li>Checkpoints track the last committed position in the source log.</li>
<li>Configurable commit policies (<code>all</code>, <code>required</code>, <code>quorum</code>) control when checkpoints advance.</li>
<li>Kafka sink supports idempotent producers; transactional writes available via <code>exactly_once: true</code>.</li>
</ul>
<p><strong>Default behavior</strong>: DeltaForge provides at-least-once delivery out of the box. Exactly-once semantics require sink support and explicit configuration.</p>
<h3 id="high-availability"><a class="header" href="#high-availability">High availability</a></h3>
<p>Production CDC pipelines need to handle failures without data loss or extended downtime.</p>
<p><strong>Best practices</strong>:</p>
<ul>
<li>Run multiple pipeline instances with leader election.</li>
<li>Store checkpoints in durable storage (DeltaForge persists to local files, mountable volumes in containers).</li>
<li>Monitor lag between source position and checkpoint position.</li>
<li>Set up alerts for pipeline failures and excessive lag.</li>
</ul>
<p><strong>Expectations</strong>: DeltaForge checkpoints ensure no data loss on restart, but does not currently include built-in leader election. For HA deployments, use external coordination (Kubernetes leader election, etcd locks) or run active-passive with health-check-based failover.</p>
<h3 id="backpressure"><a class="header" href="#backpressure">Backpressure</a></h3>
<p>When sinks canâ€™t keep up with the change rate, pipelines need to slow down gracefully rather than dropping events or exhausting memory.</p>
<p><strong>DeltaForge handles backpressure through</strong>:</p>
<ul>
<li>Configurable batch sizes (<code>max_events</code>, <code>max_bytes</code>, <code>max_ms</code>).</li>
<li>In-flight limits (<code>max_inflight</code>) that bound concurrent sink writes.</li>
<li>Blocking reads from source when batches queue up.</li>
</ul>
<hr>
<h2 id="performance-considerations"><a class="header" href="#performance-considerations">Performance considerations</a></h2>
<h3 id="batching-trade-offs"><a class="header" href="#batching-trade-offs">Batching trade-offs</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Setting</th><th>Low value</th><th>High value</th></tr>
</thead>
<tbody>
<tr><td><code>max_events</code></td><td>Lower latency, more overhead</td><td>Higher throughput, more latency</td></tr>
<tr><td><code>max_ms</code></td><td>Faster flush, smaller batches</td><td>Larger batches, delayed flush</td></tr>
<tr><td><code>max_bytes</code></td><td>Memory-safe, frequent commits</td><td>Efficient for large rows</td></tr>
</tbody>
</table>
</div>
<p>Start with DeltaForge defaults and tune based on observed latency and throughput.</p>
<h3 id="source-database-impact"><a class="header" href="#source-database-impact">Source database impact</a></h3>
<p>Log-based CDC has minimal impact, but consider:</p>
<ul>
<li><strong>Replication slot retention</strong>: Paused pipelines cause WAL/binlog accumulation.</li>
<li><strong>Connection limits</strong>: Each pipeline holds a replication connection.</li>
<li><strong>Network bandwidth</strong>: High-volume tables generate significant log traffic.</li>
</ul>
<h3 id="sink-throughput"><a class="header" href="#sink-throughput">Sink throughput</a></h3>
<ul>
<li><strong>Kafka</strong>: Tune <code>batch.size</code>, <code>linger.ms</code>, and compression in <code>client_conf</code>.</li>
<li><strong>Redis</strong>: Use pipelining and connection pooling for high-volume streams.</li>
</ul>
<hr>
<h2 id="monitoring-and-observability"><a class="header" href="#monitoring-and-observability">Monitoring and observability</a></h2>
<p>CDC pipelines are long-running, stateful processes; without metrics and alerts, failures are silent by default. A healthy pipeline requires visibility into lag, throughput, and errors.</p>
<h3 id="key-metrics-to-track"><a class="header" href="#key-metrics-to-track">Key metrics to track</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Metric</th><th>Description</th><th>Alert threshold</th></tr>
</thead>
<tbody>
<tr><td><code>cdc_lag_seconds</code></td><td>Time between event timestamp and processing</td><td>&gt; 60s</td></tr>
<tr><td><code>events_processed_total</code></td><td>Throughput counter</td><td>Sudden drops</td></tr>
<tr><td><code>checkpoint_lag_events</code></td><td>Events since last checkpoint</td><td>&gt; 10,000</td></tr>
<tr><td><code>sink_errors_total</code></td><td>Failed sink writes</td><td>Any sustained errors</td></tr>
<tr><td><code>batch_size_avg</code></td><td>Events per batch</td><td>Outside expected range</td></tr>
</tbody>
</table>
</div>
<p>DeltaForge exposes Prometheus metrics on the configurable metrics endpoint (default <code>:9000</code>).</p>
<h3 id="health-checks"><a class="header" href="#health-checks">Health checks</a></h3>
<ul>
<li><code>GET /healthz</code>: Liveness probe - is the process running?</li>
<li><code>GET /readyz</code>: Readiness probe - are pipelines connected and processing?</li>
<li><code>GET /pipelines</code>: Detailed status of each pipeline including configuration.</li>
</ul>
<hr>
<h2 id="choosing-a-cdc-solution"><a class="header" href="#choosing-a-cdc-solution">Choosing a CDC solution</a></h2>
<p>When evaluating CDC tools, consider:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Factor</th><th>Questions to ask</th></tr>
</thead>
<tbody>
<tr><td><strong>Source support</strong></td><td>Does it support your databases? MySQL binlog? Postgres logical replication?</td></tr>
<tr><td><strong>Sink flexibility</strong></td><td>Can it write to your target systems? Kafka, Redis, HTTP, custom?</td></tr>
<tr><td><strong>Transformation</strong></td><td>Can you filter, enrich, or reshape events in-flight?</td></tr>
<tr><td><strong>Operational overhead</strong></td><td>How much infrastructure does it require? JVM? Distributed coordinator?</td></tr>
<tr><td><strong>Resource efficiency</strong></td><td>Whatâ€™s the memory/CPU footprint per pipeline?</td></tr>
<tr><td><strong>Cloud-native</strong></td><td>Does it containerize cleanly? Support health checks? Emit metrics?</td></tr>
</tbody>
</table>
</div>
<h3 id="where-deltaforge-fits"><a class="header" href="#where-deltaforge-fits">Where DeltaForge fits</a></h3>
<p>DeltaForge intentionally avoids the operational complexity of JVM-based CDC stacks (Kafka Connect-style deployments with Zookeeper, Connect workers, and converter configurations) while remaining compatible with Kafka-centric architectures.</p>
<p><strong>DeltaForge is designed for teams that want</strong>:</p>
<ul>
<li><strong>Lightweight runtime</strong>: Single binary, minimal memory footprint, no JVM warmup.</li>
<li><strong>Config-driven pipelines</strong>: YAML specs instead of code for common patterns.</li>
<li><strong>Inline transformation</strong>: JavaScript processors for custom logic without recompilation.</li>
<li><strong>Container-native operations</strong>: Built for Kubernetes with health endpoints and Prometheus metrics.</li>
</ul>
<p><strong>DeltaForge is not designed for</strong>:</p>
<ul>
<li>Complex DAG-based stream processing with windowed aggregations</li>
<li>Stateful joins across multiple streams</li>
<li>Sources beyond MySQL and Postgres (currently)</li>
<li>Built-in schema registry integration (use external registries)</li>
</ul>
<p>If you need those capabilities, consider dedicated stream processors or the broader Kafka ecosystem. DeltaForge excels at getting data out of databases and into your event infrastructure reliably and efficiently.</p>
<hr>
<h2 id="getting-started-1"><a class="header" href="#getting-started-1">Getting started</a></h2>
<p>Ready to try CDC with DeltaForge? Head to the <a href="#quickstart">Quickstart guide</a> to run your first pipeline in minutes.</p>
<p>For production deployments, review the <a href="#development-guide">Development guide</a> for container builds and operational best practices.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="quickstart"><a class="header" href="#quickstart">Quickstart</a></h1>
<p>Get DeltaForge running in minutes.</p>
<h2 id="1-prepare-a-pipeline-spec"><a class="header" href="#1-prepare-a-pipeline-spec">1. Prepare a pipeline spec</a></h2>
<p>Create a YAML file that defines your CDC pipeline. Environment variables are expanded at runtime, so secrets stay out of version control.</p>
<pre><code class="language-yaml">metadata:
  name: orders-mysql-to-kafka
  tenant: acme

spec:
  source:
    type: mysql
    config:
      id: orders-mysql
      dsn: ${MYSQL_DSN}
      tables:
        - shop.orders

  processors:
    - type: javascript
      id: transform
      inline: |
        (event) =&gt; {
          event.tags = ["processed"];
          return [event];
        }

  sinks:
    - type: kafka
      config:
        id: orders-kafka
        brokers: ${KAFKA_BROKERS}
        topic: orders
        required: true

  batch:
    max_events: 500
    max_bytes: 1048576
    max_ms: 1000
</code></pre>
<h2 id="2-start-deltaforge"><a class="header" href="#2-start-deltaforge">2. Start DeltaForge</a></h2>
<p><strong>Using Docker (recommended):</strong></p>
<pre><code class="language-bash">docker run --rm \
  -p 8080:8080 -p 9000:9000 \
  -e MYSQL_DSN="mysql://user:pass@host:3306/db" \
  -e KAFKA_BROKERS="kafka:9092" \
  -v $(pwd)/pipeline.yaml:/etc/deltaforge/pipeline.yaml:ro \
  -v deltaforge-checkpoints:/app/data \
  ghcr.io/vnvo/deltaforge:latest \
  --config /etc/deltaforge/pipeline.yaml
</code></pre>
<p><strong>From source:</strong></p>
<pre><code class="language-bash">cargo run -p runner -- --config ./pipeline.yaml
</code></pre>
<h3 id="runner-options"><a class="header" href="#runner-options">Runner options</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Flag</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>--config</code></td><td>(required)</td><td>Path to pipeline spec file or directory</td></tr>
<tr><td><code>--api-addr</code></td><td><code>0.0.0.0:8080</code></td><td>REST API address</td></tr>
<tr><td><code>--metrics-addr</code></td><td><code>0.0.0.0:9095</code></td><td>Prometheus metrics address</td></tr>
</tbody>
</table>
</div>
<h2 id="3-verify-its-running"><a class="header" href="#3-verify-its-running">3. Verify itâ€™s running</a></h2>
<p>Check health and pipeline status:</p>
<pre><code class="language-bash"># Liveness probe
curl http://localhost:8080/healthz

# Readiness with pipeline status
curl http://localhost:8080/readyz

# List all pipelines
curl http://localhost:8080/pipelines
</code></pre>
<h2 id="4-manage-pipelines"><a class="header" href="#4-manage-pipelines">4. Manage pipelines</a></h2>
<p>Control pipelines via the REST API:</p>
<pre><code class="language-bash"># Pause a pipeline
curl -X POST http://localhost:8080/pipelines/orders-mysql-to-kafka/pause

# Resume a pipeline
curl -X POST http://localhost:8080/pipelines/orders-mysql-to-kafka/resume

# Stop a pipeline
curl -X POST http://localhost:8080/pipelines/orders-mysql-to-kafka/stop
</code></pre>
<h2 id="next-steps"><a class="header" href="#next-steps">Next steps</a></h2>
<ul>
<li><a href="#change-data-capture-cdc">CDC Overview</a> : Understand how Change Data Capture works</li>
<li><a href="#configuration">Configuration</a> : Full pipeline spec reference</li>
<li><a href="sources/README.html">Sources</a> : MySQL and Postgres setup</li>
<li><a href="sinks/README.html">Sinks</a> : Kafka, Redis, and NATS configuration</li>
<li><a href="#dynamic-routing">Dynamic Routing</a> : Route events to per-table destinations</li>
<li><a href="#envelopes-and-encodings">Envelopes</a> : Native, Debezium, and CloudEvents output formats</li>
<li><a href="#development-guide">Development</a> : Build from source, run locally</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="configuration"><a class="header" href="#configuration">Configuration</a></h1>
<p>Pipelines are defined as YAML documents that map directly to the <code>PipelineSpec</code> type. Environment variables are expanded before parsing using <code>${VAR}</code> syntax, so secrets and connection strings can be injected at runtime. Unknown variables (e.g. <code>${source.table}</code>) pass through for use as <a href="#dynamic-routing">routing templates</a>.</p>
<h2 id="document-structure"><a class="header" href="#document-structure">Document structure</a></h2>
<pre><code class="language-yaml">apiVersion: deltaforge/v1
kind: Pipeline
metadata:
  name: &lt;pipeline-name&gt;
  tenant: &lt;tenant-id&gt;
spec:
  source: { ... }
  processors: [ ... ]
  sinks: [ ... ]
  batch: { ... }
  commit_policy: { ... }
  schema_sensing: { ... }
</code></pre>
<h2 id="metadata"><a class="header" href="#metadata">Metadata</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Required</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>name</code></td><td>string</td><td>Yes</td><td>Unique pipeline identifier. Used in API routes and metrics.</td></tr>
<tr><td><code>tenant</code></td><td>string</td><td>Yes</td><td>Business-oriented tenant label for multi-tenancy.</td></tr>
</tbody>
</table>
</div>
<h2 id="spec-fields"><a class="header" href="#spec-fields">Spec fields</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Required</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>source</code></td><td>object</td><td>Yes</td><td>Database source configuration. See <a href="#sources">Sources</a>.</td></tr>
<tr><td><code>processors</code></td><td>array</td><td>No</td><td>Ordered list of processors. See <a href="#processors">Processors</a>.</td></tr>
<tr><td><code>sinks</code></td><td>array</td><td>Yes (at least one)</td><td>One or more sinks that receive each batch. See <a href="#sinks">Sinks</a>.</td></tr>
<tr><td><code>sharding</code></td><td>object</td><td>No</td><td>Optional hint for downstream distribution.</td></tr>
<tr><td><code>connection_policy</code></td><td>object</td><td>No</td><td>How the runtime establishes upstream connections.</td></tr>
<tr><td><code>batch</code></td><td>object</td><td>No</td><td>Commit unit thresholds. See <a href="#batching">Batching</a>.</td></tr>
<tr><td><code>commit_policy</code></td><td>object</td><td>No</td><td>How sink acknowledgements gate checkpoints. See <a href="#commit-policy">Commit policy</a>.</td></tr>
<tr><td><code>schema_sensing</code></td><td>object</td><td>No</td><td>Automatic schema inference from event payloads. See <a href="#schema-sensing">Schema sensing</a>.</td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="sources"><a class="header" href="#sources">Sources</a></h2>
<h3 id="mysql"><a class="header" href="#mysql"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/mysql/mysql-original.svg" width="24" height="24" style="vertical-align: middle;"> MySQL</a></h3>
<p>Captures row-level changes via binlog replication. See <a href="#mysql-source">MySQL source documentation</a> for prerequisites and detailed configuration.</p>
<table>
<tr>
<td>
<pre><code class="language-yaml">source:
  type: mysql
  config:
    id: orders-mysql
    dsn: ${MYSQL_DSN}
    tables:
      - shop.orders
      - shop.order_items
</code></pre>
</td>
<td>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>id</code></td><td>string</td><td>Unique identifier for checkpoints and metrics</td></tr>
<tr><td><code>dsn</code></td><td>string</td><td>MySQL connection string with replication privileges</td></tr>
<tr><td><code>tables</code></td><td>array</td><td>Table patterns to capture; omit for all tables</td></tr>
</tbody>
</table>
</div>
</td>
</tr>

</table>

<p><strong>Table patterns</strong> support SQL LIKE syntax:</p>
<ul>
<li><code>db.table</code> - exact match</li>
<li><code>db.prefix%</code> - tables matching prefix</li>
<li><code>db.%</code> - all tables in database</li>
</ul>
<h3 id="postgresql"><a class="header" href="#postgresql"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/postgresql/postgresql-original.svg" width="24" height="24" style="vertical-align: middle;"> PostgreSQL</a></h3>
<p>Captures row-level changes via logical replication. See <a href="#postgresql-source">PostgreSQL source documentation</a> for prerequisites and detailed configuration.</p>
<table>
<tr>
<td>
<pre><code class="language-yaml">source:
  type: postgres
  config:
    id: users-postgres
    dsn: ${POSTGRES_DSN}
    slot: deltaforge_users
    publication: users_pub
    tables:
      - public.users
      - public.sessions
    start_position: earliest
</code></pre>
</td>
<td>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>id</code></td><td>string</td><td>Unique identifier</td></tr>
<tr><td><code>dsn</code></td><td>string</td><td>PostgreSQL connection string</td></tr>
<tr><td><code>slot</code></td><td>string</td><td>Replication slot name</td></tr>
<tr><td><code>publication</code></td><td>string</td><td>Publication name</td></tr>
<tr><td><code>tables</code></td><td>array</td><td>Table patterns to capture</td></tr>
<tr><td><code>start_position</code></td><td>string</td><td><code>earliest</code>, <code>latest</code>, or <code>lsn</code></td></tr>
</tbody>
</table>
</div>
</td>
</tr>

</table>

<hr>
<h2 id="processors"><a class="header" href="#processors">Processors</a></h2>
<p>Processors transform events between source and sinks. They run in order and can filter, enrich, or modify events.</p>
<h3 id="javascript"><a class="header" href="#javascript">JavaScript</a></h3>
<table>
<tr>
<td>
<pre><code class="language-yaml">processors:
  - type: javascript
    id: transform
    inline: |
      function processBatch(events) {
        return events.map(e =&gt; {
          e.tags = ["processed"];
          return e;
        });
      }
    limits:
      cpu_ms: 50
      mem_mb: 128
      timeout_ms: 500
</code></pre>
</td>
<td>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>id</code></td><td>string</td><td>Processor identifier</td></tr>
<tr><td><code>inline</code></td><td>string</td><td>JavaScript code</td></tr>
<tr><td><code>limits.cpu_ms</code></td><td>int</td><td>CPU time limit</td></tr>
<tr><td><code>limits.mem_mb</code></td><td>int</td><td>Memory limit</td></tr>
<tr><td><code>limits.timeout_ms</code></td><td>int</td><td>Execution timeout</td></tr>
</tbody>
</table>
</div>
</td>
</tr>

</table>

<hr>
<h3 id="flatten"><a class="header" href="#flatten">Flatten</a></h3>
<pre><code class="language-yaml">processors:
  - type: flatten
    id: flat
    separator: "__"
    max_depth: 3
    on_collision: last
    empty_object: preserve
    lists: preserve
    empty_list: drop
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>id</code></td><td>string</td><td><code>"flatten"</code></td><td>Processor identifier</td></tr>
<tr><td><code>separator</code></td><td>string</td><td><code>"__"</code></td><td>Separator between path segments</td></tr>
<tr><td><code>max_depth</code></td><td>int</td><td>unlimited</td><td>Stop recursing at this depth; objects at the boundary kept as-is</td></tr>
<tr><td><code>on_collision</code></td><td>string</td><td><code>last</code></td><td>Key collision policy: <code>last</code>, <code>first</code>, or <code>error</code></td></tr>
<tr><td><code>empty_object</code></td><td>string</td><td><code>preserve</code></td><td>Empty object policy: <code>preserve</code>, <code>drop</code>, or <code>null</code></td></tr>
<tr><td><code>lists</code></td><td>string</td><td><code>preserve</code></td><td>Array policy: <code>preserve</code> or <code>index</code></td></tr>
<tr><td><code>empty_list</code></td><td>string</td><td><code>preserve</code></td><td>Empty array policy: <code>preserve</code>, <code>drop</code>, or <code>null</code></td></tr>
</tbody>
</table>
</div>
<hr>
<h2 id="sinks"><a class="header" href="#sinks">Sinks</a></h2>
<p>Sinks deliver events to downstream systems. Each sink supports configurable <a href="#envelopes-and-encodings">envelope formats and wire encodings</a> to match consumer expectations. See the <a href="sinks/README.html">Sinks documentation</a> for detailed information on multi-sink patterns, commit policies, and failure handling.</p>
<h3 id="envelope-and-encoding"><a class="header" href="#envelope-and-encoding">Envelope and encoding</a></h3>
<p>All sinks support these serialization options:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>envelope</code></td><td>object</td><td><code>native</code></td><td>Output structure format. See <a href="#envelopes-and-encodings">Envelopes</a>.</td></tr>
<tr><td><code>encoding</code></td><td>string</td><td><code>json</code></td><td>Wire encoding format</td></tr>
</tbody>
</table>
</div>
<p><strong>Envelope types:</strong></p>
<ul>
<li><code>native</code> - Direct Debezium payload structure (default, most efficient)</li>
<li><code>debezium</code> - Full <code>{"payload": ...}</code> wrapper</li>
<li><code>cloudevents</code> - CloudEvents 1.0 specification (requires <code>type_prefix</code>)</li>
</ul>
<pre><code class="language-yaml"># Native (default)
envelope:
  type: native

# Debezium wrapper
envelope:
  type: debezium

# CloudEvents
envelope:
  type: cloudevents
  type_prefix: "com.example.cdc"
</code></pre>
<h3 id="kafka"><a class="header" href="#kafka"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/apachekafka/apachekafka-original.svg" width="24" height="24" style="vertical-align: middle;"> Kafka</a></h3>
<p>See <a href="#kafka-sink">Kafka sink documentation</a> for detailed configuration options and best practices.</p>
<table>
<tr>
<td>
<pre><code class="language-yaml">sinks:
  - type: kafka
    config:
      id: orders-kafka
      brokers: ${KAFKA_BROKERS}
      topic: orders
      envelope:
        type: debezium
      encoding: json
      required: true
      exactly_once: false
      send_timeout_secs: 30
      client_conf:
        security.protocol: SASL_SSL
</code></pre>
</td>
<td>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>id</code></td><td>string</td><td>-</td><td>Sink identifier</td></tr>
<tr><td><code>brokers</code></td><td>string</td><td>-</td><td>Kafka broker addresses</td></tr>
<tr><td><code>topic</code></td><td>string</td><td>-</td><td>Target topic or <a href="#dynamic-routing">template</a></td></tr>
<tr><td><code>key</code></td><td>string</td><td>-</td><td>Message key <a href="#dynamic-routing">template</a></td></tr>
<tr><td><code>envelope</code></td><td>object</td><td><code>native</code></td><td>Output format</td></tr>
<tr><td><code>encoding</code></td><td>string</td><td><code>json</code></td><td>Wire encoding</td></tr>
<tr><td><code>required</code></td><td>bool</td><td><code>true</code></td><td>Gates checkpoints</td></tr>
<tr><td><code>exactly_once</code></td><td>bool</td><td><code>false</code></td><td>Transactional mode</td></tr>
<tr><td><code>send_timeout_secs</code></td><td>int</td><td><code>30</code></td><td>Send timeout</td></tr>
<tr><td><code>client_conf</code></td><td>map</td><td>-</td><td>librdkafka overrides</td></tr>
</tbody>
</table>
</div>
</td>
</tr>

</table>

<p><strong>CloudEvents example:</strong></p>
<pre><code class="language-yaml">sinks:
  - type: kafka
    config:
      id: events-kafka
      brokers: ${KAFKA_BROKERS}
      topic: events
      envelope:
        type: cloudevents
        type_prefix: "com.acme.cdc"
      encoding: json
</code></pre>
<h3 id="redis"><a class="header" href="#redis"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/redis/redis-original.svg" width="24" height="24" style="vertical-align: middle;"> Redis</a></h3>
<p>See <a href="#redis-sink">Redis sink documentation</a> for detailed configuration options and best practices.</p>
<table>
<tr>
<td>
<pre><code class="language-yaml">sinks:
  - type: redis
    config:
      id: orders-redis
      uri: ${REDIS_URI}
      stream: orders
      envelope:
        type: native
      encoding: json
      required: true
</code></pre>
</td>
<td>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>id</code></td><td>string</td><td>-</td><td>Sink identifier</td></tr>
<tr><td><code>uri</code></td><td>string</td><td>-</td><td>Redis connection URI</td></tr>
<tr><td><code>stream</code></td><td>string</td><td>-</td><td>Redis stream key or <a href="#dynamic-routing">template</a></td></tr>
<tr><td><code>key</code></td><td>string</td><td>-</td><td>Entry key <a href="#dynamic-routing">template</a></td></tr>
<tr><td><code>envelope</code></td><td>object</td><td><code>native</code></td><td>Output format</td></tr>
<tr><td><code>encoding</code></td><td>string</td><td><code>json</code></td><td>Wire encoding</td></tr>
<tr><td><code>required</code></td><td>bool</td><td><code>true</code></td><td>Gates checkpoints</td></tr>
<tr><td><code>send_timeout_secs</code></td><td>int</td><td><code>5</code></td><td>XADD timeout</td></tr>
<tr><td><code>batch_timeout_secs</code></td><td>int</td><td><code>30</code></td><td>Pipeline timeout</td></tr>
<tr><td><code>connect_timeout_secs</code></td><td>int</td><td><code>10</code></td><td>Connection timeout</td></tr>
</tbody>
</table>
</div>
</td>
</tr>

</table>

<h3 id="nats"><a class="header" href="#nats"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/nats/nats-original.svg" width="24" height="24" style="vertical-align: middle;"> NATS</a></h3>
<p>See <a href="#nats-sink">NATS sink documentation</a> for detailed configuration options and best practices.</p>
<table>
<tr>
<td>
<pre><code class="language-yaml">sinks:
  - type: nats
    config:
      id: orders-nats
      url: ${NATS_URL}
      subject: orders.events
      stream: ORDERS
      envelope:
        type: native
      encoding: json
      required: true
      send_timeout_secs: 5
      batch_timeout_secs: 30
</code></pre>
</td>
<td>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>id</code></td><td>string</td><td>-</td><td>Sink identifier</td></tr>
<tr><td><code>url</code></td><td>string</td><td>-</td><td>NATS server URL</td></tr>
<tr><td><code>subject</code></td><td>string</td><td>-</td><td>Subject or <a href="#dynamic-routing">template</a></td></tr>
<tr><td><code>key</code></td><td>string</td><td>-</td><td>Message key <a href="#dynamic-routing">template</a></td></tr>
<tr><td><code>stream</code></td><td>string</td><td>-</td><td>JetStream stream name</td></tr>
<tr><td><code>envelope</code></td><td>object</td><td><code>native</code></td><td>Output format</td></tr>
<tr><td><code>encoding</code></td><td>string</td><td><code>json</code></td><td>Wire encoding</td></tr>
<tr><td><code>required</code></td><td>bool</td><td><code>true</code></td><td>Gates checkpoints</td></tr>
<tr><td><code>send_timeout_secs</code></td><td>int</td><td><code>5</code></td><td>Publish timeout</td></tr>
<tr><td><code>batch_timeout_secs</code></td><td>int</td><td><code>30</code></td><td>Batch timeout</td></tr>
<tr><td><code>connect_timeout_secs</code></td><td>int</td><td><code>10</code></td><td>Connection timeout</td></tr>
<tr><td><code>credentials_file</code></td><td>string</td><td>-</td><td>NATS credentials file</td></tr>
<tr><td><code>username</code></td><td>string</td><td>-</td><td>Auth username</td></tr>
<tr><td><code>password</code></td><td>string</td><td>-</td><td>Auth password</td></tr>
<tr><td><code>token</code></td><td>string</td><td>-</td><td>Auth token</td></tr>
</tbody>
</table>
</div>
</td>
</tr>

</table>

<hr>
<h2 id="batching"><a class="header" href="#batching">Batching</a></h2>
<table>
<tr>
<td>
<pre><code class="language-yaml">batch:
  max_events: 500
  max_bytes: 1048576
  max_ms: 1000
  respect_source_tx: true
  max_inflight: 2
</code></pre>
</td>
<td>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>max_events</code></td><td>int</td><td><code>500</code></td><td>Flush after this many events</td></tr>
<tr><td><code>max_bytes</code></td><td>int</td><td><code>1048576</code></td><td>Flush after size reaches limit</td></tr>
<tr><td><code>max_ms</code></td><td>int</td><td><code>1000</code></td><td>Flush after time (ms)</td></tr>
<tr><td><code>respect_source_tx</code></td><td>bool</td><td><code>true</code></td><td>Never split source transactions</td></tr>
<tr><td><code>max_inflight</code></td><td>int</td><td><code>2</code></td><td>Max concurrent batches</td></tr>
</tbody>
</table>
</div>
</td>
</tr>

</table>

<hr>
<h2 id="commit-policy"><a class="header" href="#commit-policy">Commit policy</a></h2>
<table>
<tr>
<td>
<pre><code class="language-yaml">commit_policy:
  mode: required

# For quorum mode:
commit_policy:
  mode: quorum
  quorum: 2
</code></pre>
</td>
<td>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Mode</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>all</code></td><td>Every sink must acknowledge before checkpoint</td></tr>
<tr><td><code>required</code></td><td>Only <code>required: true</code> sinks must acknowledge (default)</td></tr>
<tr><td><code>quorum</code></td><td>Checkpoint after <code>quorum</code> sinks acknowledge</td></tr>
</tbody>
</table>
</div>
</td>
</tr>

</table>

<hr>
<h2 id="schema-sensing"><a class="header" href="#schema-sensing">Schema sensing</a></h2>
<p>Schema sensing automatically infers and tracks schema from event payloads. See the <a href="#schema-sensing-2">Schema Sensing documentation</a> for detailed information on how it works, drift detection, and API endpoints.</p>
<blockquote>
<p><strong>Performance tip</strong>: Schema sensing can be CPU-intensive, especially with deep JSON inspection. Consider your throughput requirements when configuring:</p>
<ul>
<li>Set <code>enabled: false</code> if you donâ€™t need runtime schema inference</li>
<li>Limit <code>deep_inspect.max_depth</code> to avoid traversing deeply nested structures</li>
<li>Increase <code>sampling.sample_rate</code> to analyze fewer events (e.g., 1 in 100 instead of 1 in 10)</li>
<li>Reduce <code>sampling.warmup_events</code> if youâ€™re confident in schema stability</li>
</ul>
</blockquote>
<table>
<tr>
<td>
<pre><code class="language-yaml">schema_sensing:
  enabled: true
  deep_inspect:
    enabled: true
    max_depth: 3
    max_sample_size: 500
  sampling:
    warmup_events: 50
    sample_rate: 5
    structure_cache: true
    structure_cache_size: 50
  high_cardinality:
    enabled: true
    min_events: 100
    stable_threshold: 0.5
    min_dynamic_fields: 5
</code></pre>
</td>
<td>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>enabled</code></td><td>bool</td><td><code>false</code></td><td>Enable schema sensing</td></tr>
<tr><td><code>deep_inspect.enabled</code></td><td>bool</td><td><code>true</code></td><td>Inspect nested JSON</td></tr>
<tr><td><code>deep_inspect.max_depth</code></td><td>int</td><td><code>10</code></td><td>Max nesting depth</td></tr>
<tr><td><code>deep_inspect.max_sample_size</code></td><td>int</td><td><code>1000</code></td><td>Max events for deep analysis</td></tr>
<tr><td><code>sampling.warmup_events</code></td><td>int</td><td><code>1000</code></td><td>Events to fully analyze first</td></tr>
<tr><td><code>sampling.sample_rate</code></td><td>int</td><td><code>10</code></td><td>After warmup, analyze 1 in N</td></tr>
<tr><td><code>sampling.structure_cache</code></td><td>bool</td><td><code>true</code></td><td>Cache structure fingerprints</td></tr>
<tr><td><code>sampling.structure_cache_size</code></td><td>int</td><td><code>100</code></td><td>Max cached structures</td></tr>
<tr><td><code>high_cardinality.enabled</code></td><td>bool</td><td><code>true</code></td><td>Detect dynamic map keys</td></tr>
<tr><td><code>high_cardinality.min_events</code></td><td>int</td><td><code>100</code></td><td>Events before classification</td></tr>
<tr><td><code>high_cardinality.stable_threshold</code></td><td>float</td><td><code>0.5</code></td><td>Frequency for stable fields</td></tr>
<tr><td><code>high_cardinality.min_dynamic_fields</code></td><td>int</td><td><code>5</code></td><td>Min unique fields for map</td></tr>
</tbody>
</table>
</div>
</td>
</tr>

</table>

<hr>
<h2 id="complete-examples"><a class="header" href="#complete-examples">Complete examples</a></h2>
<h3 id="mysql-to-kafka-with-debezium-envelope"><a class="header" href="#mysql-to-kafka-with-debezium-envelope">MySQL to Kafka with Debezium envelope</a></h3>
<pre><code class="language-yaml">apiVersion: deltaforge/v1
kind: Pipeline
metadata:
  name: orders-mysql-to-kafka
  tenant: acme

spec:
  source:
    type: mysql
    config:
      id: orders-mysql
      dsn: ${MYSQL_DSN}
      tables:
        - shop.orders

  processors:
    - type: javascript
      id: transform
      inline: |
        function processBatch(events) {
          return events.map(event =&gt; {
            event.tags = (event.tags || []).concat(["normalized"]);
            return event;
          });
        }
      limits:
        cpu_ms: 50
        mem_mb: 128
        timeout_ms: 500

  sinks:
    - type: kafka
      config:
        id: orders-kafka
        brokers: ${KAFKA_BROKERS}
        topic: orders
        envelope:
          type: debezium
        encoding: json
        required: true
        exactly_once: false
        client_conf:
          message.timeout.ms: "5000"

  batch:
    max_events: 500
    max_bytes: 1048576
    max_ms: 1000
    respect_source_tx: true
    max_inflight: 2

  commit_policy:
    mode: required
</code></pre>
<h3 id="postgresql-to-kafka-with-cloudevents"><a class="header" href="#postgresql-to-kafka-with-cloudevents">PostgreSQL to Kafka with CloudEvents</a></h3>
<pre><code class="language-yaml">apiVersion: deltaforge/v1
kind: Pipeline
metadata:
  name: users-postgres-to-kafka
  tenant: acme

spec:
  source:
    type: postgres
    config:
      id: users-postgres
      dsn: ${POSTGRES_DSN}
      slot: deltaforge_users
      publication: users_pub
      tables:
        - public.users
        - public.user_sessions
      start_position: earliest

  sinks:
    - type: kafka
      config:
        id: users-kafka
        brokers: ${KAFKA_BROKERS}
        topic: user-events
        envelope:
          type: cloudevents
          type_prefix: "com.acme.users"
        encoding: json
        required: true

  batch:
    max_events: 500
    max_ms: 1000
    respect_source_tx: true

  commit_policy:
    mode: required
</code></pre>
<h3 id="multi-sink-with-different-formats"><a class="header" href="#multi-sink-with-different-formats">Multi-sink with different formats</a></h3>
<pre><code class="language-yaml">apiVersion: deltaforge/v1
kind: Pipeline
metadata:
  name: orders-multi-sink
  tenant: acme

spec:
  source:
    type: mysql
    config:
      id: orders-mysql
      dsn: ${MYSQL_DSN}
      tables:
        - shop.orders

  sinks:
    # Kafka Connect expects Debezium format
    - type: kafka
      config:
        id: connect-sink
        brokers: ${KAFKA_BROKERS}
        topic: connect-events
        envelope:
          type: debezium
        required: true

    # Lambda expects CloudEvents
    - type: kafka
      config:
        id: lambda-sink
        brokers: ${KAFKA_BROKERS}
        topic: lambda-events
        envelope:
          type: cloudevents
          type_prefix: "com.acme.cdc"
        required: false

    # Redis cache uses native format
    - type: redis
      config:
        id: cache-redis
        uri: ${REDIS_URI}
        stream: orders-cache
        envelope:
          type: native
        required: false

  batch:
    max_events: 500
    max_ms: 1000
    respect_source_tx: true

  commit_policy:
    mode: required
</code></pre>
<h3 id="mysql-to-nats"><a class="header" href="#mysql-to-nats">MySQL to NATS</a></h3>
<pre><code class="language-yaml">apiVersion: deltaforge/v1
kind: Pipeline
metadata:
  name: orders-mysql-to-nats
  tenant: acme

spec:
  source:
    type: mysql
    config:
      id: orders-mysql
      dsn: ${MYSQL_DSN}
      tables:
        - shop.orders
        - shop.order_items

  sinks:
    - type: nats
      config:
        id: orders-nats
        url: ${NATS_URL}
        subject: orders.events
        stream: ORDERS
        envelope:
          type: native
        encoding: json
        required: true

  batch:
    max_events: 500
    max_ms: 1000
    respect_source_tx: true

  commit_policy:
    mode: required
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="rest-api-reference"><a class="header" href="#rest-api-reference">REST API Reference</a></h1>
<p>DeltaForge exposes a REST API for health checks, pipeline management, schema
inspection, and drift detection. All endpoints return JSON.</p>
<h2 id="base-url"><a class="header" href="#base-url">Base URL</a></h2>
<p>Default: <code>http://localhost:8080</code></p>
<p>Configure with <code>--api-addr</code>:</p>
<pre><code class="language-bash">deltaforge --config pipelines.yaml --api-addr 0.0.0.0:9090
</code></pre>
<hr>
<h2 id="health-endpoints"><a class="header" href="#health-endpoints">Health Endpoints</a></h2>
<h3 id="liveness-probe"><a class="header" href="#liveness-probe">Liveness Probe</a></h3>
<pre><code class="language-http">GET /healthz
</code></pre>
<p>Returns <code>ok</code> if the process is running. Use for Kubernetes liveness probes.</p>
<p><strong>Response:</strong> <code>200 OK</code></p>
<pre><code>ok
</code></pre>
<h3 id="readiness-probe"><a class="header" href="#readiness-probe">Readiness Probe</a></h3>
<pre><code class="language-http">GET /readyz
</code></pre>
<p>Returns pipeline states. Use for Kubernetes readiness probes.</p>
<p><strong>Response:</strong> <code>200 OK</code></p>
<pre><code class="language-json">{
  "status": "ready",
  "pipelines": [
    {
      "name": "orders-cdc",
      "status": "running",
      "spec": { ... }
    }
  ]
}
</code></pre>
<hr>
<h2 id="pipeline-management"><a class="header" href="#pipeline-management">Pipeline Management</a></h2>
<h3 id="list-pipelines"><a class="header" href="#list-pipelines">List Pipelines</a></h3>
<pre><code class="language-http">GET /pipelines
</code></pre>
<p>Returns all pipelines with current status.</p>
<p><strong>Response:</strong> <code>200 OK</code></p>
<pre><code class="language-json">[
  {
    "name": "orders-cdc",
    "status": "running",
    "spec": {
      "metadata": { "name": "orders-cdc", "tenant": "acme" },
      "spec": { ... }
    }
  }
]
</code></pre>
<h3 id="get-pipeline"><a class="header" href="#get-pipeline">Get Pipeline</a></h3>
<pre><code class="language-http">GET /pipelines/{name}
</code></pre>
<p>Returns a single pipeline by name.</p>
<p><strong>Response:</strong> <code>200 OK</code></p>
<pre><code class="language-json">{
  "name": "orders-cdc",
  "status": "running",
  "spec": { ... }
}
</code></pre>
<p><strong>Errors:</strong></p>
<ul>
<li><code>404 Not Found</code> - Pipeline doesnâ€™t exist</li>
</ul>
<h3 id="create-pipeline"><a class="header" href="#create-pipeline">Create Pipeline</a></h3>
<pre><code class="language-http">POST /pipelines
Content-Type: application/json
</code></pre>
<p>Creates a new pipeline from a full spec.</p>
<p><strong>Request:</strong></p>
<pre><code class="language-json">{
  "metadata": {
    "name": "orders-cdc",
    "tenant": "acme"
  },
  "spec": {
    "source": {
      "type": "mysql",
      "config": {
        "id": "mysql-1",
        "dsn": "mysql://user:pass@host/db",
        "tables": ["shop.orders"]
      }
    },
    "processors": [],
    "sinks": [
      {
        "type": "kafka",
        "config": {
          "id": "kafka-1",
          "brokers": "localhost:9092",
          "topic": "orders"
        }
      }
    ]
  }
}
</code></pre>
<p><strong>Response:</strong> <code>200 OK</code></p>
<pre><code class="language-json">{
  "name": "orders-cdc",
  "status": "running",
  "spec": { ... }
}
</code></pre>
<p><strong>Errors:</strong></p>
<ul>
<li><code>409 Conflict</code> - Pipeline already exists</li>
</ul>
<h3 id="update-pipeline"><a class="header" href="#update-pipeline">Update Pipeline</a></h3>
<pre><code class="language-http">PATCH /pipelines/{name}
Content-Type: application/json
</code></pre>
<p>Applies a partial update to an existing pipeline. The pipeline is stopped,
updated, and restarted.</p>
<p><strong>Request:</strong></p>
<pre><code class="language-json">{
  "spec": {
    "batch": {
      "max_events": 1000,
      "max_ms": 500
    }
  }
}
</code></pre>
<p><strong>Response:</strong> <code>200 OK</code></p>
<pre><code class="language-json">{
  "name": "orders-cdc",
  "status": "running",
  "spec": { ... }
}
</code></pre>
<p><strong>Errors:</strong></p>
<ul>
<li><code>404 Not Found</code> - Pipeline doesnâ€™t exist</li>
<li><code>400 Bad Request</code> - Name mismatch in patch</li>
</ul>
<h3 id="delete-pipeline"><a class="header" href="#delete-pipeline">Delete Pipeline</a></h3>
<pre><code class="language-http">DELETE /pipelines/{name}
</code></pre>
<p>Permanently deletes a pipeline. This removes the pipeline from the runtime
and cannot be undone.</p>
<p><strong>Response:</strong> <code>204 No Content</code></p>
<p><strong>Errors:</strong></p>
<ul>
<li><code>404 Not Found</code> - Pipeline doesnâ€™t exist</li>
</ul>
<h3 id="pause-pipeline"><a class="header" href="#pause-pipeline">Pause Pipeline</a></h3>
<pre><code class="language-http">POST /pipelines/{name}/pause
</code></pre>
<p>Pauses ingestion. Events in the buffer are not processed until resumed.</p>
<p><strong>Response:</strong> <code>200 OK</code></p>
<pre><code class="language-json">{
  "name": "orders-cdc",
  "status": "paused",
  "spec": { ... }
}
</code></pre>
<h3 id="resume-pipeline"><a class="header" href="#resume-pipeline">Resume Pipeline</a></h3>
<pre><code class="language-http">POST /pipelines/{name}/resume
</code></pre>
<p>Resumes a paused pipeline.</p>
<p><strong>Response:</strong> <code>200 OK</code></p>
<pre><code class="language-json">{
  "name": "orders-cdc",
  "status": "running",
  "spec": { ... }
}
</code></pre>
<h3 id="stop-pipeline"><a class="header" href="#stop-pipeline">Stop Pipeline</a></h3>
<pre><code class="language-http">POST /pipelines/{name}/stop
</code></pre>
<p>Stops a pipeline. Final checkpoint is saved.</p>
<p><strong>Response:</strong> <code>200 OK</code></p>
<pre><code class="language-json">{
  "name": "orders-cdc",
  "status": "stopped",
  "spec": { ... }
}
</code></pre>
<hr>
<h2 id="schema-management"><a class="header" href="#schema-management">Schema Management</a></h2>
<h3 id="list-database-schemas"><a class="header" href="#list-database-schemas">List Database Schemas</a></h3>
<pre><code class="language-http">GET /pipelines/{name}/schemas
</code></pre>
<p>Returns all tracked database schemas for a pipeline. These are the schemas
loaded directly from the source database.</p>
<p><strong>Response:</strong> <code>200 OK</code></p>
<pre><code class="language-json">[
  {
    "database": "shop",
    "table": "orders",
    "column_count": 5,
    "primary_key": ["id"],
    "fingerprint": "sha256:a1b2c3d4e5f6...",
    "registry_version": 2
  },
  {
    "database": "shop",
    "table": "customers",
    "column_count": 8,
    "primary_key": ["id"],
    "fingerprint": "sha256:f6e5d4c3b2a1...",
    "registry_version": 1
  }
]
</code></pre>
<h3 id="get-schema-details"><a class="header" href="#get-schema-details">Get Schema Details</a></h3>
<pre><code class="language-http">GET /pipelines/{name}/schemas/{db}/{table}
</code></pre>
<p>Returns detailed schema information including all columns.</p>
<p><strong>Response:</strong> <code>200 OK</code></p>
<pre><code class="language-json">{
  "database": "shop",
  "table": "orders",
  "columns": [
    {
      "name": "id",
      "type": "bigint(20) unsigned",
      "nullable": false,
      "default": null,
      "extra": "auto_increment"
    },
    {
      "name": "customer_id",
      "type": "bigint(20)",
      "nullable": false,
      "default": null
    }
  ],
  "primary_key": ["id"],
  "fingerprint": "sha256:a1b2c3d4..."
}
</code></pre>
<hr>
<h2 id="schema-sensing-1"><a class="header" href="#schema-sensing-1">Schema Sensing</a></h2>
<p>Schema sensing automatically infers schema structure from JSON event payloads.
This is useful for sources that donâ€™t provide schema metadata or for detecting
schema evolution in JSON columns.</p>
<h3 id="list-inferred-schemas"><a class="header" href="#list-inferred-schemas">List Inferred Schemas</a></h3>
<pre><code class="language-http">GET /pipelines/{name}/sensing/schemas
</code></pre>
<p>Returns all schemas inferred via sensing for a pipeline.</p>
<p><strong>Response:</strong> <code>200 OK</code></p>
<pre><code class="language-json">[
  {
    "table": "orders",
    "fingerprint": "sha256:abc123...",
    "sequence": 3,
    "event_count": 1500,
    "stabilized": true,
    "first_seen": "2025-01-15T10:30:00Z",
    "last_seen": "2025-01-15T14:22:00Z"
  }
]
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>table</code></td><td>Table name (or <code>table:column</code> for JSON column sensing)</td></tr>
<tr><td><code>fingerprint</code></td><td>SHA-256 content hash of current schema</td></tr>
<tr><td><code>sequence</code></td><td>Monotonic version number (increments on evolution)</td></tr>
<tr><td><code>event_count</code></td><td>Total events observed</td></tr>
<tr><td><code>stabilized</code></td><td>Whether schema has stopped sampling (structure stable)</td></tr>
<tr><td><code>first_seen</code></td><td>First observation timestamp</td></tr>
<tr><td><code>last_seen</code></td><td>Most recent observation timestamp</td></tr>
</tbody>
</table>
</div>
<h3 id="get-inferred-schema-details"><a class="header" href="#get-inferred-schema-details">Get Inferred Schema Details</a></h3>
<pre><code class="language-http">GET /pipelines/{name}/sensing/schemas/{table}
</code></pre>
<p>Returns detailed inferred schema including all fields.</p>
<p><strong>Response:</strong> <code>200 OK</code></p>
<pre><code class="language-json">{
  "table": "orders",
  "fingerprint": "sha256:abc123...",
  "sequence": 3,
  "event_count": 1500,
  "stabilized": true,
  "fields": [
    {
      "name": "id",
      "types": ["integer"],
      "nullable": false,
      "optional": false
    },
    {
      "name": "metadata",
      "types": ["object"],
      "nullable": true,
      "optional": false,
      "nested_field_count": 5
    },
    {
      "name": "tags",
      "types": ["array"],
      "nullable": false,
      "optional": true,
      "array_element_types": ["string"]
    }
  ],
  "first_seen": "2025-01-15T10:30:00Z",
  "last_seen": "2025-01-15T14:22:00Z"
}
</code></pre>
<h3 id="export-json-schema"><a class="header" href="#export-json-schema">Export JSON Schema</a></h3>
<pre><code class="language-http">GET /pipelines/{name}/sensing/schemas/{table}/json-schema
</code></pre>
<p>Exports the inferred schema as a standard JSON Schema document.</p>
<p><strong>Response:</strong> <code>200 OK</code></p>
<pre><code class="language-json">{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "orders",
  "type": "object",
  "properties": {
    "id": { "type": "integer" },
    "metadata": { "type": ["object", "null"] },
    "tags": {
      "type": "array",
      "items": { "type": "string" }
    }
  },
  "required": ["id", "metadata"]
}
</code></pre>
<h3 id="get-sensing-cache-statistics"><a class="header" href="#get-sensing-cache-statistics">Get Sensing Cache Statistics</a></h3>
<pre><code class="language-http">GET /pipelines/{name}/sensing/stats
</code></pre>
<p>Returns cache performance statistics for schema sensing.</p>
<p><strong>Response:</strong> <code>200 OK</code></p>
<pre><code class="language-json">{
  "tables": [
    {
      "table": "orders",
      "cached_structures": 3,
      "max_cache_size": 100,
      "cache_hits": 1450,
      "cache_misses": 50
    }
  ],
  "total_cache_hits": 1450,
  "total_cache_misses": 50,
  "hit_rate": 0.9667
}
</code></pre>
<hr>
<h2 id="drift-detection"><a class="header" href="#drift-detection">Drift Detection</a></h2>
<p>Drift detection compares expected database schema against observed data patterns
to detect mismatches, unexpected nulls, and type drift.</p>
<h3 id="get-drift-results"><a class="header" href="#get-drift-results">Get Drift Results</a></h3>
<pre><code class="language-http">GET /pipelines/{name}/drift
</code></pre>
<p>Returns drift detection results for all tables in a pipeline.</p>
<p><strong>Response:</strong> <code>200 OK</code></p>
<pre><code class="language-json">[
  {
    "table": "orders",
    "has_drift": true,
    "columns": [
      {
        "column": "amount",
        "expected_type": "decimal(10,2)",
        "observed_types": ["string"],
        "mismatch_count": 42,
        "examples": ["\"99.99\""]
      }
    ],
    "events_analyzed": 1500,
    "events_with_drift": 42
  }
]
</code></pre>
<h3 id="get-table-drift"><a class="header" href="#get-table-drift">Get Table Drift</a></h3>
<pre><code class="language-http">GET /pipelines/{name}/drift/{table}
</code></pre>
<p>Returns drift detection results for a specific table.</p>
<p><strong>Response:</strong> <code>200 OK</code></p>
<pre><code class="language-json">{
  "table": "orders",
  "has_drift": false,
  "columns": [],
  "events_analyzed": 1000,
  "events_with_drift": 0
}
</code></pre>
<p><strong>Errors:</strong></p>
<ul>
<li><code>404 Not Found</code> - Table not found or no drift data available</li>
</ul>
<hr>
<h2 id="error-responses"><a class="header" href="#error-responses">Error Responses</a></h2>
<p>All error responses follow this format:</p>
<pre><code class="language-json">{
  "error": "Description of the error"
}
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Status Code</th><th>Meaning</th></tr>
</thead>
<tbody>
<tr><td><code>400 Bad Request</code></td><td>Invalid request body or parameters</td></tr>
<tr><td><code>404 Not Found</code></td><td>Resource doesnâ€™t exist</td></tr>
<tr><td><code>409 Conflict</code></td><td>Resource already exists</td></tr>
<tr><td><code>500 Internal Server Error</code></td><td>Unexpected server error</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="pipelines"><a class="header" href="#pipelines">Pipelines</a></h1>
<p>Each pipeline is created from a single <code>PipelineSpec</code>. The runtime spawns the source, processors, and sinks defined in the spec and coordinates them with batching and checkpointing.</p>
<ul>
<li>ğŸ”„ <strong>Live control</strong>: pause, resume, or stop pipelines through the REST API without redeploying.</li>
<li>ğŸ“¦ <strong>Coordinated delivery</strong>: batching and commit policy keep sinks consistent even when multiple outputs are configured.</li>
</ul>
<h2 id="lifecycle-controls"><a class="header" href="#lifecycle-controls">Lifecycle controls</a></h2>
<p>The REST API addresses pipelines by <code>metadata.name</code> and returns <code>PipeInfo</code> records containing the live spec and status.</p>
<ul>
<li><code>GET /healthz</code> - liveness probe.</li>
<li><code>GET /readyz</code> - readiness with pipeline states.</li>
<li><code>GET /pipelines</code> - list pipelines.</li>
<li><code>POST /pipelines</code> - create from a full spec.</li>
<li><code>PATCH /pipelines/{name}</code> - merge a partial spec (for example, adjust batch thresholds) and restart the pipeline.</li>
<li><code>POST /pipelines/{name}/pause</code> - pause ingestion and coordination.</li>
<li><code>POST /pipelines/{name}/resume</code> - resume a paused pipeline.</li>
<li><code>POST /pipelines/{name}/stop</code> - stop a running pipeline.</li>
</ul>
<p>Pausing halts both source ingestion and the coordinator. Resuming re-enables both ends so buffered events can drain cleanly.</p>
<h2 id="processors-1"><a class="header" href="#processors-1">Processors</a></h2>
<p>Processors run in the declared order for each batch. The built-in processor type is JavaScript, powered by <code>deno_core</code>.</p>
<ul>
<li><code>type: javascript</code>
<ul>
<li><code>id</code>: processor label.</li>
<li><code>inline</code>: JS source. Export a <code>processBatch(events)</code> function that returns the transformed batch.</li>
<li><code>limits</code> (optional): resource guardrails (<code>cpu_ms</code>, <code>mem_mb</code>, <code>timeout_ms</code>).</li>
</ul>
</li>
</ul>
<h2 id="batching-1"><a class="header" href="#batching-1">Batching</a></h2>
<p>The coordinator builds batches using soft thresholds:</p>
<ul>
<li><code>max_events</code>: flush after this many events.</li>
<li><code>max_bytes</code>: flush after the serialized size reaches this limit.</li>
<li><code>max_ms</code>: flush after this much time has elapsed since the batch started.</li>
<li><code>respect_source_tx</code>: when true, never split a single source transaction across batches.</li>
<li><code>max_inflight</code>: cap the number of batches being processed concurrently.</li>
</ul>
<h2 id="commit-policy-1"><a class="header" href="#commit-policy-1">Commit policy</a></h2>
<p>When multiple sinks are configured, checkpoints can wait for different acknowledgement rules:</p>
<ul>
<li><code>all</code>: every sink must acknowledge.</li>
<li><code>required</code> (default): only sinks marked <code>required: true</code> must acknowledge; others are best-effort.</li>
<li><code>quorum</code>: checkpoint after at least <code>quorum</code> sinks acknowledge.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="sources-1"><a class="header" href="#sources-1">Sources</a></h1>
<p>DeltaForge captures database changes through pluggable source connectors. Each source is configured under <code>spec.source</code> with a <code>type</code> field and a <code>config</code> object. Environment variables are expanded before parsing using <code>${VAR}</code> syntax.</p>
<h2 id="supported-sources"><a class="header" href="#supported-sources">Supported Sources</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Source</th><th>Status</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/mysql/mysql-original.svg" width="20" height="20"> <a href="#mysql-source"><code>mysql</code></a></td><td>âœ… Production</td><td>MySQL binlog CDC with GTID support</td></tr>
<tr><td><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/postgresql/postgresql-original.svg" width="20" height="20"> <a href="#postgresql-source"><code>postgres</code></a></td><td>âœ… Production</td><td>PostgreSQL logical replication via pgoutput</td></tr>
<tr><td><a href="#turso-source"><code>turso</code></a></td><td>ğŸ”§ Beta</td><td>Turso/libSQL CDC with multiple modes</td></tr>
</tbody>
</table>
</div>
<h2 id="common-behavior"><a class="header" href="#common-behavior">Common Behavior</a></h2>
<p>All sources share these characteristics:</p>
<ul>
<li><strong>Checkpointing</strong>: Progress is automatically saved and resumed on restart</li>
<li><strong>Schema tracking</strong>: Table schemas are loaded and fingerprinted for change detection</li>
<li><strong>At-least-once delivery</strong>: Events may be redelivered after failures; sinks should be idempotent</li>
<li><strong>Batching</strong>: Events are batched according to the pipelineâ€™s <code>batch</code> configuration</li>
<li><strong>Transaction boundaries</strong>: <code>respect_source_tx: true</code> (default) keeps source transactions intact</li>
</ul>
<h2 id="source-interface"><a class="header" href="#source-interface">Source Interface</a></h2>
<p>Sources implement a common trait that provides:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>trait Source {
    fn checkpoint_key(&amp;self) -&gt; &amp;str;
    async fn run(&amp;self, tx: Sender&lt;Event&gt;, checkpoint_store: Arc&lt;dyn CheckpointStore&gt;) -&gt; SourceHandle;
}
<span class="boring">}</span></code></pre>
<p>The returned <code>SourceHandle</code> supports pause/resume and graceful cancellation.</p>
<h2 id="adding-custom-sources"><a class="header" href="#adding-custom-sources">Adding Custom Sources</a></h2>
<p>The source interface is pluggable. To add a new source:</p>
<ol>
<li>Implement the <code>Source</code> trait</li>
<li>Add configuration parsing in <code>deltaforge-config</code></li>
<li>Register the source type in the pipeline builder</li>
</ol>
<p>See existing sources for implementation patterns.</p>
<div style="break-before: page; page-break-before: always;"></div>
<p align="center">
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/mysql/mysql-original.svg" alt="MySQL" width="80" height="80">
</p>

<h1 id="mysql-source"><a class="header" href="#mysql-source">MySQL source</a></h1>
<p>DeltaForge tails the MySQL binlog to capture row-level changes with automatic checkpointing and schema tracking.</p>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<h3 id="mysql-server-configuration"><a class="header" href="#mysql-server-configuration">MySQL Server Configuration</a></h3>
<p>Ensure your MySQL server has binary logging enabled with row-based format:</p>
<pre><code class="language-sql">-- Required server settings (my.cnf or SET GLOBAL)
log_bin = ON
binlog_format = ROW
binlog_row_image = FULL  -- Recommended for complete before-images
</code></pre>
<p>If <code>binlog_row_image</code> is not <code>FULL</code>, DeltaForge will warn at startup and before-images on UPDATE/DELETE events may be incomplete.</p>
<h3 id="user-privileges"><a class="header" href="#user-privileges">User Privileges</a></h3>
<p>Create a dedicated replication user with the required grants:</p>
<pre><code class="language-sql">-- Create user with mysql_native_password (required by binlog connector)
CREATE USER 'deltaforge'@'%' IDENTIFIED WITH mysql_native_password BY 'your_password';

-- Replication privileges (required)
GRANT REPLICATION REPLICA, REPLICATION CLIENT ON *.* TO 'deltaforge'@'%';

-- Schema introspection (required for table discovery)
GRANT SELECT, SHOW VIEW ON your_database.* TO 'deltaforge'@'%';

FLUSH PRIVILEGES;
</code></pre>
<p>For capturing all databases, grant <code>SELECT</code> on <code>*.*</code> instead.</p>
<h2 id="configuration-1"><a class="header" href="#configuration-1">Configuration</a></h2>
<p>Set <code>spec.source.type</code> to <code>mysql</code> and provide a config object:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Required</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>id</code></td><td>string</td><td>Yes</td><td>Unique identifier used for checkpoints, server_id derivation, and metrics</td></tr>
<tr><td><code>dsn</code></td><td>string</td><td>Yes</td><td>MySQL connection string with replication privileges</td></tr>
<tr><td><code>tables</code></td><td>array</td><td>No</td><td>Table patterns to capture; omit or leave empty to capture all user tables</td></tr>
</tbody>
</table>
</div>
<h3 id="table-patterns"><a class="header" href="#table-patterns">Table Patterns</a></h3>
<p>The <code>tables</code> field supports flexible pattern matching using SQL LIKE syntax:</p>
<pre><code class="language-yaml">tables:
  - shop.orders          # exact match: database "shop", table "orders"
  - shop.order_%         # LIKE pattern: tables starting with "order_" in "shop"
  - analytics.*          # wildcard: all tables in "analytics" database
  - %.audit_log          # cross-database: "audit_log" table in any database
  # omit entirely to capture all user tables (excludes system schemas)
</code></pre>
<p>System schemas (<code>mysql</code>, <code>information_schema</code>, <code>performance_schema</code>, <code>sys</code>) are always excluded.</p>
<h3 id="example"><a class="header" href="#example">Example</a></h3>
<pre><code class="language-yaml">source:
  type: mysql
  config:
    id: orders-mysql
    dsn: ${MYSQL_DSN}
    tables:
      - shop.orders
      - shop.order_items
</code></pre>
<h2 id="resume-behavior"><a class="header" href="#resume-behavior">Resume Behavior</a></h2>
<p>DeltaForge automatically checkpoints progress and resumes from the last position on restart. The resume strategy follows this priority:</p>
<ol>
<li><strong>GTID</strong> - Preferred if the MySQL server has GTID enabled. Provides the most reliable resume across binlog rotations and failovers.</li>
<li><strong>File:position</strong> - Used when GTID is not available. Resumes from the exact binlog file and byte offset.</li>
<li><strong>Binlog tail</strong> - On first run with no checkpoint, starts from the current end of the binlog (no historical replay).</li>
</ol>
<p>Checkpoints are stored using the <code>id</code> field as the key.</p>
<h2 id="server-id-handling"><a class="header" href="#server-id-handling">Server ID Handling</a></h2>
<p>MySQL replication requires each replica to have a unique <code>server_id</code>. DeltaForge derives this automatically from the source <code>id</code> using a CRC32 hash:</p>
<pre><code>server_id = 1 + (CRC32(id) % 4,000,000,000)
</code></pre>
<p>When running multiple DeltaForge instances against the same MySQL server, ensure each has a unique <code>id</code> to avoid server_id conflicts.</p>
<h2 id="schema-tracking"><a class="header" href="#schema-tracking">Schema Tracking</a></h2>
<p>DeltaForge has a built-in schema registry to track table schemas, per source. For MySQL source:</p>
<ul>
<li>Schemas are preloaded at startup by querying <code>INFORMATION_SCHEMA</code></li>
<li>Each schema is fingerprinted using SHA-256 for change detection</li>
<li>Events carry <code>schema_version</code> (fingerprint) and <code>schema_sequence</code> (monotonic counter)</li>
<li>Schema-to-checkpoint correlation enables reliable replay</li>
</ul>
<p>Schema changes (DDL) trigger automatic reload of affected table schemas.</p>
<h2 id="timeouts-and-heartbeats"><a class="header" href="#timeouts-and-heartbeats">Timeouts and Heartbeats</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Behavior</th><th>Value</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>Heartbeat interval</td><td>15s</td><td>Server sends heartbeat if no events</td></tr>
<tr><td>Read timeout</td><td>90s</td><td>Maximum wait for next binlog event</td></tr>
<tr><td>Inactivity timeout</td><td>60s</td><td>Triggers reconnect if no data received</td></tr>
<tr><td>Connect timeout</td><td>30s</td><td>Maximum time to establish connection</td></tr>
</tbody>
</table>
</div>
<p>These values are currently fixed. Reconnection uses exponential backoff with jitter.</p>
<h2 id="event-format"><a class="header" href="#event-format">Event Format</a></h2>
<p>Each captured row change produces an event with:</p>
<ul>
<li><code>op</code>: <code>insert</code>, <code>update</code>, or <code>delete</code></li>
<li><code>before</code>: Previous row state (updates and deletes only, requires <code>binlog_row_image = FULL</code>)</li>
<li><code>after</code>: New row state (inserts and updates only)</li>
<li><code>table</code>: Fully qualified table name (<code>database.table</code>)</li>
<li><code>tx_id</code>: GTID if available</li>
<li><code>checkpoint</code>: Binlog position for resume</li>
<li><code>schema_version</code>: Schema fingerprint</li>
<li><code>schema_sequence</code>: Monotonic sequence for schema correlation</li>
</ul>
<h2 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h2>
<h3 id="connection-issues"><a class="header" href="#connection-issues">Connection Issues</a></h3>
<p>If you see authentication errors mentioning <code>mysql_native_password</code>:</p>
<pre><code class="language-sql">ALTER USER 'deltaforge'@'%' IDENTIFIED WITH mysql_native_password BY 'password';
</code></pre>
<h3 id="missing-before-images"><a class="header" href="#missing-before-images">Missing Before-Images</a></h3>
<p>If UPDATE/DELETE events have incomplete <code>before</code> data:</p>
<pre><code class="language-sql">SET GLOBAL binlog_row_image = 'FULL';
</code></pre>
<h3 id="binlog-not-enabled"><a class="header" href="#binlog-not-enabled">Binlog Not Enabled</a></h3>
<p>Check binary logging status:</p>
<pre><code class="language-sql">SHOW VARIABLES LIKE 'log_bin';
SHOW VARIABLES LIKE 'binlog_format';
SHOW BINARY LOG STATUS;  -- or SHOW MASTER STATUS on older versions
</code></pre>
<h3 id="privilege-issues"><a class="header" href="#privilege-issues">Privilege Issues</a></h3>
<p>Verify grants for your user:</p>
<pre><code class="language-sql">SHOW GRANTS FOR 'deltaforge'@'%';
</code></pre>
<p>Required grants include <code>REPLICATION REPLICA</code> and <code>REPLICATION CLIENT</code>.</p>
<div style="break-before: page; page-break-before: always;"></div>
<p align="center">
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/postgresql/postgresql-original.svg" alt="PostgreSQL" width="80" height="80">
</p>

<h1 id="postgresql-source"><a class="header" href="#postgresql-source">PostgreSQL source</a></h1>
<p>DeltaForge captures row-level changes from PostgreSQL using logical replication with the pgoutput plugin.</p>
<h2 id="prerequisites-1"><a class="header" href="#prerequisites-1">Prerequisites</a></h2>
<h3 id="postgresql-server-configuration"><a class="header" href="#postgresql-server-configuration">PostgreSQL Server Configuration</a></h3>
<p>Enable logical replication in <code>postgresql.conf</code>:</p>
<pre><code class="language-ini"># Required settings
wal_level = logical
max_replication_slots = 10    # At least 1 per DeltaForge pipeline
max_wal_senders = 10          # At least 1 per DeltaForge pipeline
</code></pre>
<p>Restart PostgreSQL after changing these settings.</p>
<h3 id="user-privileges-1"><a class="header" href="#user-privileges-1">User Privileges</a></h3>
<p>Create a replication user with the required privileges:</p>
<pre><code class="language-sql">-- Create user with replication capability
CREATE ROLE deltaforge WITH LOGIN REPLICATION PASSWORD 'your_password';

-- Grant connect access
GRANT CONNECT ON DATABASE your_database TO deltaforge;

-- Grant schema usage and table access for schema introspection
GRANT USAGE ON SCHEMA public TO deltaforge;
GRANT SELECT ON ALL TABLES IN SCHEMA public TO deltaforge;

-- For automatic publication/slot creation (optional)
-- If you prefer manual setup, skip this and create them yourself
ALTER ROLE deltaforge SUPERUSER;  -- Or use manual setup below
</code></pre>
<h3 id="pg_hbaconf"><a class="header" href="#pg_hbaconf">pg_hba.conf</a></h3>
<p>Ensure your <code>pg_hba.conf</code> allows replication connections:</p>
<pre><code># TYPE  DATABASE        USER            ADDRESS                 METHOD
host    replication     deltaforge      0.0.0.0/0               scram-sha-256
host    your_database   deltaforge      0.0.0.0/0               scram-sha-256
</code></pre>
<h3 id="replication-slot-and-publication"><a class="header" href="#replication-slot-and-publication">Replication Slot and Publication</a></h3>
<p>DeltaForge can automatically create the replication slot and publication on first run. Alternatively, create them manually:</p>
<pre><code class="language-sql">-- Create publication for specific tables
CREATE PUBLICATION my_pub FOR TABLE public.orders, public.order_items;

-- Or for all tables
CREATE PUBLICATION my_pub FOR ALL TABLES;

-- Create replication slot
SELECT pg_create_logical_replication_slot('my_slot', 'pgoutput');
</code></pre>
<h3 id="replica-identity"><a class="header" href="#replica-identity">Replica Identity</a></h3>
<p>For complete before-images on UPDATE and DELETE operations, set tables to <code>REPLICA IDENTITY FULL</code>:</p>
<pre><code class="language-sql">ALTER TABLE public.orders REPLICA IDENTITY FULL;
ALTER TABLE public.order_items REPLICA IDENTITY FULL;
</code></pre>
<p>Without this setting:</p>
<ul>
<li><strong>FULL</strong>: Complete row data in before-images</li>
<li><strong>DEFAULT</strong> (primary key): Only primary key columns in before-images</li>
<li><strong>NOTHING</strong>: No before-images at all</li>
</ul>
<p>DeltaForge warns at startup if tables donâ€™t have <code>REPLICA IDENTITY FULL</code>.</p>
<h2 id="configuration-2"><a class="header" href="#configuration-2">Configuration</a></h2>
<p>Set <code>spec.source.type</code> to <code>postgres</code> and provide a config object:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Required</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>id</code></td><td>string</td><td>Yes</td><td>â€”</td><td>Unique identifier for checkpoints and metrics</td></tr>
<tr><td><code>dsn</code></td><td>string</td><td>Yes</td><td>â€”</td><td>PostgreSQL connection string</td></tr>
<tr><td><code>slot</code></td><td>string</td><td>Yes</td><td>â€”</td><td>Replication slot name</td></tr>
<tr><td><code>publication</code></td><td>string</td><td>Yes</td><td>â€”</td><td>Publication name</td></tr>
<tr><td><code>tables</code></td><td>array</td><td>Yes</td><td>â€”</td><td>Table patterns to capture</td></tr>
<tr><td><code>start_position</code></td><td>string/object</td><td>No</td><td><code>earliest</code></td><td>Where to start when no checkpoint exists</td></tr>
</tbody>
</table>
</div>
<h3 id="dsn-formats"><a class="header" href="#dsn-formats">DSN Formats</a></h3>
<p>DeltaForge accepts both URL-style and key=value DSN formats:</p>
<pre><code class="language-yaml"># URL style
dsn: "postgres://user:pass@localhost:5432/mydb"

# Key=value style
dsn: "host=localhost port=5432 user=deltaforge password=pass dbname=mydb"
</code></pre>
<h3 id="table-patterns-1"><a class="header" href="#table-patterns-1">Table Patterns</a></h3>
<p>The <code>tables</code> field supports flexible pattern matching:</p>
<pre><code class="language-yaml">tables:
  - public.orders          # exact match: schema "public", table "orders"
  - public.order_%         # LIKE pattern: tables starting with "order_"
  - myschema.*             # wildcard: all tables in "myschema"
  - %.audit_log            # cross-schema: "audit_log" table in any schema
  - orders                 # defaults to public schema: "public.orders"
</code></pre>
<p>System schemas (<code>pg_catalog</code>, <code>information_schema</code>, <code>pg_toast</code>) are always excluded.</p>
<h3 id="start-position"><a class="header" href="#start-position">Start Position</a></h3>
<p>Controls where replication begins when no checkpoint exists:</p>
<pre><code class="language-yaml"># Start from the earliest available position (slot's restart_lsn)
start_position: earliest

# Start from current WAL position (skip existing data)
start_position: latest

# Start from a specific LSN
start_position:
  lsn: "0/16B6C50"
</code></pre>
<h3 id="example-1"><a class="header" href="#example-1">Example</a></h3>
<pre><code class="language-yaml">source:
  type: postgres
  config:
    id: orders-postgres
    dsn: ${POSTGRES_DSN}
    slot: deltaforge_orders
    publication: orders_pub
    tables:
      - public.orders
      - public.order_items
    start_position: earliest
</code></pre>
<h2 id="resume-behavior-1"><a class="header" href="#resume-behavior-1">Resume Behavior</a></h2>
<p>DeltaForge checkpoints progress using PostgreSQLâ€™s LSN (Log Sequence Number):</p>
<ol>
<li><strong>With checkpoint</strong>: Resumes from the stored LSN</li>
<li><strong>Without checkpoint</strong>: Uses the slotâ€™s <code>confirmed_flush_lsn</code> or <code>restart_lsn</code></li>
<li><strong>New slot</strong>: Starts from <code>pg_current_wal_lsn()</code> or the configured <code>start_position</code></li>
</ol>
<p>Checkpoints are stored using the <code>id</code> field as the key.</p>
<h2 id="type-handling"><a class="header" href="#type-handling">Type Handling</a></h2>
<p>DeltaForge preserves PostgreSQLâ€™s native type semantics:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>PostgreSQL Type</th><th>JSON Representation</th></tr>
</thead>
<tbody>
<tr><td><code>boolean</code></td><td><code>true</code> / <code>false</code></td></tr>
<tr><td><code>integer</code>, <code>bigint</code></td><td>JSON number</td></tr>
<tr><td><code>real</code>, <code>double precision</code></td><td>JSON number</td></tr>
<tr><td><code>numeric</code></td><td>JSON string (preserves precision)</td></tr>
<tr><td><code>text</code>, <code>varchar</code></td><td>JSON string</td></tr>
<tr><td><code>json</code>, <code>jsonb</code></td><td>Parsed JSON object/array</td></tr>
<tr><td><code>bytea</code></td><td><code>{"_base64": "..."}</code></td></tr>
<tr><td><code>uuid</code></td><td>JSON string</td></tr>
<tr><td><code>timestamp</code>, <code>date</code>, <code>time</code></td><td>ISO 8601 string</td></tr>
<tr><td>Arrays (<code>int[]</code>, <code>text[]</code>, etc.)</td><td>JSON array</td></tr>
<tr><td>TOAST unchanged</td><td><code>{"_unchanged": true}</code></td></tr>
</tbody>
</table>
</div>
<h2 id="event-format-1"><a class="header" href="#event-format-1">Event Format</a></h2>
<p>Each captured row change produces an event with:</p>
<ul>
<li><code>op</code>: <code>insert</code>, <code>update</code>, <code>delete</code>, or <code>truncate</code></li>
<li><code>before</code>: Previous row state (updates and deletes, requires appropriate replica identity)</li>
<li><code>after</code>: New row state (inserts and updates)</li>
<li><code>table</code>: Fully qualified table name (<code>schema.table</code>)</li>
<li><code>tx_id</code>: PostgreSQL transaction ID (xid)</li>
<li><code>checkpoint</code>: LSN position for resume</li>
<li><code>schema_version</code>: Schema fingerprint</li>
<li><code>schema_sequence</code>: Monotonic sequence for schema correlation</li>
</ul>
<h2 id="wal-management"><a class="header" href="#wal-management">WAL Management</a></h2>
<p>Logical replication slots prevent WAL segments from being recycled until the consumer confirms receipt. To avoid disk space issues:</p>
<ol>
<li><strong>Monitor slot lag</strong>: Check <code>pg_replication_slots.restart_lsn</code> vs <code>pg_current_wal_lsn()</code></li>
<li><strong>Set retention limits</strong>: Configure <code>max_slot_wal_keep_size</code> (PostgreSQL 13+)</li>
<li><strong>Handle stale slots</strong>: Drop unused slots with <code>pg_drop_replication_slot('slot_name')</code></li>
</ol>
<pre><code class="language-sql">-- Check slot status and lag
SELECT slot_name, 
       pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn)) as lag
FROM pg_replication_slots;
</code></pre>
<h2 id="troubleshooting-1"><a class="header" href="#troubleshooting-1">Troubleshooting</a></h2>
<h3 id="connection-issues-1"><a class="header" href="#connection-issues-1">Connection Issues</a></h3>
<p>If you see authentication errors:</p>
<pre><code class="language-sql">-- Verify user has replication privilege
SELECT rolname, rolreplication FROM pg_roles WHERE rolname = 'deltaforge';

-- Check pg_hba.conf allows replication connections
-- Ensure the line type includes "replication" database
</code></pre>
<h3 id="missing-before-images-1"><a class="header" href="#missing-before-images-1">Missing Before-Images</a></h3>
<p>If UPDATE/DELETE events have incomplete <code>before</code> data:</p>
<pre><code class="language-sql">-- Check current replica identity
SELECT relname, relreplident 
FROM pg_class 
WHERE relname = 'your_table';
-- d = default, n = nothing, f = full, i = index

-- Set to FULL for complete before-images
ALTER TABLE your_table REPLICA IDENTITY FULL;
</code></pre>
<h3 id="slotpublication-not-found"><a class="header" href="#slotpublication-not-found">Slot/Publication Not Found</a></h3>
<pre><code class="language-sql">-- List existing publications
SELECT * FROM pg_publication;

-- List existing slots
SELECT * FROM pg_replication_slots;

-- Create if missing
CREATE PUBLICATION my_pub FOR TABLE public.orders;
SELECT pg_create_logical_replication_slot('my_slot', 'pgoutput');
</code></pre>
<h3 id="wal-disk-usage-growing"><a class="header" href="#wal-disk-usage-growing">WAL Disk Usage Growing</a></h3>
<pre><code class="language-sql">-- Check slot lag
SELECT slot_name, 
       active,
       pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn)) as lag
FROM pg_replication_slots;

-- If slot is inactive and not needed, drop it
SELECT pg_drop_replication_slot('unused_slot');
</code></pre>
<h3 id="logical-replication-not-enabled"><a class="header" href="#logical-replication-not-enabled">Logical Replication Not Enabled</a></h3>
<pre><code class="language-sql">-- Check wal_level
SHOW wal_level;  -- Should be 'logical'

-- If not, update postgresql.conf and restart PostgreSQL
-- wal_level = logical
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="turso-source"><a class="header" href="#turso-source">Turso Source</a></h1>
<blockquote>
<p>âš ï¸ <strong>STATUS: EXPERIMENTAL / PAUSED</strong></p>
<p>The Turso source is not yet ready for production use. Native CDC in Turso/libSQL
is still evolving and has limitations:</p>
<ul>
<li>CDC is per-connection (only changes from the enabling connection are captured)</li>
<li>File locking prevents concurrent access</li>
<li>sqld Docker image doesnâ€™t have CDC support yet</li>
</ul>
<p>This documentation is retained for reference. The code exists but is not officially supported.</p>
</blockquote>
<hr>
<p>The Turso source captures changes from Turso and libSQL databases. It supports multiple CDC modes to work with different database configurations and Turso versions.</p>
<h2 id="cdc-modes"><a class="header" href="#cdc-modes">CDC Modes</a></h2>
<p>DeltaForge supports four CDC modes for Turso/libSQL:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Mode</th><th>Description</th><th>Requirements</th></tr>
</thead>
<tbody>
<tr><td><code>native</code></td><td>Uses Tursoâ€™s built-in CDC via <code>turso_cdc</code> table</td><td>Turso v0.1.2+ with CDC enabled</td></tr>
<tr><td><code>triggers</code></td><td>Shadow tables populated by database triggers</td><td>Standard SQLite/libSQL</td></tr>
<tr><td><code>polling</code></td><td>Tracks changes via rowid comparison</td><td>Any SQLite/libSQL (inserts only)</td></tr>
<tr><td><code>auto</code></td><td>Automatic fallback: native â†’ triggers â†’ polling</td><td>Any</td></tr>
</tbody>
</table>
</div>
<h3 id="native-mode"><a class="header" href="#native-mode">Native Mode</a></h3>
<p>Native mode uses Tursoâ€™s built-in CDC capabilities. This is the most efficient mode when available.</p>
<p><strong>Requirements:</strong></p>
<ul>
<li>Turso database with CDC enabled</li>
<li>Turso server v0.1.2 or later</li>
</ul>
<p><strong>How it works:</strong></p>
<ol>
<li>Queries the <code>turso_cdc</code> system table for changes</li>
<li>Uses <code>bin_record_json_object()</code> to extract row data as JSON</li>
<li>Tracks position via change ID in checkpoints</li>
</ol>
<h3 id="triggers-mode"><a class="header" href="#triggers-mode">Triggers Mode</a></h3>
<p>Triggers mode uses shadow tables and database triggers to capture changes. This works with standard SQLite and libSQL without requiring native CDC support.</p>
<p><strong>How it works:</strong></p>
<ol>
<li>Creates shadow tables (<code>_df_cdc_{table}</code>) for each tracked table</li>
<li>Installs INSERT/UPDATE/DELETE triggers that write to shadow tables</li>
<li>Polls shadow tables for new change records</li>
<li>Cleans up processed records periodically</li>
</ol>
<h3 id="polling-mode"><a class="header" href="#polling-mode">Polling Mode</a></h3>
<p>Polling mode uses rowid tracking to detect new rows. This is the simplest mode but only captures inserts (not updates or deletes).</p>
<p><strong>How it works:</strong></p>
<ol>
<li>Tracks the maximum rowid seen per table</li>
<li>Queries for rows with rowid greater than last seen</li>
<li>Emits insert events for new rows</li>
</ol>
<p><strong>Limitations:</strong></p>
<ul>
<li>Only captures INSERT operations</li>
<li>Cannot detect UPDATE or DELETE</li>
<li>Requires tables to have accessible rowid (not WITHOUT ROWID tables)</li>
</ul>
<h3 id="auto-mode"><a class="header" href="#auto-mode">Auto Mode</a></h3>
<p>Auto mode tries each CDC mode in order and uses the first one that works:</p>
<ol>
<li>Try native mode (check for <code>turso_cdc</code> table)</li>
<li>Try triggers mode (check for existing CDC triggers)</li>
<li>Fall back to polling mode</li>
</ol>
<p>This is useful for deployments where the database capabilities may vary.</p>
<h2 id="configuration-3"><a class="header" href="#configuration-3">Configuration</a></h2>
<pre><code class="language-yaml">source:
  type: turso
  config:
    id: turso-main
    url: "libsql://your-db.turso.io"
    auth_token: "${TURSO_AUTH_TOKEN}"
    tables: ["users", "orders", "order_items"]
    cdc_mode: auto
    poll_interval_ms: 1000
    native_cdc:
      level: data
</code></pre>
<h3 id="configuration-options"><a class="header" href="#configuration-options">Configuration Options</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Required</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>id</code></td><td>string</td><td>Yes</td><td>â€”</td><td>Logical identifier for metrics and logging</td></tr>
<tr><td><code>url</code></td><td>string</td><td>Yes</td><td>â€”</td><td>Database URL (<code>libsql://</code>, <code>http://</code>, or file path)</td></tr>
<tr><td><code>auth_token</code></td><td>string</td><td>No</td><td>â€”</td><td>Authentication token for Turso cloud</td></tr>
<tr><td><code>tables</code></td><td>array</td><td>Yes</td><td>â€”</td><td>Tables to track (supports wildcards)</td></tr>
<tr><td><code>cdc_mode</code></td><td>string</td><td>No</td><td><code>auto</code></td><td>CDC mode: <code>native</code>, <code>triggers</code>, <code>polling</code>, <code>auto</code></td></tr>
<tr><td><code>poll_interval_ms</code></td><td>integer</td><td>No</td><td><code>1000</code></td><td>Polling interval in milliseconds</td></tr>
<tr><td><code>native_cdc.level</code></td><td>string</td><td>No</td><td><code>data</code></td><td>Native CDC level: <code>binlog</code> or <code>data</code></td></tr>
</tbody>
</table>
</div>
<h3 id="table-patterns-2"><a class="header" href="#table-patterns-2">Table Patterns</a></h3>
<p>The <code>tables</code> field supports patterns:</p>
<pre><code class="language-yaml">tables:
  - users              # Exact match
  - order%             # Prefix match (order, orders, order_items)
  - "*"                # All tables (excluding system tables)
</code></pre>
<p>System tables and DeltaForge infrastructure tables are automatically excluded:</p>
<ul>
<li><code>sqlite_*</code> â€” SQLite system tables</li>
<li><code>_df_*</code> â€” DeltaForge CDC shadow tables</li>
<li><code>_litestream*</code> â€” Litestream replication tables</li>
<li><code>_turso*</code> â€” Turso internal tables</li>
<li><code>turso_cdc</code> â€” Turso CDC system table</li>
</ul>
<h3 id="native-cdc-levels"><a class="header" href="#native-cdc-levels">Native CDC Levels</a></h3>
<p>When using native mode, you can choose the CDC level:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Level</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>data</code></td><td>Only row data changes (default, more efficient)</td></tr>
<tr><td><code>binlog</code></td><td>Full binlog-style events with additional metadata</td></tr>
</tbody>
</table>
</div>
<h2 id="examples"><a class="header" href="#examples">Examples</a></h2>
<h3 id="local-development"><a class="header" href="#local-development">Local Development</a></h3>
<pre><code class="language-yaml">source:
  type: turso
  config:
    id: local-dev
    url: "http://127.0.0.1:8080"
    tables: ["users", "orders"]
    cdc_mode: auto
    poll_interval_ms: 500
</code></pre>
<h3 id="turso-cloud"><a class="header" href="#turso-cloud">Turso Cloud</a></h3>
<pre><code class="language-yaml">source:
  type: turso
  config:
    id: turso-prod
    url: "libsql://mydb-myorg.turso.io"
    auth_token: "${TURSO_AUTH_TOKEN}"
    tables: ["*"]
    cdc_mode: native
    poll_interval_ms: 1000
</code></pre>
<h3 id="sqlite-file-polling-only"><a class="header" href="#sqlite-file-polling-only">SQLite File (Polling Only)</a></h3>
<pre><code class="language-yaml">source:
  type: turso
  config:
    id: sqlite-file
    url: "file:./data/myapp.db"
    tables: ["events", "audit_log"]
    cdc_mode: polling
    poll_interval_ms: 2000
</code></pre>
<h2 id="checkpoints"><a class="header" href="#checkpoints">Checkpoints</a></h2>
<p>Turso checkpoints track position differently depending on the CDC mode:</p>
<pre><code class="language-json">{
  "mode": "native",
  "last_change_id": 12345,
  "table_positions": {}
}
</code></pre>
<p>For polling mode, positions are tracked per-table:</p>
<pre><code class="language-json">{
  "mode": "polling",
  "last_change_id": null,
  "table_positions": {
    "users": 1000,
    "orders": 2500
  }
}
</code></pre>
<h2 id="schema-loading"><a class="header" href="#schema-loading">Schema Loading</a></h2>
<p>The Turso source includes a schema loader that:</p>
<ul>
<li>Queries <code>PRAGMA table_info()</code> for column metadata</li>
<li>Detects SQLite type affinities (INTEGER, TEXT, REAL, BLOB, NUMERIC)</li>
<li>Identifies primary keys and autoincrement columns</li>
<li>Handles WITHOUT ROWID tables</li>
<li>Checks for existing CDC triggers</li>
</ul>
<p>Schema information is available via the REST API at <code>/pipelines/{name}/schemas</code>.</p>
<h2 id="notes"><a class="header" href="#notes">Notes</a></h2>
<ul>
<li><strong>WITHOUT ROWID tables</strong>: Polling mode cannot track WITHOUT ROWID tables. Use triggers or native mode instead.</li>
<li><strong>Type affinity</strong>: SQLite uses type affinity rather than strict types. The schema loader maps declared types to SQLite affinities.</li>
<li><strong>Trigger cleanup</strong>: In triggers mode, processed change records are cleaned up automatically based on checkpoint position.</li>
<li><strong>Connection handling</strong>: The source maintains a single connection and reconnects automatically on failure.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="processors-2"><a class="header" href="#processors-2">Processors</a></h1>
<p>Processors can transform, modify or take extra action per event batch between the source and sinks. They run in the order listed in <code>spec.processors</code>, and each receives the output of the previous one. A processor can modify events, filter them out, or add routing metadata.</p>
<h2 id="available-processors"><a class="header" href="#available-processors">Available Processors</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Processor</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><a href="#javascript-1"><code>javascript</code></a></td><td>Custom transformations using V8-powered JavaScript</td></tr>
<tr><td><a href="#outbox"><code>outbox</code></a></td><td>Transactional outbox pattern - extracts payload, resolves topic, sets routing headers</td></tr>
<tr><td><a href="#flatten-1"><code>flatten</code></a></td><td>Flatten nested JSON objects into conmbined keys</td></tr>
</tbody>
</table>
</div>
<h2 id="javascript-1"><a class="header" href="#javascript-1">JavaScript</a></h2>
<p>Run arbitrary JavaScript against each event batch. Uses the V8 engine via <code>deno_core</code> for near-native speed with configurable resource limits.</p>
<pre><code class="language-yaml">processors:
  - type: javascript
    id: enrich
    inline: |
      function processBatch(events) {
        return events.map(e =&gt; {
          e.tags = ["processed"];
          return e;
        });
      }
    limits:
      cpu_ms: 50
      mem_mb: 128
      timeout_ms: 500
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>id</code></td><td>string</td><td><em>(required)</em></td><td>Processor identifier</td></tr>
<tr><td><code>inline</code></td><td>string</td><td><em>(required)</em></td><td>JavaScript source code</td></tr>
<tr><td><code>limits.cpu_ms</code></td><td>int</td><td><code>50</code></td><td>CPU time limit per batch</td></tr>
<tr><td><code>limits.mem_mb</code></td><td>int</td><td><code>128</code></td><td>V8 heap memory limit</td></tr>
<tr><td><code>limits.timeout_ms</code></td><td>int</td><td><code>500</code></td><td>Wall-clock timeout per batch</td></tr>
</tbody>
</table>
</div>
<h3 id="performance"><a class="header" href="#performance">Performance</a></h3>
<p>JavaScript processing adds a fixed overhead per batch plus linear scaling per event. Benchmarks show ~70K+ events/sec throughput with typical transforms â€” roughly 61Ã— slower than native Rust processors, but sufficient for most workloads. If you need higher throughput, keep transform logic simple or move heavy processing downstream.</p>
<h3 id="tips"><a class="header" href="#tips">Tips</a></h3>
<ul>
<li>Return an empty array to drop all events in a batch.</li>
<li>Return multiple copies of an event to fan out.</li>
<li>JavaScript numbers are <code>f64</code> â€” integer columns wider than 53 bits may lose precision. Use string representation for such values.</li>
</ul>
<h2 id="outbox"><a class="header" href="#outbox">Outbox</a></h2>
<p>The outbox processor transforms events captured by the <a href="#outbox-pattern">outbox pattern</a> into routed, sink-ready events. It extracts business fields from the raw outbox payload, resolves the destination topic, and passes through all non-outbox events unchanged.</p>
<pre><code class="language-yaml">processors:
  - type: outbox
    topic: "${aggregate_type}.${event_type}"
    default_topic: "events.unrouted"
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>id</code></td><td>string</td><td><code>"outbox"</code></td><td>Processor identifier</td></tr>
<tr><td><code>tables</code></td><td>array</td><td><code>[]</code></td><td>Only process outbox events matching these patterns. Empty = all.</td></tr>
<tr><td><code>topic</code></td><td>string</td><td>â€”</td><td>Topic template with <code>${field}</code> placeholders (resolved against raw payload columns)</td></tr>
<tr><td><code>default_topic</code></td><td>string</td><td>â€”</td><td>Fallback when template resolution fails</td></tr>
<tr><td><code>columns</code></td><td>object</td><td><em>(defaults below)</em></td><td>Field name mappings</td></tr>
<tr><td><code>additional_headers</code></td><td>map</td><td><code>{}</code></td><td>Forward extra payload fields as routing headers. Key = header name, value = column name.</td></tr>
<tr><td><code>raw_payload</code></td><td>bool</td><td><code>false</code></td><td>Deliver payload as-is, bypassing envelope wrapping</td></tr>
</tbody>
</table>
</div>
<h3 id="column-defaults"><a class="header" href="#column-defaults">Column Defaults</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Key</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>payload</code></td><td><code>"payload"</code></td><td>Field containing the event body</td></tr>
<tr><td><code>aggregate_type</code></td><td><code>"aggregate_type"</code></td><td>Domain aggregate type</td></tr>
<tr><td><code>aggregate_id</code></td><td><code>"aggregate_id"</code></td><td>Aggregate identifier</td></tr>
<tr><td><code>event_type</code></td><td><code>"event_type"</code></td><td>Domain event type</td></tr>
<tr><td><code>topic</code></td><td><code>"topic"</code></td><td>Optional explicit topic override in payload</td></tr>
</tbody>
</table>
</div>
<p>See the <a href="#outbox-pattern">Outbox Pattern</a> guide for source configuration, complete examples, and multi-outbox routing.</p>
<h2 id="flatten-1"><a class="header" href="#flatten-1">Flatten</a></h2>
<p>Flattens nested JSON objects in event payloads into top-level keys joined by a configurable separator. Works on every object-valued field present on the event without assuming any particular envelope structure - CDC <code>before</code>/<code>after</code>, outbox business payloads, or any custom fields introduced by upstream processors are all handled uniformly.</p>
<pre><code class="language-yaml">processors:
  - type: flatten
    id: flat
    separator: "__"
    max_depth: 3
    on_collision: last
    empty_object: preserve
    lists: preserve
    empty_list: drop
</code></pre>
<p><strong>Input:</strong></p>
<pre><code class="language-json">{
  "after": {
    "order_id": "abc",
    "customer": {
      "id": 1,
      "address": { "city": "Berlin", "zip": "10115" }
    },
    "tags": ["vip"],
    "meta": {}
  }
}
</code></pre>
<p><strong>Output</strong> (with defaults â€” <code>separator: "__"</code>, <code>empty_object: preserve</code>, <code>lists: preserve</code>):</p>
<pre><code class="language-json">{
  "after": {
    "order_id": "abc",
    "customer__id": 1,
    "customer__address__city": "Berlin",
    "customer__address__zip": "10115",
    "tags": ["vip"],
    "meta": {}
  }
}
</code></pre>
<h3 id="configuration-4"><a class="header" href="#configuration-4">Configuration</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>id</code></td><td>string</td><td><code>"flatten"</code></td><td>Processor identifier</td></tr>
<tr><td><code>separator</code></td><td>string</td><td><code>"__"</code></td><td>Separator inserted between path segments</td></tr>
<tr><td><code>max_depth</code></td><td>int</td><td>unlimited</td><td>Stop recursing at this depth; objects at the boundary are kept as opaque leaves</td></tr>
<tr><td><code>on_collision</code></td><td>string</td><td><code>last</code></td><td>What to do when two paths produce the same key. <code>last</code>, <code>first</code>, or <code>error</code></td></tr>
<tr><td><code>empty_object</code></td><td>string</td><td><code>preserve</code></td><td>How to handle <code>{}</code> values. <code>preserve</code>, <code>drop</code>, or <code>null</code></td></tr>
<tr><td><code>lists</code></td><td>string</td><td><code>preserve</code></td><td>How to handle array values. <code>preserve</code> (keep as-is) or <code>index</code> (expand to <code>field__0</code>, <code>field__1</code>, â€¦)</td></tr>
<tr><td><code>empty_list</code></td><td>string</td><td><code>preserve</code></td><td>How to handle <code>[]</code> values. <code>preserve</code>, <code>drop</code>, or <code>null</code></td></tr>
</tbody>
</table>
</div>
<h3 id="max_depth-in-practice"><a class="header" href="#max_depth-in-practice">max_depth in practice</a></h3>
<p><code>max_depth</code> counts nesting levels from the top of the payload object. Objects at the boundary are treated as opaque leaves rather than expanded further, still subject to <code>empty_object</code> policy.</p>
<pre><code># max_depth: 2

depth 0: customer               -&gt; object, recurse
depth 1: customer__address      -&gt; object, recurse
depth 2: customer__address__geo -&gt; STOP, kept as leaf {"lat": 52.5, "lng": 13.4}
</code></pre>
<p>Without <code>max_depth</code>, a deeply nested or recursive payload could produce a large number of keys. Setting a limit is recommended for payloads with variable or unknown nesting depth.</p>
<h3 id="collision-policy"><a class="header" href="#collision-policy">Collision policy</a></h3>
<p>A collision occurs when two input paths produce the same flattened key - typically when a payload already contains a pre-flattened key (e.g. <code>"a__b": 1</code>) alongside a nested object (<code>"a": {"b": 2}</code>).</p>
<ul>
<li><code>last</code> - the last path to write a key wins (default, never fails)</li>
<li><code>first</code> - the first path to write a key wins, subsequent writes are ignored</li>
<li><code>error</code> - the batch fails immediately, useful in strict pipelines where collisions indicate a schema problem</li>
</ul>
<h3 id="working-with-outbox-payloads"><a class="header" href="#working-with-outbox-payloads">Working with outbox payloads</a></h3>
<p>After the <a href="#outbox"><code>outbox</code></a> processor runs, <code>event.after</code> holds the extracted business payload - there is no <code>before</code>. The flatten processor handles this naturally since it operates on whatever fields are present:</p>
<pre><code class="language-yaml">processors:
  - type: outbox
    topic: "${aggregate_type}.${event_type}"
  - type: flatten
    id: flat
    separator: "."
    empty_list: drop
</code></pre>
<h3 id="envelope-interaction"><a class="header" href="#envelope-interaction">Envelope interaction</a></h3>
<p>The flatten processor runs on the raw <code>Event</code> struct <strong>before</strong> sink delivery. Envelope wrapping happens inside the sink, after all processors have run. This means the envelope always wraps already-flattened data, no special configuration needed.</p>
<pre><code>Source â†’ [flatten processor] â†’ Sink (envelope â†’ bytes)
</code></pre>
<p>All envelope formats work as expected:</p>
<pre><code class="language-json">// Native
{ "before": null, "after": { "customer__id": 1, "customer__address__city": "Berlin" }, "op": "c" }

// Debezium
{ "payload": { "before": null, "after": { "customer__id": 1, "customer__address__city": "Berlin" }, "op": "c" } }

// CloudEvents
{ "specversion": "1.0", ..., "data": { "before": null, "after": { "customer__id": 1, "customer__address__city": "Berlin" } } }
</code></pre>
<p><strong>Outbox + <code>raw_payload: true</code></strong></p>
<p>When the outbox processor is configured with <code>raw_payload: true</code>, the sink delivers <code>event.after</code> directly, bypassing the envelope entirely. If the flatten processor runs after outbox, the raw payload delivered to the sink is the flattened object â€” which is the intended behavior for analytics sinks that canâ€™t handle nested JSON.</p>
<pre><code class="language-yaml">processors:
  - type: outbox
    topic: "${aggregate_type}.${event_type}"
    raw_payload: true       # sink delivers event.after directly, no envelope
  - type: flatten
    id: flat
    separator: "__"
    empty_list: drop
</code></pre>
<p>The flatten processor runs second, so by the time the sink delivers the raw payload it is already flat.</p>
<h3 id="analytics-sink-example"><a class="header" href="#analytics-sink-example">Analytics sink example</a></h3>
<p>When sending to column-oriented sinks (ClickHouse, BigQuery, S3 Parquet) that donâ€™t handle nested JSON:</p>
<pre><code class="language-yaml">processors:
  - type: flatten
    id: flat
    separator: "__"
    lists: index          # expand arrays to indexed columns
    empty_object: drop    # remove sparse marker objects
    empty_list: drop      # remove empty arrays
</code></pre>
<h2 id="processor-chain"><a class="header" href="#processor-chain">Processor Chain</a></h2>
<p>Processors execute in order. Events flow through each processor sequentially:</p>
<pre><code>Source â†’ [Processor 1] â†’ [Processor 2] â†’ ... â†’ Sinks
</code></pre>
<p>Each processor receives a <code>Vec&lt;Event&gt;</code> and returns a <code>Vec&lt;Event&gt;</code>. This means processors can:</p>
<ul>
<li><strong>Transform</strong>: Modify event fields in place</li>
<li><strong>Filter</strong>: Return a subset of events (drop unwanted ones)</li>
<li><strong>Fan-out</strong>: Return more events than received</li>
<li><strong>Route</strong>: Set <code>event.routing.topic</code> to override sink defaults</li>
<li><strong>Enrich</strong>: Add headers via <code>event.routing.headers</code></li>
</ul>
<p>Non-outbox events always pass through the outbox processor untouched, so it is safe to combine it with JavaScript processors in any order.</p>
<h2 id="adding-custom-processors"><a class="header" href="#adding-custom-processors">Adding Custom Processors</a></h2>
<p>The processor interface is a simple trait:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async_trait]
pub trait Processor: Send + Sync {
    fn id(&amp;self) -&gt; &amp;str;
    async fn process(&amp;self, events: Vec&lt;Event&gt;) -&gt; Result&lt;Vec&lt;Event&gt;&gt;;
}
<span class="boring">}</span></code></pre>
<p>To add a new processor:</p>
<ol>
<li>Implement the <code>Processor</code> trait in <code>crates/processors</code></li>
<li>Add a config variant to <code>ProcessorCfg</code> in <code>deltaforge-config</code></li>
<li>Register the build step in <code>build_processors()</code></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="sinks-1"><a class="header" href="#sinks-1">Sinks</a></h1>
<p>Sinks receive batches from the coordinator after processors run. Each sink lives under <code>spec.sinks</code> and can be marked as required or best-effort via the <code>required</code> flag. Checkpoint behavior is governed by the pipelineâ€™s commit policy.</p>
<h2 id="envelope-and-encoding-1"><a class="header" href="#envelope-and-encoding-1">Envelope and Encoding</a></h2>
<p>All sinks support configurable <strong>envelope formats</strong> and <strong>wire encodings</strong>. See the <a href="#envelopes-and-encodings">Envelopes and Encodings</a> page for detailed documentation.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Option</th><th>Values</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>envelope</code></td><td><code>native</code>, <code>debezium</code>, <code>cloudevents</code></td><td><code>native</code></td><td>Output JSON structure</td></tr>
<tr><td><code>encoding</code></td><td><code>json</code></td><td><code>json</code></td><td>Wire format</td></tr>
</tbody>
</table>
</div>
<p><strong>Quick example:</strong></p>
<pre><code class="language-yaml">sinks:
  - type: kafka
    config:
      id: events-kafka
      brokers: localhost:9092
      topic: events
      envelope:
        type: cloudevents
        type_prefix: "com.example.cdc"
      encoding: json
</code></pre>
<h2 id="available-sinks"><a class="header" href="#available-sinks">Available Sinks</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th style="text-align: center"></th><th style="text-align: left">Sink</th><th style="text-align: left">Description</th></tr>
</thead>
<tbody>
<tr><td style="text-align: center"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/apachekafka/apachekafka-original.svg" width="24" height="24"></td><td style="text-align: left"><a href="#kafka-sink"><code>kafka</code></a></td><td style="text-align: left">Kafka producer sink</td></tr>
<tr><td style="text-align: center"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/nats/nats-original.svg" width="24" height="24"></td><td style="text-align: left"><a href="#nats-sink"><code>nats</code></a></td><td style="text-align: left">NATS JetStream sink</td></tr>
<tr><td style="text-align: center"><img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/redis/redis-original.svg" width="24" height="24"></td><td style="text-align: left"><a href="#redis-sink"><code>redis</code></a></td><td style="text-align: left">Redis stream sink</td></tr>
</tbody>
</table>
</div>
<h2 id="multiple-sinks-in-one-pipeline"><a class="header" href="#multiple-sinks-in-one-pipeline">Multiple sinks in one pipeline</a></h2>
<p>You can combine multiple sinks in one pipeline to fan out events to different destinations. However, multi-sink pipelines introduce complexity that requires careful consideration.</p>
<h3 id="why-multiple-sinks-are-challenging"><a class="header" href="#why-multiple-sinks-are-challenging">Why multiple sinks are challenging</a></h3>
<p><strong>Different performance characteristics</strong>: Kafka might handle 100K events/sec while a downstream HTTP webhook processes 100/sec. The slowest sink becomes the bottleneck for the entire pipeline.</p>
<p><strong>Independent failure modes</strong>: Each sink can fail independently. Redis might be healthy while Kafka experiences broker failures. Without proper handling, a single sink failure could block the entire pipeline or cause data loss.</p>
<p><strong>No distributed transactions</strong>: DeltaForge cannot atomically commit across heterogeneous systems. If Kafka succeeds but Redis fails mid-batch, you face a choice: retry Redis (risking duplicates in Kafka) or skip Redis (losing data there).</p>
<p><strong>Checkpoint semantics</strong>: The checkpoint represents â€œhow far weâ€™ve processed from the source.â€ With multiple sinks, when is it safe to advance? After one sink succeeds? All of them? A majority?</p>
<p>Read the <code>required</code> and <code>commit_policy</code> sections below for options to manage these challenges.</p>
<h3 id="the-required-flag"><a class="header" href="#the-required-flag">The <code>required</code> flag</a></h3>
<p>The <code>required</code> flag on each sink determines whether that sink must acknowledge successful delivery before the checkpoint advances:</p>
<pre><code class="language-yaml">sinks:
  - type: kafka
    config:
      id: primary-kafka
      required: true    # Must succeed for checkpoint to advance
      
  - type: redis
    config:
      id: cache-redis
      required: false   # Best-effort; failures don't block checkpoint
</code></pre>
<p><strong>When <code>required: true</code></strong> (default): The sink must acknowledge the batch before the checkpoint can advance. If this sink fails, the pipeline blocks and retries until it succeeds or the operator intervenes.</p>
<p><strong>When <code>required: false</code></strong>: The sink is best-effort. Failures are logged but donâ€™t prevent the checkpoint from advancing. Use this for non-critical destinations where some data loss is acceptable.</p>
<h3 id="commit-policy-2"><a class="header" href="#commit-policy-2">Commit policy</a></h3>
<p>The <code>commit_policy</code> works with the <code>required</code> flag to determine checkpoint behavior:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Policy</th><th>Behavior</th></tr>
</thead>
<tbody>
<tr><td><code>all</code></td><td>Every sink (regardless of <code>required</code> flag) must acknowledge</td></tr>
<tr><td><code>required</code></td><td>Only sinks with <code>required: true</code> must acknowledge (default)</td></tr>
<tr><td><code>quorum</code></td><td>At least N sinks must acknowledge</td></tr>
</tbody>
</table>
</div>
<pre><code class="language-yaml">commit_policy:
  mode: required   # Only wait for required sinks

sinks:
  - type: kafka
    config:
      required: true   # Checkpoint waits for this
  - type: redis  
    config:
      required: false  # Checkpoint doesn't wait for this
  - type: nats
    config:
      required: true   # Checkpoint waits for this
</code></pre>
<h3 id="practical-patterns"><a class="header" href="#practical-patterns">Practical patterns</a></h3>
<p><strong>Primary + secondary</strong>: One critical sink (Kafka for durability) marked <code>required: true</code>, with secondary sinks (Redis for caching, testing or experimentation) marked <code>required: false</code>.</p>
<p><strong>Quorum for redundancy</strong>: Three sinks with <code>commit_policy.mode: quorum</code> and <code>quorum: 2</code>. Checkpoint advances when any two succeed, providing fault tolerance.</p>
<p><strong>All-or-nothing</strong>: Use <code>commit_policy.mode: all</code> when every destination is critical and you need the strongest consistency guarantee (but affecting rate of delivery).</p>
<h3 id="multi-format-fan-out"><a class="header" href="#multi-format-fan-out">Multi-format fan-out</a></h3>
<p>For sending the same events to different consumers that expect different formats:</p>
<pre><code class="language-yaml">sinks:
  # Kafka Connect expects Debezium format
  - type: kafka
    config:
      id: connect-sink
      brokers: ${KAFKA_BROKERS}
      topic: connect-events
      envelope:
        type: debezium
      required: true

  # Lambda expects CloudEvents
  - type: kafka
    config:
      id: lambda-sink
      brokers: ${KAFKA_BROKERS}
      topic: lambda-events
      envelope:
        type: cloudevents
        type_prefix: "com.acme.cdc"
      required: false

  # Analytics wants raw events
  - type: redis
    config:
      id: analytics-redis
      uri: ${REDIS_URI}
      stream: analytics
      envelope:
        type: native
      required: false
</code></pre>
<p>This allows each consumer to receive events in their preferred format without post-processing.</p>
<div style="break-before: page; page-break-before: always;"></div>
<p align="center">
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/redis/redis-original.svg" width="80" height="80" alt="Redis">
</p>

<h1 id="redis-sink"><a class="header" href="#redis-sink">Redis sink</a></h1>
<p>The Redis sink publishes events to a Redis Stream for real-time consumption with consumer groups.</p>
<h2 id="when-to-use-redis"><a class="header" href="#when-to-use-redis">When to use Redis</a></h2>
<p>Redis Streams shine when you need low-latency event delivery with simple operational requirements and built-in consumer group support.</p>
<h3 id="real-world-applications"><a class="header" href="#real-world-applications">Real-world applications</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Use Case</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><strong>Real-time notifications</strong></td><td>Push database changes instantly to WebSocket servers for live UI updates</td></tr>
<tr><td><strong>Cache invalidation</strong></td><td>Trigger cache eviction when source records change; keep Redis cache consistent</td></tr>
<tr><td><strong>Session synchronization</strong></td><td>Replicate user session changes across application instances in real-time</td></tr>
<tr><td><strong>Rate limiting state</strong></td><td>Stream counter updates for distributed rate limiting decisions</td></tr>
<tr><td><strong>Live dashboards</strong></td><td>Feed real-time metrics and KPIs to dashboard backends</td></tr>
<tr><td><strong>Job queuing</strong></td><td>Use CDC events to trigger background job processing with consumer groups</td></tr>
<tr><td><strong>Feature flags</strong></td><td>Propagate feature flag changes instantly across all application instances</td></tr>
</tbody>
</table>
</div>
<h3 id="pros-and-cons"><a class="header" href="#pros-and-cons">Pros and cons</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Pros</th><th>Cons</th></tr>
</thead>
<tbody>
<tr><td>âœ… <strong>Ultra-low latency</strong> - Sub-millisecond publish; ideal for real-time apps</td><td>âŒ <strong>Memory-bound</strong> - All data in RAM; expensive for high-volume retention</td></tr>
<tr><td>âœ… <strong>Simple operations</strong> - Single binary, minimal configuration</td><td>âŒ <strong>Limited retention</strong> - Not designed for long-term event storage</td></tr>
<tr><td>âœ… <strong>Consumer groups</strong> - Built-in competing consumers with acknowledgements</td><td>âŒ <strong>Durability trade-offs</strong> - AOF/RDB persistence has limitations</td></tr>
<tr><td>âœ… <strong>Familiar tooling</strong> - redis-cli, widespread client library support</td><td>âŒ <strong>Single-threaded</strong> - CPU-bound for very high throughput</td></tr>
<tr><td>âœ… <strong>Versatile</strong> - Combine with caching, pub/sub, and data structures</td><td>âŒ <strong>No native replay</strong> - XRANGE exists but no offset management</td></tr>
<tr><td>âœ… <strong>Atomic operations</strong> - MULTI/EXEC for transactional guarantees</td><td>âŒ <strong>Cluster complexity</strong> - Sharding requires careful key design</td></tr>
</tbody>
</table>
</div>
<h2 id="configuration-5"><a class="header" href="#configuration-5">Configuration</a></h2>
<table>
<tr>
<td>
<pre><code class="language-yaml">sinks:
  - type: redis
    config:
      id: orders-redis
      uri: ${REDIS_URI}
      stream: orders.events
      required: true
</code></pre>
</td>
<td>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>id</code></td><td>string</td><td>â€”</td><td>Sink identifier</td></tr>
<tr><td><code>uri</code></td><td>string</td><td>â€”</td><td>Redis connection URI</td></tr>
<tr><td><code>stream</code></td><td>string</td><td>â€”</td><td>Redis stream key</td></tr>
<tr><td><code>required</code></td><td>bool</td><td><code>true</code></td><td>Gates checkpoints</td></tr>
</tbody>
</table>
</div>
</td>
</tr>

</table>

<h2 id="consuming-events"><a class="header" href="#consuming-events">Consuming events</a></h2>
<h3 id="consumer-groups-recommended"><a class="header" href="#consumer-groups-recommended">Consumer groups (recommended)</a></h3>
<pre><code class="language-bash"># Create consumer group
redis-cli XGROUP CREATE orders.events mygroup $ MKSTREAM

# Read as consumer (blocking)
redis-cli XREADGROUP GROUP mygroup consumer1 BLOCK 5000 COUNT 10 STREAMS orders.events &gt;

# Acknowledge processing
redis-cli XACK orders.events mygroup 1234567890123-0

# Check pending (unacknowledged) messages
redis-cli XPENDING orders.events mygroup
</code></pre>
<h3 id="simple-subscription"><a class="header" href="#simple-subscription">Simple subscription</a></h3>
<pre><code class="language-bash"># Read latest entries
redis-cli XREAD COUNT 10 STREAMS orders.events 0-0

# Block for new entries
redis-cli XREAD BLOCK 5000 STREAMS orders.events $
</code></pre>
<h3 id="go-consumer-example"><a class="header" href="#go-consumer-example">Go consumer example</a></h3>
<pre><code class="language-go">import "github.com/redis/go-redis/v9"

rdb := redis.NewClient(&amp;redis.Options{Addr: "localhost:6379"})

// Create consumer group (once)
rdb.XGroupCreateMkStream(ctx, "orders.events", "mygroup", "0")

for {
    streams, err := rdb.XReadGroup(ctx, &amp;redis.XReadGroupArgs{
        Group:    "mygroup",
        Consumer: "worker1",
        Streams:  []string{"orders.events", "&gt;"},
        Count:    10,
        Block:    5 * time.Second,
    }).Result()
    
    if err != nil {
        continue
    }
    
    for _, stream := range streams {
        for _, msg := range stream.Messages {
            var event Event
            json.Unmarshal([]byte(msg.Values["event"].(string)), &amp;event)
            process(event)
            rdb.XAck(ctx, "orders.events", "mygroup", msg.ID)
        }
    }
}
</code></pre>
<h3 id="rust-consumer-example"><a class="header" href="#rust-consumer-example">Rust consumer example</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use redis::AsyncCommands;

let client = redis::Client::open("redis://localhost:6379")?;
let mut con = client.get_async_connection().await?;

// Create consumer group
let _: () = redis::cmd("XGROUP")
    .arg("CREATE").arg("orders.events").arg("mygroup").arg("0").arg("MKSTREAM")
    .query_async(&amp;mut con).await.unwrap_or(());

loop {
    let results: Vec&lt;StreamReadReply&gt; = con.xread_options(
        &amp;["orders.events"],
        &amp;["&gt;"],
        &amp;StreamReadOptions::default()
            .group("mygroup", "worker1")
            .count(10)
            .block(5000)
    ).await?;
    
    for stream in results {
        for msg in stream.ids {
            let event: Event = serde_json::from_str(&amp;msg.map["event"])?;
            process(event);
            con.xack("orders.events", "mygroup", &amp;[&amp;msg.id]).await?;
        }
    }
}
<span class="boring">}</span></code></pre>
<h3 id="python-consumer-example"><a class="header" href="#python-consumer-example">Python consumer example</a></h3>
<pre><code class="language-python">import redis
import json

r = redis.Redis.from_url("redis://localhost:6379")

# Create consumer group (once)
try:
    r.xgroup_create("orders.events", "mygroup", id="0", mkstream=True)
except redis.ResponseError:
    pass  # Group already exists

# Consume events
while True:
    events = r.xreadgroup("mygroup", "worker1", {"orders.events": "&gt;"}, count=10, block=5000)
    for stream, messages in events:
        for msg_id, data in messages:
            event = json.loads(data[b"event"])
            process(event)
            r.xack("orders.events", "mygroup", msg_id)
</code></pre>
<h2 id="failure-modes"><a class="header" href="#failure-modes">Failure modes</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Failure</th><th>Symptoms</th><th>DeltaForge behavior</th><th>Resolution</th></tr>
</thead>
<tbody>
<tr><td><strong>Server unavailable</strong></td><td>Connection refused</td><td>Retries with backoff; blocks checkpoint</td><td>Restore Redis; check network</td></tr>
<tr><td><strong>Authentication failure</strong></td><td><code>NOAUTH</code> / <code>WRONGPASS</code></td><td>Fails fast, no retry</td><td>Fix auth details in URI</td></tr>
<tr><td><strong>OOM (Out of Memory)</strong></td><td><code>OOM command not allowed</code></td><td>Fails batch; retries</td><td>Increase <code>maxmemory</code>; enable eviction or trim streams</td></tr>
<tr><td><strong>Stream doesnâ€™t exist</strong></td><td>Auto-created by XADD</td><td>No failure</td><td>N/A (XADD creates stream)</td></tr>
<tr><td><strong>Connection timeout</strong></td><td>Command hangs</td><td>Timeout after configured duration</td><td>Check network; increase timeout</td></tr>
<tr><td><strong>Cluster MOVED/ASK</strong></td><td>Redirect errors</td><td>Automatic redirect (if cluster mode)</td><td>Ensure cluster client configured</td></tr>
<tr><td><strong>Replication lag</strong></td><td>Writes to replica fail</td><td>Fails with <code>READONLY</code></td><td>Write to master only</td></tr>
<tr><td><strong>Max stream length</strong></td><td>If MAXLEN enforced</td><td>Oldest entries trimmed</td><td>Expected behavior; not a failure</td></tr>
<tr><td><strong>Network partition</strong></td><td>Intermittent timeouts</td><td>Retries; may have gaps</td><td>Restore network</td></tr>
</tbody>
</table>
</div>
<h3 id="failure-scenarios-and-data-guarantees"><a class="header" href="#failure-scenarios-and-data-guarantees">Failure scenarios and data guarantees</a></h3>
<p><strong>Redis OOM during batch delivery</strong></p>
<ol>
<li>DeltaForge sends batch of 100 events via pipeline</li>
<li>50 events written, Redis hits maxmemory</li>
<li>Pipeline fails atomically (all or nothing per pipeline)</li>
<li>DeltaForge retries entire batch</li>
<li>If OOM persists: batch blocked until memory available</li>
<li>Checkpoint only saved after ALL events acknowledged</li>
</ol>
<p><strong>DeltaForge crash after XADD, before checkpoint</strong></p>
<ol>
<li>Batch written to Redis stream successfully</li>
<li>DeltaForge crashes before saving checkpoint</li>
<li>On restart: replays from last checkpoint</li>
<li>Result: Duplicate events in stream (at-least-once)</li>
<li>Consumer must handle idempotently (check event.id)</li>
</ol>
<p><strong>Redis failover (Sentinel/Cluster)</strong></p>
<ol>
<li>Master fails, Sentinel promotes replica</li>
<li>In-flight XADD may fail with connection error</li>
<li>DeltaForge reconnects to new master</li>
<li>Retries failed batch</li>
<li>Possible duplicates if original write succeeded</li>
</ol>
<h3 id="handling-duplicates-in-consumers"><a class="header" href="#handling-duplicates-in-consumers">Handling duplicates in consumers</a></h3>
<pre><code class="language-python"># Idempotent consumer using event ID
processed_ids = set()  # Or use Redis SET for distributed dedup

for msg_id, data in messages:
    event = json.loads(data[b"event"])
    event_id = event["id"]
    
    if event_id in processed_ids:
        r.xack("orders.events", "mygroup", msg_id)
        continue  # Skip duplicate
    
    process(event)
    processed_ids.add(event_id)
    r.xack("orders.events", "mygroup", msg_id)
</code></pre>
<h2 id="monitoring"><a class="header" href="#monitoring">Monitoring</a></h2>
<p>DeltaForge exposes these metrics for Redis sink monitoring:</p>
<pre><code class="language-yaml"># DeltaForge sink metrics (exposed at /metrics on port 9000)
deltaforge_sink_events_total{pipeline,sink}      # Events delivered
deltaforge_sink_batch_total{pipeline,sink}       # Batches delivered  
deltaforge_sink_latency_seconds{pipeline,sink}   # Delivery latency histogram
deltaforge_stage_latency_seconds{pipeline,stage="sink"}  # Stage timing
</code></pre>
<p>For Redis server visibility, use Redisâ€™s built-in monitoring:</p>
<pre><code class="language-bash"># Monitor commands in real-time
redis-cli MONITOR

# Get server stats
redis-cli INFO stats

# Check memory usage
redis-cli INFO memory

# Stream-specific info
redis-cli XINFO STREAM orders.events
redis-cli XINFO GROUPS orders.events
</code></pre>
<h2 id="stream-management"><a class="header" href="#stream-management">Stream management</a></h2>
<pre><code class="language-bash"># Check stream length
redis-cli XLEN orders.events

# Trim to last 10000 entries (approximate)
redis-cli XTRIM orders.events MAXLEN ~ 10000

# Trim to exact length
redis-cli XTRIM orders.events MAXLEN 10000

# View consumer group info
redis-cli XINFO GROUPS orders.events

# Check pending messages
redis-cli XPENDING orders.events mygroup

# Claim stuck messages (after 60 seconds)
redis-cli XCLAIM orders.events mygroup worker2 60000 &lt;message-id&gt;

# Delete processed messages (careful!)
redis-cli XDEL orders.events &lt;message-id&gt;
</code></pre>
<h2 id="notes-1"><a class="header" href="#notes-1">Notes</a></h2>
<ul>
<li>Redis Streams provide at-least-once delivery with consumer group acknowledgements</li>
<li>Use <code>MAXLEN ~</code> trimming to prevent unbounded memory growth (approximate is faster)</li>
<li>Consider Redis Cluster for horizontal scaling with multiple streams</li>
<li>Combine with Redis pub/sub for fan-out to ephemeral subscribers</li>
<li>For durability, enable AOF persistence with <code>appendfsync everysec</code> or <code>always</code></li>
<li>Monitor memory usage closely; Redis will reject writes when <code>maxmemory</code> is reached</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<p align="center">
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/apachekafka/apachekafka-original.svg" width="80" height="80" alt="Kafka">
</p>

<h1 id="kafka-sink"><a class="header" href="#kafka-sink">Kafka sink</a></h1>
<p>The Kafka sink publishes batches to a Kafka topic using <code>rdkafka</code>.</p>
<h2 id="when-to-use-kafka"><a class="header" href="#when-to-use-kafka">When to use Kafka</a></h2>
<p>Kafka excels as the backbone for event-driven architectures where durability, ordering, and replay capabilities are critical.</p>
<h3 id="real-world-applications-1"><a class="header" href="#real-world-applications-1">Real-world applications</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Use Case</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><strong>Event sourcing</strong></td><td>Store all state changes as an immutable log; rebuild application state by replaying events</td></tr>
<tr><td><strong>Microservices integration</strong></td><td>Decouple services with async messaging; each service consumes relevant topics</td></tr>
<tr><td><strong>Real-time analytics pipelines</strong></td><td>Feed CDC events to Spark, Flink, or ksqlDB for streaming transformations</td></tr>
<tr><td><strong>Data lake ingestion</strong></td><td>Stream database changes to S3/HDFS via Kafka Connect for analytics and ML</td></tr>
<tr><td><strong>Audit logging</strong></td><td>Capture every database mutation for compliance, debugging, and forensics</td></tr>
<tr><td><strong>Cross-datacenter replication</strong></td><td>Use MirrorMaker 2 to replicate topics across regions for DR</td></tr>
</tbody>
</table>
</div>
<h3 id="pros-and-cons-1"><a class="header" href="#pros-and-cons-1">Pros and cons</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Pros</th><th>Cons</th></tr>
</thead>
<tbody>
<tr><td>âœ… <strong>Durability</strong> - Configurable replication ensures no data loss</td><td>âŒ <strong>Operational complexity</strong> - Requires ZooKeeper/KRaft, careful tuning</td></tr>
<tr><td>âœ… <strong>Ordering guarantees</strong> - Per-partition ordering with consumer groups</td><td>âŒ <strong>Latency</strong> - Batching and replication add milliseconds of delay</td></tr>
<tr><td>âœ… <strong>Replay capability</strong> - Configurable retention allows reprocessing</td><td>âŒ <strong>Resource intensive</strong> - High disk I/O and memory requirements</td></tr>
<tr><td>âœ… <strong>Ecosystem</strong> - Connect, Streams, Schema Registry, ksqlDB</td><td>âŒ <strong>Learning curve</strong> - Partitioning, offsets, consumer groups to master</td></tr>
<tr><td>âœ… <strong>Throughput</strong> - Handles millions of messages per second</td><td>âŒ <strong>Cold start</strong> - Cluster setup and topic configuration overhead</td></tr>
<tr><td>âœ… <strong>Exactly-once semantics</strong> - Transactions for critical workloads</td><td>âŒ <strong>Cost</strong> - Managed services can be expensive at scale</td></tr>
</tbody>
</table>
</div>
<h2 id="configuration-6"><a class="header" href="#configuration-6">Configuration</a></h2>
<table>
<tr>
<td>
<pre><code class="language-yaml">sinks:
  - type: kafka
    config:
      id: orders-kafka
      brokers: ${KAFKA_BROKERS}
      topic: orders
      required: true
      exactly_once: false
      client_conf:
        message.timeout.ms: "5000"
        acks: "all"
</code></pre>
</td>
<td>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>id</code></td><td>string</td><td>â€”</td><td>Sink identifier</td></tr>
<tr><td><code>brokers</code></td><td>string</td><td>â€”</td><td>Comma-separated broker list</td></tr>
<tr><td><code>topic</code></td><td>string</td><td>â€”</td><td>Destination topic</td></tr>
<tr><td><code>required</code></td><td>bool</td><td><code>true</code></td><td>Gates checkpoints</td></tr>
<tr><td><code>exactly_once</code></td><td>bool</td><td><code>false</code></td><td>Enable EOS semantics</td></tr>
<tr><td><code>client_conf</code></td><td>map</td><td><code>{}</code></td><td>librdkafka overrides</td></tr>
</tbody>
</table>
</div>
</td>
</tr>

</table>

<h2 id="recommended-client_conf-settings"><a class="header" href="#recommended-client_conf-settings">Recommended client_conf settings</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Setting</th><th>Recommended</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>acks</code></td><td><code>all</code></td><td>Wait for all replicas for durability</td></tr>
<tr><td><code>message.timeout.ms</code></td><td><code>30000</code></td><td>Total time to deliver a message</td></tr>
<tr><td><code>retries</code></td><td><code>2147483647</code></td><td>Retry indefinitely (with backoff)</td></tr>
<tr><td><code>enable.idempotence</code></td><td><code>true</code></td><td>Prevent duplicates on retry</td></tr>
<tr><td><code>compression.type</code></td><td><code>lz4</code></td><td>Balance between CPU and bandwidth</td></tr>
</tbody>
</table>
</div>
<h2 id="consuming-events-1"><a class="header" href="#consuming-events-1">Consuming events</a></h2>
<h3 id="kafka-cli"><a class="header" href="#kafka-cli">Kafka CLI</a></h3>
<pre><code class="language-bash"># Consume from beginning
kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic orders \
  --from-beginning

# Consume with consumer group
kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic orders \
  --group deltaforge-consumers
</code></pre>
<h3 id="go-consumer-example-1"><a class="header" href="#go-consumer-example-1">Go consumer example</a></h3>
<pre><code class="language-go">config := sarama.NewConfig()
config.Consumer.Group.Rebalance.Strategy = sarama.BalanceStrategyRoundRobin
config.Consumer.Offsets.Initial = sarama.OffsetOldest

group, _ := sarama.NewConsumerGroup([]string{"localhost:9092"}, "my-group", config)

for {
    err := group.Consume(ctx, []string{"orders"}, &amp;handler{})
    if err != nil {
        log.Printf("Consumer error: %v", err)
    }
}

type handler struct{}

func (h *handler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {
    for msg := range claim.Messages() {
        var event Event
        json.Unmarshal(msg.Value, &amp;event)
        process(event)
        session.MarkMessage(msg, "")
    }
    return nil
}
</code></pre>
<h3 id="rust-consumer-example-1"><a class="header" href="#rust-consumer-example-1">Rust consumer example</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use rdkafka::consumer::{Consumer, StreamConsumer};
use rdkafka::Message;

let consumer: StreamConsumer = ClientConfig::new()
    .set("bootstrap.servers", "localhost:9092")
    .set("group.id", "my-group")
    .set("auto.offset.reset", "earliest")
    .create()?;

consumer.subscribe(&amp;["orders"])?;

loop {
    match consumer.recv().await {
        Ok(msg) =&gt; {
            let payload = msg.payload_view::&lt;str&gt;().unwrap()?;
            let event: Event = serde_json::from_str(payload)?;
            process(event);
            consumer.commit_message(&amp;msg, CommitMode::Async)?;
        }
        Err(e) =&gt; eprintln!("Kafka error: {}", e),
    }
}
<span class="boring">}</span></code></pre>
<h3 id="python-consumer-example-1"><a class="header" href="#python-consumer-example-1">Python consumer example</a></h3>
<pre><code class="language-python">from kafka import KafkaConsumer
import json

consumer = KafkaConsumer(
    'orders',
    bootstrap_servers=['localhost:9092'],
    group_id='my-group',
    auto_offset_reset='earliest',
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

for message in consumer:
    event = message.value
    process(event)
    consumer.commit()
</code></pre>
<h2 id="failure-modes-1"><a class="header" href="#failure-modes-1">Failure modes</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Failure</th><th>Symptoms</th><th>DeltaForge behavior</th><th>Resolution</th></tr>
</thead>
<tbody>
<tr><td><strong>Broker unavailable</strong></td><td>Connection refused, timeout</td><td>Retries with backoff; blocks checkpoint</td><td>Restore broker; check network</td></tr>
<tr><td><strong>Topic not found</strong></td><td><code>UnknownTopicOrPartition</code></td><td>Fails batch; retries</td><td>Create topic or enable auto-create</td></tr>
<tr><td><strong>Authentication failure</strong></td><td><code>SaslAuthenticationFailed</code></td><td>Fails fast, no retry</td><td>Fix credentials in config</td></tr>
<tr><td><strong>Authorization failure</strong></td><td><code>TopicAuthorizationFailed</code></td><td>Fails fast, no retry</td><td>Grant ACLs for producer</td></tr>
<tr><td><strong>Message too large</strong></td><td><code>MessageSizeTooLarge</code></td><td>Fails message permanently</td><td>Increase <code>message.max.bytes</code> or filter large events</td></tr>
<tr><td><strong>Leader election</strong></td><td><code>NotLeaderForPartition</code></td><td>Automatic retry after metadata refresh</td><td>Wait for election; usually transient</td></tr>
<tr><td><strong>Disk full</strong></td><td><code>KafkaStorageException</code></td><td>Retries indefinitely</td><td>Add disk space; purge old segments</td></tr>
<tr><td><strong>Network partition</strong></td><td>Timeouts, partial failures</td><td>Retries; may produce duplicates</td><td>Restore network; idempotence prevents dups</td></tr>
<tr><td><strong>SSL/TLS errors</strong></td><td>Handshake failures</td><td>Fails fast</td><td>Fix certificates, verify truststore</td></tr>
</tbody>
</table>
</div>
<h3 id="failure-scenarios-and-data-guarantees-1"><a class="header" href="#failure-scenarios-and-data-guarantees-1">Failure scenarios and data guarantees</a></h3>
<p><strong>Broker failure during batch delivery</strong></p>
<ol>
<li>DeltaForge sends batch of 100 events</li>
<li>50 events delivered, broker crashes</li>
<li>rdkafka detects failure, retries remaining 50</li>
<li>If idempotence enabled: no duplicates</li>
<li>If not: possible duplicates of events near failure point</li>
<li>Checkpoint only saved after ALL events acknowledged</li>
</ol>
<p><strong>DeltaForge crash after Kafka ack, before checkpoint</strong></p>
<ol>
<li>Batch delivered to Kafka successfully</li>
<li>DeltaForge crashes before saving checkpoint</li>
<li>On restart: replays from last checkpoint</li>
<li>Result: Duplicate events in Kafka (at-least-once)</li>
<li>Consumer must handle idempotently</li>
</ol>
<h3 id="monitoring-recommendations"><a class="header" href="#monitoring-recommendations">Monitoring recommendations</a></h3>
<p>DeltaForge exposes these metrics for Kafka sink monitoring:</p>
<pre><code class="language-yaml"># DeltaForge sink metrics (exposed at /metrics on port 9000)
deltaforge_sink_events_total{pipeline,sink}      # Events delivered
deltaforge_sink_batch_total{pipeline,sink}       # Batches delivered
deltaforge_sink_latency_seconds{pipeline,sink}   # Delivery latency histogram
deltaforge_stage_latency_seconds{pipeline,stage="sink"}  # Stage timing
</code></pre>
<p>For deeper Kafka broker visibility, monitor your Kafka cluster directly:</p>
<ul>
<li>Broker metrics via JMX or Kafkaâ€™s built-in metrics</li>
<li>Consumer lag via <code>kafka-consumer-groups.sh</code></li>
<li>Topic throughput via broker dashboards</li>
</ul>
<blockquote>
<p><strong>Note</strong>: Internal <code>rdkafka</code> producer statistics (message queues, broker RTT, etc.) are not currently exposed by DeltaForge. This is a potential future enhancement.</p>
</blockquote>
<h2 id="notes-2"><a class="header" href="#notes-2">Notes</a></h2>
<ul>
<li>Combine Kafka with other sinks to fan out data; use commit policy to control checkpoint behavior</li>
<li>For exactly-once semantics, ensure your Kafka cluster supports transactions (2.5+)</li>
<li>Adjust <code>client_conf</code> for durability (<code>acks=all</code>) or performance based on your requirements</li>
<li>Consider partitioning strategy for ordering guarantees within partitions</li>
<li>Enable <code>enable.idempotence=true</code> to prevent duplicates during retries</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<p align="center">
  <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/nats/nats-original.svg" width="80" height="80" alt="NATS">
</p>

<h1 id="nats-sink"><a class="header" href="#nats-sink">NATS sink</a></h1>
<p>The NATS sink publishes events to a NATS JetStream stream for durable, at-least-once delivery.</p>
<h2 id="when-to-use-nats"><a class="header" href="#when-to-use-nats">When to use NATS</a></h2>
<p>NATS JetStream is ideal when you need a lightweight, high-performance messaging system with persistence, without the operational overhead of Kafka.</p>
<h3 id="real-world-applications-2"><a class="header" href="#real-world-applications-2">Real-world applications</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Use Case</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><strong>Edge computing</strong></td><td>Lightweight footprint perfect for IoT gateways and edge nodes syncing to cloud</td></tr>
<tr><td><strong>Microservices mesh</strong></td><td>Request-reply and pub/sub patterns with automatic load balancing</td></tr>
<tr><td><strong>Multi-cloud sync</strong></td><td>Leaf nodes and superclusters for seamless cross-cloud data replication</td></tr>
<tr><td><strong>Kubernetes-native events</strong></td><td>NATS Operator for cloud-native deployment; sidecar-friendly architecture</td></tr>
<tr><td><strong>Real-time gaming</strong></td><td>Low-latency state synchronization for multiplayer game servers</td></tr>
<tr><td><strong>Financial data feeds</strong></td><td>Stream market data with subject-based routing and wildcards</td></tr>
<tr><td><strong>Command and control</strong></td><td>Distribute configuration changes and commands to distributed systems</td></tr>
</tbody>
</table>
</div>
<h3 id="pros-and-cons-2"><a class="header" href="#pros-and-cons-2">Pros and cons</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Pros</th><th>Cons</th></tr>
</thead>
<tbody>
<tr><td>âœ… <strong>Lightweight</strong> - Single binary ~20MB; minimal resource footprint</td><td>âŒ <strong>Smaller ecosystem</strong> - Fewer connectors and integrations than Kafka</td></tr>
<tr><td>âœ… <strong>Simple operations</strong> - Zero external dependencies; easy clustering</td><td>âŒ <strong>Younger persistence</strong> - JetStream newer than Kafkaâ€™s battle-tested log</td></tr>
<tr><td>âœ… <strong>Low latency</strong> - Sub-millisecond message delivery</td><td>âŒ <strong>Community size</strong> - Smaller community than Kafka or Redis</td></tr>
<tr><td>âœ… <strong>Flexible patterns</strong> - Pub/sub, queues, request-reply, streams</td><td>âŒ <strong>Tooling maturity</strong> - Fewer monitoring and management tools</td></tr>
<tr><td>âœ… <strong>Subject hierarchy</strong> - Powerful wildcard routing (<code>orders.&gt;</code>, <code>*.events</code>)</td><td>âŒ <strong>Learning curve</strong> - JetStream concepts differ from traditional queues</td></tr>
<tr><td>âœ… <strong>Multi-tenancy</strong> - Built-in accounts and security isolation</td><td>âŒ <strong>Less enterprise adoption</strong> - Fewer case studies at massive scale</td></tr>
<tr><td>âœ… <strong>Cloud-native</strong> - Designed for Kubernetes and distributed systems</td><td></td></tr>
</tbody>
</table>
</div>
<h2 id="configuration-7"><a class="header" href="#configuration-7">Configuration</a></h2>
<table>
<tr>
<td>
<pre><code class="language-yaml">sinks:
  - type: nats
    config:
      id: orders-nats
      url: ${NATS_URL}
      subject: orders.events
      stream: ORDERS
      required: true
      send_timeout_secs: 5
      batch_timeout_secs: 30
</code></pre>
</td>
<td>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>id</code></td><td>string</td><td>â€”</td><td>Sink identifier</td></tr>
<tr><td><code>url</code></td><td>string</td><td>â€”</td><td>NATS server URL</td></tr>
<tr><td><code>subject</code></td><td>string</td><td>â€”</td><td>Subject to publish to</td></tr>
<tr><td><code>stream</code></td><td>string</td><td>â€”</td><td>JetStream stream name</td></tr>
<tr><td><code>required</code></td><td>bool</td><td><code>true</code></td><td>Gates checkpoints</td></tr>
<tr><td><code>send_timeout_secs</code></td><td>int</td><td><code>5</code></td><td>Publish timeout</td></tr>
<tr><td><code>batch_timeout_secs</code></td><td>int</td><td><code>30</code></td><td>Batch timeout</td></tr>
<tr><td><code>connect_timeout_secs</code></td><td>int</td><td><code>10</code></td><td>Connection timeout</td></tr>
</tbody>
</table>
</div>
</td>
</tr>

</table>

<h3 id="authentication-options"><a class="header" href="#authentication-options">Authentication options</a></h3>
<pre><code class="language-yaml"># Credentials file
credentials_file: /etc/nats/creds/user.creds

# Username/password
username: ${NATS_USER}
password: ${NATS_PASSWORD}

# Token
token: ${NATS_TOKEN}
</code></pre>
<h2 id="jetstream-setup"><a class="header" href="#jetstream-setup">JetStream setup</a></h2>
<p>Before using the NATS sink with JetStream, create a stream that captures your subject:</p>
<pre><code class="language-bash"># Using NATS CLI
nats stream add ORDERS \
  --subjects "orders.&gt;" \
  --retention limits \
  --storage file \
  --replicas 3 \
  --max-age 7d

# Verify stream
nats stream info ORDERS
</code></pre>
<h2 id="consuming-events-2"><a class="header" href="#consuming-events-2">Consuming events</a></h2>
<h3 id="nats-cli"><a class="header" href="#nats-cli">NATS CLI</a></h3>
<pre><code class="language-bash"># Subscribe to subject (ephemeral)
nats sub "orders.&gt;"

# Create durable consumer
nats consumer add ORDERS orders-processor \
  --pull \
  --ack explicit \
  --deliver all \
  --max-deliver 3 \
  --filter "orders.events"

# Consume messages
nats consumer next ORDERS orders-processor --count 10
</code></pre>
<h3 id="go-consumer-example-2"><a class="header" href="#go-consumer-example-2">Go consumer example</a></h3>
<pre><code class="language-go">nc, _ := nats.Connect("nats://localhost:4222")
js, _ := nc.JetStream()

// Create or bind to consumer
sub, _ := js.PullSubscribe("orders.events", "orders-processor",
    nats.Durable("orders-processor"),
    nats.AckExplicit(),
)

for {
    msgs, _ := sub.Fetch(10, nats.MaxWait(5*time.Second))
    for _, msg := range msgs {
        var event Event
        json.Unmarshal(msg.Data, &amp;event)
        process(event)
        msg.Ack()
    }
}
</code></pre>
<h3 id="rust-consumer-example-2"><a class="header" href="#rust-consumer-example-2">Rust consumer example</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use async_nats::jetstream;

let client = async_nats::connect("nats://localhost:4222").await?;
let js = jetstream::new(client);

let stream = js.get_stream("ORDERS").await?;
let consumer = stream.get_consumer("orders-processor").await?;

let mut messages = consumer.messages().await?;
while let Some(msg) = messages.next().await {
    let msg = msg?;
    let event: Event = serde_json::from_slice(&amp;msg.payload)?;
    process(event);
    msg.ack().await?;
}
<span class="boring">}</span></code></pre>
<h2 id="monitoring-1"><a class="header" href="#monitoring-1">Monitoring</a></h2>
<p>DeltaForge exposes these metrics for NATS sink monitoring:</p>
<pre><code class="language-yaml"># DeltaForge sink metrics (exposed at /metrics on port 9000)
deltaforge_sink_events_total{pipeline,sink}      # Events delivered
deltaforge_sink_batch_total{pipeline,sink}       # Batches delivered
deltaforge_sink_latency_seconds{pipeline,sink}   # Delivery latency histogram
deltaforge_stage_latency_seconds{pipeline,stage="sink"}  # Stage timing
</code></pre>
<p>For NATS server visibility, use the NATS CLI or monitoring endpoint:</p>
<pre><code class="language-bash"># Server info
nats server info

# JetStream account info
nats account info

# Stream statistics
nats stream info ORDERS

# Consumer statistics  
nats consumer info ORDERS orders-processor

# Real-time event monitoring
nats events
</code></pre>
<p>NATS also exposes a monitoring endpoint (default <code>:8222</code>) with JSON stats:</p>
<ul>
<li><code>http://localhost:8222/varz</code> - General server stats</li>
<li><code>http://localhost:8222/jsz</code> - JetStream stats</li>
<li><code>http://localhost:8222/connz</code> - Connection stats</li>
</ul>
<h2 id="subject-design-patterns"><a class="header" href="#subject-design-patterns">Subject design patterns</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Pattern</th><th>Example</th><th>Use Case</th></tr>
</thead>
<tbody>
<tr><td>Hierarchical</td><td><code>orders.us.created</code></td><td>Regional routing</td></tr>
<tr><td>Wildcard single</td><td><code>orders.*.created</code></td><td>Any region, specific event</td></tr>
<tr><td>Wildcard multi</td><td><code>orders.&gt;</code></td><td>All order events</td></tr>
<tr><td>Versioned</td><td><code>v1.orders.events</code></td><td>API versioning</td></tr>
</tbody>
</table>
</div>
<h2 id="failure-modes-2"><a class="header" href="#failure-modes-2">Failure modes</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Failure</th><th>Symptoms</th><th>DeltaForge behavior</th><th>Resolution</th></tr>
</thead>
<tbody>
<tr><td><strong>Server unavailable</strong></td><td>Connection refused</td><td>Retries with backoff; blocks checkpoint</td><td>Restore NATS; check network</td></tr>
<tr><td><strong>Stream not found</strong></td><td><code>stream not found</code> error</td><td>Fails batch; no retry</td><td>Create stream or remove <code>stream</code> config</td></tr>
<tr><td><strong>Authentication failure</strong></td><td><code>authorization violation</code></td><td>Fails fast, no retry</td><td>Fix credentials</td></tr>
<tr><td><strong>Subject mismatch</strong></td><td><code>no responders</code> (core NATS)</td><td>Fails if no subscribers</td><td>Add subscribers or use JetStream</td></tr>
<tr><td><strong>JetStream disabled</strong></td><td><code>jetstream not enabled</code></td><td>Fails fast</td><td>Enable JetStream on server</td></tr>
<tr><td><strong>Storage full</strong></td><td><code>insufficient resources</code></td><td>Retries; eventually fails</td><td>Add storage; adjust retention</td></tr>
<tr><td><strong>Message too large</strong></td><td><code>message size exceeds maximum</code></td><td>Fails message permanently</td><td>Increase <code>max_payload</code> or filter large events</td></tr>
<tr><td><strong>Cluster partition</strong></td><td>Intermittent failures</td><td>Retries with backoff</td><td>Restore network; wait for quorum</td></tr>
<tr><td><strong>Slow consumer</strong></td><td>Publish backpressure</td><td>Slows down; may timeout</td><td>Scale consumers; increase buffer</td></tr>
<tr><td><strong>TLS errors</strong></td><td>Handshake failures</td><td>Fails fast</td><td>Fix certificates</td></tr>
</tbody>
</table>
</div>
<h3 id="failure-scenarios-and-data-guarantees-2"><a class="header" href="#failure-scenarios-and-data-guarantees-2">Failure scenarios and data guarantees</a></h3>
<p><strong>NATS server restart during batch delivery</strong></p>
<ol>
<li>DeltaForge sends batch of 100 events</li>
<li>50 events published, server restarts</li>
<li>async_nats detects disconnect, starts reconnecting</li>
<li>After reconnect, DeltaForge retries remaining 50</li>
<li>JetStream deduplication prevents duplicates (if enabled)</li>
<li>Checkpoint only saved after ALL events acknowledged</li>
</ol>
<p><strong>DeltaForge crash after JetStream ack, before checkpoint</strong></p>
<ol>
<li>Batch published to JetStream successfully</li>
<li>DeltaForge crashes before saving checkpoint</li>
<li>On restart: replays from last checkpoint</li>
<li>Result: Duplicate events in stream (at-least-once)</li>
<li>Consumer must handle idempotently (check event.id)</li>
</ol>
<p><strong>Stream storage exhausted</strong></p>
<ol>
<li>JetStream stream hits max_bytes or max_msgs limit</li>
<li>With <code>discard: old</code> â†’ oldest messages removed, publish succeeds</li>
<li>With <code>discard: new</code> â†’ publish rejected</li>
<li>DeltaForge retries on rejection</li>
<li>Resolution: Increase limits or enable <code>discard: old</code></li>
</ol>
<h3 id="jetstream-acknowledgement-levels"><a class="header" href="#jetstream-acknowledgement-levels">JetStream acknowledgement levels</a></h3>
<pre><code class="language-yaml"># Stream configuration affects durability
nats stream add ORDERS \
  --replicas 3 \           # R=3 for production
  --retention limits \     # or 'workqueue' for single consumer
  --discard old \          # Remove oldest when full
  --max-age 7d \           # Auto-expire after 7 days
  --storage file           # Persistent (vs memory)
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Replicas</th><th>Guarantee</th><th>Use Case</th></tr>
</thead>
<tbody>
<tr><td>R=1</td><td>Single node; lost if node fails</td><td>Development, non-critical</td></tr>
<tr><td>R=3</td><td>Survives 1 node failure</td><td>Production default</td></tr>
<tr><td>R=5</td><td>Survives 2 node failures</td><td>Critical data</td></tr>
</tbody>
</table>
</div>
<h3 id="handling-duplicates-in-consumers-1"><a class="header" href="#handling-duplicates-in-consumers-1">Handling duplicates in consumers</a></h3>
<pre><code class="language-go">// Use event ID for idempotency
processedIDs := make(map[string]bool)  // Or use Redis/DB

for _, msg := range msgs {
    var event Event
    json.Unmarshal(msg.Data, &amp;event)
    
    if processedIDs[event.ID] {
        msg.Ack()  // Already processed
        continue
    }
    
    if err := process(event); err == nil {
        processedIDs[event.ID] = true
    }
    msg.Ack()
}
</code></pre>
<h2 id="notes-3"><a class="header" href="#notes-3">Notes</a></h2>
<ul>
<li>When <code>stream</code> is specified, the sink verifies the stream exists at connection time</li>
<li>Without <code>stream</code>, events are published to core NATS (no persistence guarantees)</li>
<li>Connection pooling ensures efficient reuse across batches</li>
<li>Use replicated streams (<code>--replicas 3</code>) for production durability</li>
<li>Combine with other sinks to fan out data; use commit policy to control checkpoint behavior</li>
<li>JetStream provides exactly-once semantics when combined with message deduplication</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="envelopes-and-encodings"><a class="header" href="#envelopes-and-encodings">Envelopes and Encodings</a></h1>
<p>DeltaForge supports configurable <strong>envelope formats</strong> and <strong>wire encodings</strong> for sink output. This allows you to match the output format expected by your downstream consumers without forcing code changes on them.</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>Every CDC event flows through two stages before being written to a sink:</p>
<pre><code>Event -&gt; Envelope (structure) -&gt; Encoding (bytes) -&gt; Sink
</code></pre>
<ul>
<li><strong>Envelope</strong>: Controls the JSON structure of the output (what fields exist, how theyâ€™re nested)</li>
<li><strong>Encoding</strong>: Controls the wire format (JSON bytes, future: Avro, Protobuf)</li>
</ul>
<h2 id="envelope-formats"><a class="header" href="#envelope-formats">Envelope Formats</a></h2>
<h3 id="native-default"><a class="header" href="#native-default">Native (default)</a></h3>
<p>The native envelope serializes events directly with minimal overhead. This is DeltaForgeâ€™s own format, optimized for efficiency and practical use cases.</p>
<blockquote class="blockquote-tag blockquote-tag-note">
<p class="blockquote-tag-title"><svg viewbox="0 0 16 16" width="18" height="18"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p>
<p>The native envelope format may evolve over time as we adapt to user needs and optimize for the lowest possible overhead. If you need a stable, standardized format, consider using <code>debezium</code> or <code>cloudevents</code> envelopes which follow their respective established specifications.</p>
</blockquote>
<pre><code class="language-yaml">sinks:
  - type: kafka
    config:
      id: events-kafka
      brokers: localhost:9092
      topic: events
      envelope:
        type: native
</code></pre>
<p><strong>Output:</strong></p>
<pre><code class="language-json">{
  "before": null,
  "after": {"id": 1, "name": "Alice", "email": "alice@example.com"},
  "source": {
    "version": "0.1.0",
    "connector": "mysql",
    "name": "orders-db",
    "ts_ms": 1700000000000,
    "db": "shop",
    "table": "customers",
    "server_id": 1,
    "file": "mysql-bin.000003",
    "pos": 12345
  },
  "op": "c",
  "ts_ms": 1700000000000
}
</code></pre>
<p><strong>When to use:</strong></p>
<ul>
<li>Maximum performance with lowest overhead</li>
<li>Custom consumers that parse the payload directly</li>
<li>When format stability is less important than efficiency</li>
<li>Internal systems where you control both producer and consumer</li>
</ul>
<h3 id="debezium"><a class="header" href="#debezium">Debezium</a></h3>
<p>The Debezium envelope wraps the event in a <code>{"schema": null, "payload": ...}</code> structure,
following the <a href="https://debezium.io/documentation/reference/stable/connectors/mysql.html#mysql-events">Debezium event format specification</a>.</p>
<p>This uses <strong>schemaless mode</strong> (<code>schema: null</code>), which is equivalent to Debeziumâ€™s
<code>JsonConverter</code> with <code>schemas.enable=false</code>. This is the recommended configuration
for most production deployments as it avoids the overhead of inline schemas.</p>
<pre><code class="language-yaml">sinks:
  - type: kafka
    config:
      id: events-kafka
      brokers: localhost:9092
      topic: events
      envelope:
        type: debezium
</code></pre>
<p><strong>Output:</strong></p>
<pre><code class="language-json">{
  "schema": null,
  "payload": {
    "before": null,
    "after": {"id": 1, "name": "Alice", "email": "alice@example.com"},
    "source": {
      "version": "0.1.0",
      "connector": "mysql",
      "name": "orders-db",
      "ts_ms": 1700000000000,
      "db": "shop",
      "table": "customers"
    },
    "op": "c",
    "ts_ms": 1700000000000
  }
}
</code></pre>
<p><strong>When to use:</strong></p>
<ul>
<li>Kafka Connect consumers expecting full Debezium format</li>
<li>Existing Debezium-based pipelines youâ€™re migrating from</li>
<li>Tools that specifically parse the <code>payload</code> wrapper</li>
<li>When you need a stable, well-documented format with broad ecosystem support</li>
</ul>
<blockquote>
<p><strong>Note:</strong> For Schema Registry integration with Avro encoding (planned), schema handling
will move to the encoding layer where schema IDs are embedded in the wire format.</p>
</blockquote>
<h3 id="cloudevents"><a class="header" href="#cloudevents">CloudEvents</a></h3>
<p>The CloudEvents envelope restructures events to the <a href="https://cloudevents.io/">CloudEvents 1.0 specification</a>, a CNCF project that defines a vendor-neutral format for event data. This format strictly follows the CloudEvents spec and is guaranteed to remain compliant.</p>
<pre><code class="language-yaml">sinks:
  - type: kafka
    config:
      id: events-kafka
      brokers: localhost:9092
      topic: events
      envelope:
        type: cloudevents
        type_prefix: "com.example.cdc"
</code></pre>
<p><strong>Output:</strong></p>
<pre><code class="language-json">{
  "specversion": "1.0",
  "id": "550e8400-e29b-41d4-a716-446655440000",
  "source": "deltaforge/orders-db/shop.customers",
  "type": "com.example.cdc.created",
  "time": "2024-01-15T10:30:00.000Z",
  "datacontenttype": "application/json",
  "subject": "shop.customers",
  "data": {
    "before": null,
    "after": {"id": 1, "name": "Alice", "email": "alice@example.com"},
    "op": "c"
  }
}
</code></pre>
<p>The <code>type</code> field is constructed from your <code>type_prefix</code> plus the operation:</p>
<ul>
<li><code>com.example.cdc.created</code> (INSERT)</li>
<li><code>com.example.cdc.updated</code> (UPDATE)</li>
<li><code>com.example.cdc.deleted</code> (DELETE)</li>
<li><code>com.example.cdc.snapshot</code> (READ/snapshot)</li>
<li><code>com.example.cdc.truncated</code> (TRUNCATE)</li>
</ul>
<p><strong>When to use:</strong></p>
<ul>
<li>AWS EventBridge, Azure Event Grid, or other CloudEvents-native platforms</li>
<li>Serverless architectures (Lambda, Cloud Functions)</li>
<li>Event-driven microservices using CloudEvents SDKs</li>
<li>Standardized event routing based on <code>type</code> field</li>
<li>When you need a vendor-neutral, CNCF-backed standard format</li>
</ul>
<h2 id="wire-encodings"><a class="header" href="#wire-encodings">Wire Encodings</a></h2>
<h3 id="json-default"><a class="header" href="#json-default">JSON (default)</a></h3>
<p>Standard UTF-8 JSON encoding. Human-readable and widely supported.</p>
<pre><code class="language-yaml">sinks:
  - type: kafka
    config:
      id: events-kafka
      brokers: localhost:9092
      topic: events
      encoding: json
</code></pre>
<p><strong>Content-Type:</strong> <code>application/json</code></p>
<p><strong>When to use:</strong></p>
<ul>
<li>Development and debugging</li>
<li>Consumers that expect JSON</li>
<li>When human readability matters</li>
<li>Most use cases (good default)</li>
</ul>
<h3 id="future-avro"><a class="header" href="#future-avro">Future: Avro</a></h3>
<blockquote>
<p><strong>Coming soon</strong>: Avro encoding with Schema Registry integration for compact binary serialization and schema evolution support.</p>
</blockquote>
<pre><code class="language-yaml"># Future configuration (not yet implemented)
sinks:
  - type: kafka
    config:
      id: events-kafka
      brokers: localhost:9092
      topic: events
      encoding:
        type: avro
        schema_registry: http://schema-registry:8081
</code></pre>
<h2 id="configuration-examples"><a class="header" href="#configuration-examples">Configuration Examples</a></h2>
<h3 id="kafka-with-cloudevents"><a class="header" href="#kafka-with-cloudevents">Kafka with CloudEvents</a></h3>
<pre><code class="language-yaml">sinks:
  - type: kafka
    config:
      id: orders-kafka
      brokers: ${KAFKA_BROKERS}
      topic: order-events
      envelope:
        type: cloudevents
        type_prefix: "com.acme.orders"
      encoding: json
      required: true
</code></pre>
<h3 id="redis-with-debezium-envelope"><a class="header" href="#redis-with-debezium-envelope">Redis with Debezium envelope</a></h3>
<pre><code class="language-yaml">sinks:
  - type: redis
    config:
      id: orders-redis
      uri: ${REDIS_URI}
      stream: orders
      envelope:
        type: debezium
      encoding: json
</code></pre>
<h3 id="nats-with-native-envelope"><a class="header" href="#nats-with-native-envelope">NATS with native envelope</a></h3>
<pre><code class="language-yaml">sinks:
  - type: nats
    config:
      id: orders-nats
      url: ${NATS_URL}
      subject: orders.events
      stream: ORDERS
      envelope:
        type: native
      encoding: json
</code></pre>
<h3 id="multi-sink-with-different-formats-1"><a class="header" href="#multi-sink-with-different-formats-1">Multi-sink with different formats</a></h3>
<p>Different consumers may expect different formats. Configure each sink independently:</p>
<pre><code class="language-yaml">sinks:
  # Kafka Connect expects Debezium format
  - type: kafka
    config:
      id: connect-sink
      brokers: ${KAFKA_BROKERS}
      topic: connect-events
      envelope:
        type: debezium
      required: true

  # Lambda expects CloudEvents
  - type: kafka
    config:
      id: lambda-sink
      brokers: ${KAFKA_BROKERS}
      topic: lambda-events
      envelope:
        type: cloudevents
        type_prefix: "com.acme.cdc"
      required: false

  # Analytics wants raw events
  - type: redis
    config:
      id: analytics-redis
      uri: ${REDIS_URI}
      stream: analytics
      envelope:
        type: native
</code></pre>
<h2 id="operation-mapping"><a class="header" href="#operation-mapping">Operation Mapping</a></h2>
<p>DeltaForge uses Debezium-compatible operation codes:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Operation</th><th>Code</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td>Create/Insert</td><td><code>c</code></td><td>New row inserted</td></tr>
<tr><td>Update</td><td><code>u</code></td><td>Existing row modified</td></tr>
<tr><td>Delete</td><td><code>d</code></td><td>Row deleted</td></tr>
<tr><td>Read</td><td><code>r</code></td><td>Snapshot read (initial load)</td></tr>
<tr><td>Truncate</td><td><code>t</code></td><td>Table truncated</td></tr>
</tbody>
</table>
</div>
<p>These codes appear in the <code>op</code> field regardless of envelope format.</p>
<h2 id="performance-considerations-1"><a class="header" href="#performance-considerations-1">Performance Considerations</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Envelope</th><th>Overhead</th><th>Format Stability</th><th>Use Case</th></tr>
</thead>
<tbody>
<tr><td>Native</td><td>Baseline (minimal)</td><td>May evolve</td><td>High-throughput, internal systems</td></tr>
<tr><td>Debezium</td><td>~14 bytes</td><td>Stable (follows Debezium spec)</td><td>Kafka Connect, Debezium ecosystem</td></tr>
<tr><td>CloudEvents</td><td>~150-200 bytes</td><td>Stable (follows CNCF spec)</td><td>Serverless, event-driven architectures</td></tr>
</tbody>
</table>
</div>
<p>The native envelope is recommended for maximum throughput when you control both ends of the pipeline. For interoperability with external systems or when format stability is critical, use <code>debezium</code> or <code>cloudevents</code>.</p>
<h2 id="defaults"><a class="header" href="#defaults">Defaults</a></h2>
<p>If not specified, sinks use:</p>
<ul>
<li><strong>Envelope</strong>: <code>native</code></li>
<li><strong>Encoding</strong>: <code>json</code></li>
</ul>
<p>The native envelope provides the lowest overhead for high-throughput scenarios. If you need format stability guarantees, use <code>debezium</code> or <code>cloudevents</code> which adhere to their respective established specifications.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="dynamic-routing"><a class="header" href="#dynamic-routing">Dynamic Routing</a></h1>
<p>Dynamic routing controls where each CDC event is delivered - which Kafka topic, Redis stream, or NATS subject receives it. <strong>By default, all events go to the single destination configured in the sink (static routing).</strong> With dynamic routing, events can be split across destinations based on their content or other attributes of events, pipeline and etc.</p>
<h2 id="overview-1"><a class="header" href="#overview-1">Overview</a></h2>
<p>There are two routing mechanisms, and they compose naturally:</p>
<ol>
<li><strong>Template strings</strong> in sink config - resolve per-event from event fields</li>
<li><strong>JavaScript <code>ev.route()</code></strong> - programmatic per-event routing in processors</li>
</ol>
<p>When both are used, <code>ev.route()</code> overrides take highest priority, then template resolution, then the static config value.</p>
<h2 id="template-routing"><a class="header" href="#template-routing">Template Routing</a></h2>
<p>Replace static topic/stream/subject strings with templates containing <code>${...}</code> variables. Templates are compiled once at startup and resolved per-event with zero regex overhead.</p>
<h3 id="kafka-1"><a class="header" href="#kafka-1">Kafka</a></h3>
<pre><code class="language-yaml">sinks:
  - type: kafka
    config:
      id: kafka-routed
      brokers: ${KAFKA_BROKERS}
      topic: "cdc.${source.db}.${source.table}"
      key: "${after.customer_id}"
      envelope:
        type: debezium
</code></pre>
<p>Events from <code>shop.orders</code> -&gt; topic <code>cdc.shop.orders</code>, partitioned by <code>customer_id</code>.</p>
<h3 id="redis-1"><a class="header" href="#redis-1">Redis</a></h3>
<pre><code class="language-yaml">sinks:
  - type: redis
    config:
      id: redis-routed
      uri: ${REDIS_URI}
      stream: "events:${source.table}"
      key: "${after.id}"
</code></pre>
<p>Events from <code>orders</code> -&gt; stream <code>events:orders</code>. The <code>key</code> value appears as the <code>df-key</code> field in each stream entry.</p>
<h3 id="nats-1"><a class="header" href="#nats-1">NATS</a></h3>
<pre><code class="language-yaml">sinks:
  - type: nats
    config:
      id: nats-routed
      url: ${NATS_URL}
      subject: "cdc.${source.db}.${source.table}"
      key: "${after.id}"
      stream: CDC
</code></pre>
<p>Events from <code>shop.orders</code> -&gt; subject <code>cdc.shop.orders</code>. The <code>key</code> value appears as the <code>df-key</code> NATS header.</p>
<h3 id="available-variables"><a class="header" href="#available-variables">Available Variables</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Variable</th><th>Description</th><th>Example value</th></tr>
</thead>
<tbody>
<tr><td><code>${source.table}</code></td><td>Table name</td><td><code>orders</code></td></tr>
<tr><td><code>${source.db}</code></td><td>Database name</td><td><code>shop</code></td></tr>
<tr><td><code>${source.schema}</code></td><td>Schema name (PostgreSQL)</td><td><code>public</code></td></tr>
<tr><td><code>${source.connector}</code></td><td>Source type</td><td><code>mysql</code></td></tr>
<tr><td><code>${op}</code></td><td>Operation code</td><td><code>c</code>, <code>u</code>, <code>d</code>, <code>r</code>, <code>t</code></td></tr>
<tr><td><code>${after.&lt;field&gt;}</code></td><td>Field from after image</td><td><code>42</code>, <code>cust-abc</code></td></tr>
<tr><td><code>${before.&lt;field&gt;}</code></td><td>Field from before image</td><td><code>old-value</code></td></tr>
<tr><td><code>${tenant_id}</code></td><td>Pipeline tenant ID</td><td><code>acme</code></td></tr>
</tbody>
</table>
</div>
<p><strong>Missing fields</strong> resolve to an empty string. A warning is logged once per unique template, not per event.</p>
<p><strong>Static strings</strong> (no <code>${...}</code>) are detected at parse time and have zero overhead on the hot path - no allocation, no resolution.</p>
<h3 id="env-vars-vs-templates"><a class="header" href="#env-vars-vs-templates">Env Vars vs Templates</a></h3>
<p>Both use <code>${...}</code> syntax. The config loader expands environment variables first. Unknown variables pass through as templates for runtime resolution:</p>
<pre><code class="language-yaml">brokers: ${KAFKA_BROKERS}      # env var - expanded at load time
topic: "cdc.${source.table}"   # template - passed through to runtime
key: "${after.customer_id}"    # template - resolved per-event
</code></pre>
<h2 id="javascript-routing"><a class="header" href="#javascript-routing">JavaScript Routing</a></h2>
<p>For routing logic that goes beyond field substitution, use <code>ev.route()</code> in a JavaScript processor. This lets you make conditional routing decisions based on event content.</p>
<pre><code class="language-yaml">processors:
  - type: javascript
    id: smart-router
    inline: |
      function processBatch(events) {
        for (const ev of events) {
          if (!ev.after) continue;

          if (ev.after.total_amount &gt; 10000) {
            ev.route({
              topic: "orders.priority",
              key: String(ev.after.customer_id),
              headers: {
                "x-tier": "high-value",
                "x-amount": String(ev.after.total_amount)
              }
            });
          } else {
            ev.route({
              topic: "orders.standard",
              key: String(ev.after.customer_id)
            });
          }
        }
        return events;
      }
</code></pre>
<h3 id="evroute-fields"><a class="header" href="#evroute-fields">ev.route() fields</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>topic</code></td><td>string</td><td>Override destination (topic, stream, or subject)</td></tr>
<tr><td><code>key</code></td><td>string</td><td>Override message/partition key</td></tr>
<tr><td><code>headers</code></td><td>object</td><td>Key-value pairs added to the message</td></tr>
</tbody>
</table>
</div>
<p>All fields are optional. Only set fields override; omitted fields fall through to config templates or static values.</p>
<p>Calling <code>ev.route()</code> <strong>replaces</strong> any previous routing on that event - it does not merge.</p>
<h3 id="how-headers-are-delivered"><a class="header" href="#how-headers-are-delivered">How headers are delivered</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Sink</th><th>Key delivery</th><th>Header delivery</th></tr>
</thead>
<tbody>
<tr><td>Kafka</td><td>Kafka message key</td><td>Kafka message headers</td></tr>
<tr><td>Redis</td><td><code>df-key</code> field in stream entry</td><td><code>df-headers</code> field (JSON)</td></tr>
<tr><td>NATS</td><td><code>df-key</code> NATS header</td><td>Individual NATS headers</td></tr>
</tbody>
</table>
</div>
<h2 id="resolution-order"><a class="header" href="#resolution-order">Resolution Order</a></h2>
<p>For each event, the destination is resolved in priority order:</p>
<pre><code>ev.route() override  â†’  config template  â†’  static config value
</code></pre>
<p>Specifically:</p>
<ol>
<li>If the event has <code>routing.topic</code> set (via <code>ev.route()</code> or programmatically), use it</li>
<li>If the sink config contains a template (has <code>${...}</code>), resolve it from event fields</li>
<li>Otherwise, use the static config string</li>
</ol>
<p>The same order applies independently to <code>key</code> and <code>headers</code>.</p>
<h2 id="examples-1"><a class="header" href="#examples-1">Examples</a></h2>
<p>See the complete example configurations:</p>
<ul>
<li><a href="examples/dynamic-routing.html">Dynamic Routing</a> - template-based routing across Kafka, Redis, and NATS</li>
<li><a href="examples/dynamic-js-routing.html">JavaScript Routing</a> - conditional routing with <code>ev.route()</code> based on business logic</li>
</ul>
<h2 id="related"><a class="header" href="#related">Related</a></h2>
<ul>
<li><a href="#configuration">Configuration Reference</a> â€” full sink config fields</li>
<li><a href="processors/javascript.html">JavaScript Processors</a> â€” processor API reference</li>
<li><a href="sinks/README.html">Sinks Overview</a> â€” multi-sink patterns and commit policies</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="outbox-pattern"><a class="header" href="#outbox-pattern">Outbox Pattern</a></h1>
<p>The <a href="https://microservices.io/patterns/data/transactional-outbox.html">transactional outbox pattern</a> guarantees that domain events are published whenever the corresponding database change is committed - no two-phase commit required. DeltaForge supports this natively for both MySQL and PostgreSQL with zero application-side polling.</p>
<h2 id="how-it-works"><a class="header" href="#how-it-works">How It Works</a></h2>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚  Application    â”‚â”€â”€â”€â”€&gt;â”‚   Database   â”‚â”€â”€â”€â”€&gt;â”‚   DeltaForge     â”‚â”€â”€â”€â”€&gt;â”‚ Sink â”‚
â”‚  (writes data   â”‚     â”‚  (outbox     â”‚     â”‚  (captures +     â”‚     â”‚      â”‚
â”‚   + outbox msg) â”‚     â”‚   table/WAL) â”‚     â”‚   transforms)    â”‚     â”‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<ol>
<li>Your application writes business data <strong>and</strong> an outbox message in the same transaction.</li>
<li>DeltaForge captures the outbox event through the databaseâ€™s native replication stream.</li>
<li>The <code>OutboxProcessor</code> extracts the payload, resolves the destination topic, and sets routing headers.</li>
<li>The transformed event flows to sinks like any other CDC event.</li>
</ol>
<p>Because the outbox write is part of the application transaction, the event is guaranteed to be published if and only if the transaction commits.</p>
<h2 id="source-configuration"><a class="header" href="#source-configuration">Source Configuration</a></h2>
<p>Each database uses its native mechanism â€” there is nothing to install or poll.</p>
<h3 id="postgresql--wal-messages"><a class="header" href="#postgresql--wal-messages">PostgreSQL â€” WAL Messages</a></h3>
<p>PostgreSQL uses <code>pg_logical_emit_message()</code> to write messages directly into the WAL. No table is needed.</p>
<pre><code class="language-sql">-- In your application transaction:
BEGIN;
INSERT INTO orders (id, total) VALUES (42, 99.99);
SELECT pg_logical_emit_message(
  true,     -- transactional: tied to the enclosing TX
  'outbox', -- prefix: matched by DeltaForge
  '{"aggregate_type":"Order","aggregate_id":"42","event_type":"OrderCreated","payload":{"total":99.99}}'
);
COMMIT;
</code></pre>
<p><strong>Source config:</strong></p>
<pre><code class="language-yaml">source:
  type: postgres
  config:
    id: orders-pg
    dsn: ${POSTGRES_DSN}
    slot: deltaforge_orders
    publication: orders_pub
    tables: [public.orders]
    outbox:
      prefixes: [outbox]
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>outbox.prefixes</code></td><td>array</td><td>WAL message prefixes to capture. Supports glob patterns: <code>outbox</code> (exact), <code>outbox_%</code> (prefix match), <code>*</code> (all).</td></tr>
</tbody>
</table>
</div>
<p>The message prefix becomes <code>source.table</code> on the event, so processors can filter by prefix when multiple outbox channels share a pipeline.</p>
<h3 id="mysql--table-inserts"><a class="header" href="#mysql--table-inserts">MySQL â€” Table Inserts</a></h3>
<p>MySQL uses a regular table. For production, use the <code>BLACKHOLE</code> storage engine so rows are written to the binlog but never stored on disk.</p>
<pre><code class="language-sql">-- Create outbox table (BLACKHOLE = no disk storage, binlog only)
CREATE TABLE outbox (
  id        INT AUTO_INCREMENT PRIMARY KEY,
  aggregate_type VARCHAR(64),
  aggregate_id   VARCHAR(64),
  event_type     VARCHAR(64),
  payload        JSON
) ENGINE=BLACKHOLE;
</code></pre>
<pre><code class="language-sql">-- In your application transaction:
BEGIN;
INSERT INTO orders (id, total) VALUES (42, 99.99);
INSERT INTO outbox (aggregate_type, aggregate_id, event_type, payload)
VALUES ('Order', '42', 'OrderCreated', '{"total": 99.99}');
COMMIT;
</code></pre>
<p><strong>Source config:</strong></p>
<pre><code class="language-yaml">source:
  type: mysql
  config:
    id: orders-mysql
    dsn: ${MYSQL_DSN}
    tables:
      - shop.orders
      - shop.outbox
    outbox:
      tables: ["shop.outbox"]
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>outbox.tables</code></td><td>array</td><td>Table patterns to tag as outbox events. Supports globs: <code>shop.outbox</code> (exact), <code>*.outbox</code> (any database), <code>shop.outbox_%</code> (prefix).</td></tr>
</tbody>
</table>
</div>
<blockquote>
<p><strong>Note:</strong> The outbox table must be included in the sourceâ€™s <code>tables</code> list so DeltaForge subscribes to its binlog events. Only INSERTs are captured - UPDATE and DELETE on the outbox table are ignored.</p>
</blockquote>
<h2 id="outbox-processor"><a class="header" href="#outbox-processor">Outbox Processor</a></h2>
<p>The <code>OutboxProcessor</code> transforms raw outbox events into routed, sink-ready events. Add it to your <code>processors</code> list:</p>
<pre><code class="language-yaml">processors:
  - type: outbox
    topic: "${aggregate_type}.${event_type}"
    default_topic: "events.unrouted"
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>id</code></td><td>string</td><td><code>"outbox"</code></td><td>Processor identifier</td></tr>
<tr><td><code>tables</code></td><td>array</td><td><code>[]</code></td><td>Filter: only process outbox events matching these patterns. Empty = all outbox events.</td></tr>
<tr><td><code>topic</code></td><td>string</td><td>-</td><td>Topic template resolved against the raw payload using <code>${field}</code> placeholders</td></tr>
<tr><td><code>default_topic</code></td><td>string</td><td>-</td><td>Fallback topic when template resolution fails and no <code>topic</code> column exists</td></tr>
<tr><td><code>columns</code></td><td>object</td><td><em>(see below)</em></td><td>Column name mappings for extracting outbox fields</td></tr>
<tr><td><code>additional_headers</code></td><td>map</td><td><code>{}</code></td><td>Forward extra payload fields as routing headers. Key = header name, value = column name.</td></tr>
<tr><td><code>raw_payload</code></td><td>bool</td><td><code>false</code></td><td>When true, deliver the extracted payload as-is to sinks, bypassing envelope wrapping (native/debezium/cloudevents). Metadata is still available via routing headers.</td></tr>
<tr><td><code>key</code></td><td>string</td><td>-</td><td>Key template resolved against raw payload. Sets <code>routing.key</code> for sink partitioning. Default: aggregate_id value.</td></tr>
<tr><td><code>strict</code></td><td>bool</td><td><code>false</code></td><td>When true, fail the batch if required fields are missing (topic, payload, aggregate_type, aggregate_id, event_type). When false, missing fields are silently skipped.</td></tr>
</tbody>
</table>
</div>
<h3 id="column-mappings"><a class="header" href="#column-mappings">Column Mappings</a></h3>
<p>Column mappings control <strong>header extraction and payload rewriting</strong> - they tell the processor which fields correspond to <code>aggregate_type</code>, <code>aggregate_id</code>, etc. for setting <code>df-*</code> headers. The <strong>topic template</strong> resolves directly against the raw payload, so you reference your actual column names there.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Column</th><th>Default</th><th>Header</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>payload</code></td><td><code>"payload"</code></td><td>-</td><td>Event body. Extracted and promoted to <code>event.after</code>.</td></tr>
<tr><td><code>aggregate_type</code></td><td><code>"aggregate_type"</code></td><td><code>df-aggregate-type</code></td><td>Aggregate root type (e.g. <code>Order</code>).</td></tr>
<tr><td><code>aggregate_id</code></td><td><code>"aggregate_id"</code></td><td><code>df-aggregate-id</code></td><td>Aggregate root ID. Also used as default routing key.</td></tr>
<tr><td><code>event_type</code></td><td><code>"event_type"</code></td><td><code>df-event-type</code></td><td>Domain event type (e.g. <code>OrderCreated</code>).</td></tr>
<tr><td><code>topic</code></td><td><code>"topic"</code></td><td>-</td><td>Per-row topic override (used when template is absent).</td></tr>
<tr><td><code>event_id</code></td><td><code>"id"</code></td><td><code>df-event-id</code></td><td>Event identity for idempotency/dedup.</td></tr>
</tbody>
</table>
</div>
<p>If your outbox payload uses non-default field names, override them:</p>
<pre><code class="language-yaml">processors:
  - type: outbox
    topic: "${aggregate_type}.${event_type}"
    columns:
      payload: data           # default: "payload"
      aggregate_type: type    # default: "aggregate_type"
      aggregate_id: key       # default: "aggregate_id"
      event_type: action      # default: "event_type"
      topic: destination      # default: "topic"
      event_id: uuid          # default: "id"
</code></pre>
<h3 id="additional-headers"><a class="header" href="#additional-headers">Additional Headers</a></h3>
<p>Forward arbitrary payload fields as routing headers. This is useful when migrating from Debeziumâ€™s <code>table.fields.additional.placement</code> or when downstream consumers need extra metadata:</p>
<pre><code class="language-yaml">processors:
  - type: outbox
    topic: "${aggregate_type}.${event_type}"
    additional_headers:
      x-trace-id: trace_id
      x-correlation-id: correlation_id
      x-source-region: region
</code></pre>
<p>Each key becomes a header name, each value is the column name in the outbox payload. Missing columns are silently skipped - no error if a row doesnâ€™t contain the field.</p>
<h3 id="typed-extraction"><a class="header" href="#typed-extraction">Typed Extraction</a></h3>
<p>Header values are extracted as strings regardless of the source JSON type.
Numeric IDs, booleans, and string values are all stringified automatically:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>JSON value</th><th>Header value</th></tr>
</thead>
<tbody>
<tr><td><code>"abc-123"</code></td><td><code>abc-123</code></td></tr>
<tr><td><code>42</code></td><td><code>42</code></td></tr>
<tr><td><code>true</code></td><td><code>true</code></td></tr>
<tr><td><code>null</code> / missing</td><td><em>(skipped)</em></td></tr>
<tr><td><code>{}</code> / <code>[]</code></td><td><em>(skipped)</em></td></tr>
</tbody>
</table>
</div>
<h3 id="what-the-processor-does"><a class="header" href="#what-the-processor-does">What the Processor Does</a></h3>
<ol>
<li><strong>Identifies</strong> outbox events by the <code>__outbox</code> sentinel on <code>source.schema</code> (set by the source).</li>
<li><strong>Extracts</strong> <code>aggregate_type</code>, <code>aggregate_id</code>, <code>event_type</code>, and <code>payload</code> from <code>event.after</code>.</li>
<li><strong>Resolves the topic</strong> using a three-step cascade:
<ul>
<li>Template resolved against the <strong>raw payload</strong> (e.g. <code>${domain}.${action}</code> â€” use your actual column names)</li>
<li>Column value (a <code>topic</code> field in the payload, configurable via <code>columns.topic</code>)</li>
<li><code>default_topic</code> fallback</li>
</ul>
</li>
<li><strong>Rewrites</strong> <code>event.after</code> to just the <code>payload</code> content.</li>
<li><strong>Sets routing headers</strong>: <code>df-event-id</code>, <code>df-aggregate-type</code>, <code>df-aggregate-id</code>, <code>df-event-type</code>, plus any <code>additional_headers</code> mappings.</li>
<li><strong>Sets routing key</strong> using the key template (or falls back to <code>aggregate_id</code>).</li>
<li><strong>Marks raw delivery</strong> if <code>raw_payload: true</code> â€” sinks serialize <code>event.after</code> directly, skipping envelope wrapping.</li>
<li><strong>Clears</strong> the <code>__outbox</code> sentinel so the event looks like a normal CDC event to sinks.</li>
<li><strong>Drops</strong> non-INSERT outbox events (UPDATE/DELETE on the outbox table are meaningless).</li>
<li><strong>Validates</strong> in strict mode (<code>strict: true</code>): fails the batch with an error if any required field is missing (topic, payload, aggregate_type, aggregate_id, event_type). The error names the missing fields so operators can fix the schema. In lenient mode (default), missing fields are silently skipped.</li>
<li><strong>Passes through</strong> all non-outbox events unchanged.</li>
</ol>
<h3 id="multi-outbox-routing"><a class="header" href="#multi-outbox-routing">Multi-Outbox Routing</a></h3>
<p>When a source captures multiple outbox channels, use the <code>tables</code> filter to scope each processor:</p>
<pre><code class="language-yaml">processors:
  - type: outbox
    tables: [orders_outbox]
    topic: "orders.${event_type}"

  - type: outbox
    tables: [payments_outbox]
    topic: "payments.${event_type}"
    columns:
      payload: data
</code></pre>
<h2 id="complete-examples-1"><a class="header" href="#complete-examples-1">Complete Examples</a></h2>
<h3 id="postgresql--kafka"><a class="header" href="#postgresql--kafka">PostgreSQL â†’ Kafka</a></h3>
<pre><code class="language-yaml">apiVersion: deltaforge/v1
kind: Pipeline
metadata:
  name: order-events
  tenant: acme
spec:
  source:
    type: postgres
    config:
      id: pg1
      dsn: ${POSTGRES_DSN}
      publication: orders_pub
      slot: orders_slot
      tables: [public.orders]
      outbox:
        prefixes: [outbox]

  processors:
    - type: outbox
      topic: "${aggregate_type}.${event_type}"
      default_topic: "events.unrouted"
      raw_payload: true

  sinks:
    - type: kafka
      config:
        id: k1
        brokers: ${KAFKA_BROKERS}
        topic: "events.fallback"
</code></pre>
<p>The <code>raw_payload: true</code> means outbox events hit the wire as the extracted payload JSON. Regular CDC events (from <code>public.orders</code>) still use the sinkâ€™s configured envelope.</p>
<h3 id="mysql--kafka-multi-outbox"><a class="header" href="#mysql--kafka-multi-outbox">MySQL â†’ Kafka (Multi-Outbox)</a></h3>
<pre><code class="language-yaml">apiVersion: deltaforge/v1
kind: Pipeline
metadata:
  name: shop-events
  tenant: acme
spec:
  source:
    type: mysql
    config:
      id: m1
      dsn: ${MYSQL_DSN}
      tables:
        - shop.orders
        - shop.orders_outbox
        - shop.payments_outbox
      outbox:
        tables: ["shop.*_outbox"]

  processors:
    - type: outbox
      tables: [orders_outbox]
      topic: "orders.${event_type}"
      raw_payload: true

    - type: outbox
      tables: [payments_outbox]
      topic: "payments.${event_type}"
      raw_payload: true

  sinks:
    - type: kafka
      config:
        id: k1
        brokers: ${KAFKA_BROKERS}
        topic: "events.default"
</code></pre>
<h2 id="migrating-from-debezium"><a class="header" href="#migrating-from-debezium">Migrating from Debezium</a></h2>
<p>If youâ€™re using Debeziumâ€™s outbox event router with a custom schema, DeltaForgeâ€™s column mappings and <code>additional_headers</code> map directly. For example, given this Debezium-style outbox table:</p>
<pre><code class="language-sql">CREATE TABLE outbox_events (
  id UUID PRIMARY KEY,
  aggregatetype VARCHAR(64),
  aggregateid VARCHAR(64),
  type VARCHAR(64),
  payload JSONB,
  traceid VARCHAR(64),
  tenant VARCHAR(32)
);
</code></pre>
<p>Debezium config:</p>
<pre><code class="language-properties">transforms.outbox.table.field.event.id=id
transforms.outbox.table.field.event.key=aggregateid
transforms.outbox.table.field.event.type=type
transforms.outbox.table.field.event.payload=payload
transforms.outbox.table.fields.additional.placement=traceid:header,tenant:header
</code></pre>
<p>DeltaForge equivalent:</p>
<pre><code class="language-yaml">processors:
  - type: outbox
    topic: "${aggregatetype}.${type}"
    key: "${aggregateid}"
    raw_payload: true
    columns:
      event_id: id
      aggregate_type: aggregatetype
      aggregate_id: aggregateid
      event_type: type
      payload: payload
    additional_headers:
      x-trace-id: traceid
      x-tenant: tenant
</code></pre>
<p>Key differences from Debezium:</p>
<ul>
<li>Topic template references <strong>raw column names</strong> directly â€” <code>${aggregatetype}</code>, not <code>${aggregate_type}</code></li>
<li><code>raw_payload: true</code> delivers the extracted payload as-is â€” same behavior as Debeziumâ€™s outbox event router</li>
<li>Column mappings only affect header extraction (<code>df-*</code> headers) and payload rewriting</li>
<li><code>additional_headers</code> replaces <code>table.fields.additional.placement</code></li>
<li>No SMT chain â€” everything is in one processor config</li>
</ul>
<h2 id="observability"><a class="header" href="#observability">Observability</a></h2>
<p>The outbox processor emits Prometheus-compatible metrics:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Metric</th><th>Labels</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>deltaforge_outbox_transformed_total</code></td><td>-</td><td>Events successfully transformed</td></tr>
<tr><td><code>deltaforge_outbox_dropped_total</code></td><td><code>reason</code></td><td>Events dropped or rejected</td></tr>
</tbody>
</table>
</div>
<p>Drop reasons:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Reason</th><th>Meaning</th></tr>
</thead>
<tbody>
<tr><td><code>non_insert</code></td><td>UPDATE/DELETE on outbox table (expected, harmless)</td></tr>
<tr><td><code>non_object</code></td><td><code>event.after</code> is not a JSON object</td></tr>
<tr><td><code>null_payload</code></td><td><code>event.after</code> is null</td></tr>
<tr><td><code>strict_missing_fields</code></td><td>Strict mode: required field missing (batch fails)</td></tr>
</tbody>
</table>
</div>
<p>In strict mode, <code>strict_missing_fields</code> increments the counter <em>and</em> returns an error that halts the batch - the pipeline will not silently lose events.</p>
<h2 id="tips-1"><a class="header" href="#tips-1">Tips</a></h2>
<ul>
<li><strong>PostgreSQL WAL messages are lightweight.</strong> No table, no index, no vacuum - just a single WAL entry per message. Prefer this over table-based outbox when using PostgreSQL.</li>
<li><strong>MySQL BLACKHOLE engine</strong> avoids storing outbox rows on disk. The row is written to the binlog and immediately discarded. Use it in production to avoid unbounded table growth.</li>
<li><strong>Outbox events coexist with normal CDC.</strong> The processor passes through all non-outbox events untouched, so you can mix regular table capture and outbox in the same pipeline.</li>
<li><strong>Topic templates use <code>${field}</code> syntax</strong>, same as <a href="#dynamic-routing">dynamic routing</a>. The template resolves directly against the raw outbox payload columns - use your actual column names like <code>${domain}.${action}</code>, no remapping needed.</li>
<li><strong>At-least-once delivery</strong> applies to outbox events just like regular CDC events. Downstream consumers should be idempotent - use the <code>df-event-id</code> header for idempotency, or <code>aggregate_id</code> + <code>event_type</code> as a composite dedup key.</li>
<li><strong>Malformed events are logged and dropped</strong> by default. Non-INSERT operations, null payloads, and non-object payloads produce a WARN-level log with the table name and reason. They do not propagate to sinks. Enable <code>strict: true</code> to fail the batch instead of dropping - this ensures operators are alerted to schema issues before events are lost.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="architecture"><a class="header" href="#architecture">Architecture</a></h1>
<p>This document describes DeltaForgeâ€™s internal architecture, design decisions, and how the major components interact.</p>
<h2 id="design-principles"><a class="header" href="#design-principles">Design Principles</a></h2>
<h3 id="source-owned-semantics"><a class="header" href="#source-owned-semantics">Source-Owned Semantics</a></h3>
<p>DeltaForge avoids imposing a universal data model on all sources. Instead, each database source defines and owns its schema semantics:</p>
<ul>
<li><strong>MySQL</strong> captures MySQL-specific types, collations, and engine information</li>
<li><strong>Future sources</strong> (PostgreSQL, MongoDB, ClickHouse, Turso) will capture their native semantics</li>
</ul>
<p>This approach means downstream consumers receive schemas that accurately reflect the source database rather than a lowest-common-denominator normalization.</p>
<h3 id="delivery-guarantees-first"><a class="header" href="#delivery-guarantees-first">Delivery Guarantees First</a></h3>
<p>The checkpoint system is designed around a single invariant:</p>
<blockquote>
<p>Checkpoints are only saved after events have been successfully delivered.</p>
</blockquote>
<p>This ordering guarantees at-least-once delivery. A crash between checkpoint and delivery would lose events; DeltaForge prevents this by always checkpointing after sink acknowledgment.</p>
<h3 id="configuration-over-code"><a class="header" href="#configuration-over-code">Configuration Over Code</a></h3>
<p>Pipelines are defined declaratively in YAML. This enables:</p>
<ul>
<li>Version-controlled pipeline definitions</li>
<li>Environment-specific configuration via variable expansion</li>
<li>Rapid iteration without recompilation</li>
</ul>
<h2 id="component-overview"><a class="header" href="#component-overview">Component Overview</a></h2>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DeltaForge Runtime                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Sources   â”‚   Schema    â”‚ Coordinator â”‚    Sinks    â”‚ Control â”‚
â”‚             â”‚  Registry   â”‚  + Batch    â”‚             â”‚  Plane  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MySQL       â”‚ InMemory    â”‚ Batching    â”‚ Kafka       â”‚ REST APIâ”‚
â”‚ PostgreSQL  â”‚ Registry    â”‚ Commit      â”‚ Redis       â”‚ Metrics â”‚
â”‚ (future)    â”‚             â”‚ Policy      â”‚ (future)    â”‚ Health  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Checkpoint Store   â”‚
                    â”‚  (File/SQLite/Mem)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h2 id="data-flow"><a class="header" href="#data-flow">Data Flow</a></h2>
<h3 id="event-lifecycle"><a class="header" href="#event-lifecycle">Event Lifecycle</a></h3>
<pre><code>1. Source reads from database log (binlog/WAL)
        â”‚
        â–¼
2. Schema loader maps table_id to schema
        â”‚
        â–¼
3. Event constructed with before/after images
        â”‚
        â–¼
4. Event sent to coordinator via channel
        â”‚
        â–¼
5. Coordinator batches events
        â”‚
        â–¼
6. Processors transform batch (JavaScript)
        â”‚
        â–¼
7. Sinks deliver batch concurrently
        â”‚
        â–¼
8. Commit policy evaluated
        â”‚
        â–¼
9. Checkpoint saved (if policy satisfied)
</code></pre>
<h3 id="event-structure"><a class="header" href="#event-structure">Event Structure</a></h3>
<p>Every CDC event shares a common structure:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Event {
    pub source_id: String,          // Source identifier
    pub database: String,           // Database name
    pub table: String,              // Table name
    pub op: Op,                     // Insert, Update, Delete, Ddl
    pub tx_id: Option&lt;u64&gt;,         // Source transaction ID
    pub before: Option&lt;Value&gt;,      // Previous row state
    pub after: Option&lt;Value&gt;,       // New row state
    pub schema_version: Option&lt;String&gt;,  // Schema fingerprint
    pub schema_sequence: Option&lt;u64&gt;,    // For replay lookups
    pub ddl: Option&lt;Value&gt;,         // DDL payload if op == Ddl
    pub timestamp: DateTime&lt;Utc&gt;,   // Event timestamp
    pub checkpoint: Option&lt;CheckpointMeta&gt;,  // Position info
    pub size_bytes: usize,          // For batching
}
<span class="boring">}</span></code></pre>
<h2 id="schema-registry"><a class="header" href="#schema-registry">Schema Registry</a></h2>
<h3 id="role"><a class="header" href="#role">Role</a></h3>
<p>The schema registry serves three purposes:</p>
<ol>
<li><strong>Map table IDs to schemas</strong>: Binlog events reference tables by ID; the registry resolves these to full schema metadata</li>
<li><strong>Detect schema changes</strong>: Fingerprint comparison identifies when DDL has modified a table</li>
<li><strong>Enable replay</strong>: Sequence numbers correlate events with the schema active when they were produced</li>
</ol>
<h3 id="schema-registration-flow"><a class="header" href="#schema-registration-flow">Schema Registration Flow</a></h3>
<pre><code>1. Schema loader fetches from INFORMATION_SCHEMA
        â”‚
        â–¼
2. Compute fingerprint (SHA-256 of structure)
        â”‚
        â–¼
3. Check registry for existing schema with same fingerprint
        â”‚
        â”œâ”€â”€ Found: Return existing version (idempotent)
        â”‚
        â””â”€â”€ Not found: Allocate new version number
                â”‚
                â–¼
4. Store with: version, fingerprint, JSON, timestamp, sequence, checkpoint
</code></pre>
<h3 id="sequence-numbers"><a class="header" href="#sequence-numbers">Sequence Numbers</a></h3>
<p>The registry maintains a global monotonic counter. Each schema version receives a sequence number at registration. Events carry this sequence, enabling accurate schema lookup during replay:</p>
<pre><code>Timeline:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
     â”‚              â”‚                    â”‚
Schema v1      Schema v2           Schema v3
(seq=1)        (seq=15)            (seq=42)
     â”‚              â”‚                    â”‚
     â””â”€â”€events 1-14â”€â”˜â”€â”€events 15-41â”€â”€â”€â”€â”€â”˜â”€â”€events 42+â”€â”€â–º

Replay at seq=20: Use schema v2 (registered at seq=15, before seq=42)
</code></pre>
<h2 id="checkpoint-store"><a class="header" href="#checkpoint-store">Checkpoint Store</a></h2>
<h3 id="timing-guarantee"><a class="header" href="#timing-guarantee">Timing Guarantee</a></h3>
<p>The checkpoint is saved only after sinks acknowledge delivery:</p>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   events   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   ack    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Source â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  Sink  â”‚ â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ Checkpoint â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚   Store    â”‚
                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<p>If the process crashes after sending to sink but before checkpoint, events will be replayed. This is the â€œat-least-onceâ€ guarantee - duplicates are possible, but loss is not.</p>
<h3 id="storage-backends"><a class="header" href="#storage-backends">Storage Backends</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Backend</th><th>Versioning</th><th>Persistence</th><th>Use Case</th></tr>
</thead>
<tbody>
<tr><td><code>FileCheckpointStore</code></td><td>No</td><td>Yes</td><td>Production (simple)</td></tr>
<tr><td><code>SqliteCheckpointStore</code></td><td>Yes</td><td>Yes</td><td>Development, debugging</td></tr>
<tr><td><code>MemCheckpointStore</code></td><td>No</td><td>No</td><td>Testing</td></tr>
</tbody>
</table>
</div>
<h3 id="checkpoint-schema-correlation"><a class="header" href="#checkpoint-schema-correlation">Checkpoint-Schema Correlation</a></h3>
<p>When registering schemas, the current checkpoint can be attached:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>registry.register_with_checkpoint(
    tenant, db, table,
    &amp;fingerprint,
    &amp;schema_json,
    Some(&amp;checkpoint_bytes),  // Current binlog position
).await?;
<span class="boring">}</span></code></pre>
<p>This creates a link between schema versions and source positions, enabling coordinated rollback and point-in-time schema queries.</p>
<h2 id="coordinator"><a class="header" href="#coordinator">Coordinator</a></h2>
<p>The coordinator orchestrates event flow between source and sinks:</p>
<h3 id="batching-2"><a class="header" href="#batching-2">Batching</a></h3>
<p>Events are accumulated until a threshold triggers flush:</p>
<ul>
<li><code>max_events</code>: Event count limit</li>
<li><code>max_bytes</code>: Total serialized size limit</li>
<li><code>max_ms</code>: Time since batch started</li>
<li><code>respect_source_tx</code>: Never split source transactions</li>
</ul>
<h3 id="commit-policy-3"><a class="header" href="#commit-policy-3">Commit Policy</a></h3>
<p>When multiple sinks are configured, the commit policy determines when the checkpoint advances:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>match policy {
    All =&gt; required_acks == total_sinks,
    Required =&gt; required_acks == sinks.filter(|s| s.required).count(),
    Quorum(n) =&gt; required_acks &gt;= n,
}
<span class="boring">}</span></code></pre>
<h3 id="processor-pipeline"><a class="header" href="#processor-pipeline">Processor Pipeline</a></h3>
<p>Processors run in declared order, transforming batches:</p>
<pre><code>events â”€â”€â–¶ Processor 1 â”€â”€â–¶ Processor 2 â”€â”€â–¶ ... â”€â”€â–¶ transformed events
</code></pre>
<p>Each processor can filter, transform, or enrich events. The JavaScript processor uses <code>deno_core</code> for sandboxed execution.</p>
<h3 id="hot-paths"><a class="header" href="#hot-paths">Hot Paths</a></h3>
<p>Critical performance paths have been optimized:</p>
<ol>
<li><strong>Event construction</strong> - Minimal allocations, reuse buffers</li>
<li><strong>Checkpoint serialization</strong> - Opaque bytes avoid repeated JSON encoding</li>
<li><strong>Sink delivery</strong> - Batch operations reduce round trips</li>
<li><strong>Schema lookup</strong> - In-memory cache with stable fingerprints</li>
</ol>
<h3 id="benchmarking"><a class="header" href="#benchmarking">Benchmarking</a></h3>
<p>Performance is tracked via:</p>
<ul>
<li><strong>Micro-benchmarks</strong> for specific operations</li>
<li><strong>End-to-end benchmarks</strong> using the Coordinator component</li>
<li><strong>Regression detection</strong> in CI</li>
</ul>
<h2 id="future-architecture"><a class="header" href="#future-architecture">Future Architecture</a></h2>
<p>Planned enhancements:</p>
<ul>
<li><strong>Persistent schema registry</strong>: SQLite backend initially, mirroring the checkpoint storage pattern</li>
<li><strong>Production storage backends</strong>: PostgreSQL, S3/GCS for cloud-native and HA deployments</li>
<li><strong>Event store</strong>: Time-based replay and schema evolution</li>
<li><strong>Distributed coordination</strong>: Leader election for HA deployments</li>
<li><strong>Additional sources</strong>: Turso/SQLite, ClickHouse, MongoDB</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="checkpoints-1"><a class="header" href="#checkpoints-1">Checkpoints</a></h1>
<p>Checkpoints record pipeline progress so ingestion can resume from the last successfully delivered position. DeltaForgeâ€™s checkpoint system is designed to guarantee <strong>at-least-once delivery</strong> by carefully coordinating when checkpoints are saved relative to event delivery.</p>
<h2 id="core-guarantee-at-least-once-delivery"><a class="header" href="#core-guarantee-at-least-once-delivery">Core Guarantee: At-Least-Once Delivery</a></h2>
<p>The fundamental rule of DeltaForge checkpointing:</p>
<blockquote>
<p><strong>Checkpoints are only saved after events have been successfully delivered to sinks.</strong></p>
</blockquote>
<p>This ordering is critical. If a checkpoint were saved before events were delivered, a crash between checkpoint save and delivery would cause those events to be lost - the pipeline would resume from a position past events that were never delivered.</p>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Source    â”‚â”€â”€â”€â”€â–¶â”‚  Processor  â”‚â”€â”€â”€â”€â–¶â”‚    Sink     â”‚â”€â”€â”€â”€â–¶â”‚ Checkpoint  â”‚
â”‚   (read)    â”‚     â”‚ (transform) â”‚     â”‚  (deliver)  â”‚     â”‚   (save)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                                               â–¼
                                        Sink acknowledges
                                        successful delivery
                                               â”‚
                                               â–¼
                                        Then checkpoint
                                        is saved
</code></pre>
<h3 id="what-this-means-in-practice"><a class="header" href="#what-this-means-in-practice">What This Means in Practice</a></h3>
<ul>
<li><strong>On clean shutdown</strong>: All buffered events are flushed and checkpointed</li>
<li><strong>On crash</strong>: Events since the last checkpoint are replayed (hence â€œat-least-onceâ€)</li>
<li><strong>Duplicate handling</strong>: Consumers should be idempotent or use deduplication</li>
</ul>
<h2 id="checkpoint-storage"><a class="header" href="#checkpoint-storage">Checkpoint Storage</a></h2>
<h3 id="storage-backends-1"><a class="header" href="#storage-backends-1">Storage Backends</a></h3>
<p>DeltaForge supports pluggable checkpoint storage:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Backend</th><th>Description</th><th>Use Case</th></tr>
</thead>
<tbody>
<tr><td><code>FileCheckpointStore</code></td><td>JSON file on disk</td><td>Development, simple deployments</td></tr>
<tr><td><code>MemCheckpointStore</code></td><td>In-memory (ephemeral)</td><td>Testing</td></tr>
<tr><td><code>SqliteCheckpointStore</code></td><td>SQLite with versioning</td><td>Single-instance production</td></tr>
</tbody>
</table>
</div>
<p>The default stores checkpoints to <code>./data/df_checkpoints.json</code>.</p>
<p>For HA deployments requiring shared state across instances, additional backends (PostgreSQL, S3/GCS) are planned but not yet implemented.</p>
<h3 id="storage-interface"><a class="header" href="#storage-interface">Storage Interface</a></h3>
<p>All backends implement the <code>CheckpointStore</code> trait:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[async_trait]
pub trait CheckpointStore: Send + Sync {
    /// Get raw checkpoint bytes
    async fn get_raw(&amp;self, source_id: &amp;str) -&gt; CheckpointResult&lt;Option&lt;Vec&lt;u8&gt;&gt;&gt;;
    
    /// Store raw checkpoint bytes
    async fn put_raw(&amp;self, source_id: &amp;str, bytes: &amp;[u8]) -&gt; CheckpointResult&lt;()&gt;;
    
    /// Delete checkpoint
    async fn delete(&amp;self, source_id: &amp;str) -&gt; CheckpointResult&lt;bool&gt;;
    
    /// List all checkpoint keys
    async fn list(&amp;self) -&gt; CheckpointResult&lt;Vec&lt;String&gt;&gt;;
    
    /// Whether this backend supports versioning
    fn supports_versioning(&amp;self) -&gt; bool;
}
<span class="boring">}</span></code></pre>
<h3 id="typed-access"><a class="header" href="#typed-access">Typed Access</a></h3>
<p>The <code>CheckpointStoreExt</code> trait provides convenient typed access:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Store typed checkpoint (automatically serialized to JSON)
store.put("pipeline-1", MySqlCheckpoint { 
    file: "binlog.000042".into(),
    pos: 12345,
    gtid_set: None,
}).await?;

// Retrieve typed checkpoint
let cp: Option&lt;MySqlCheckpoint&gt; = store.get("pipeline-1").await?;
<span class="boring">}</span></code></pre>
<h2 id="checkpoint-contents"><a class="header" href="#checkpoint-contents">Checkpoint Contents</a></h2>
<h3 id="mysql-checkpoints"><a class="header" href="#mysql-checkpoints">MySQL Checkpoints</a></h3>
<p>MySQL checkpoints track binlog position:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct MySqlCheckpoint {
    pub file: String,        // e.g., "binlog.000042"
    pub pos: u64,            // Byte position in binlog file
    pub gtid_set: Option&lt;String&gt;,  // GTID set if enabled
}
<span class="boring">}</span></code></pre>
<p>The checkpoint is taken from the last event in a successfully delivered batch, ensuring resumption starts exactly where delivery left off.</p>
<h3 id="checkpoint-in-events"><a class="header" href="#checkpoint-in-events">Checkpoint in Events</a></h3>
<p>Events carry checkpoint metadata for end-to-end tracking:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Event {
    // ... other fields ...
    
    /// Checkpoint info from source
    pub checkpoint: Option&lt;CheckpointMeta&gt;,
}

pub enum CheckpointMeta {
    Opaque(Arc&lt;[u8]&gt;),  // Serialized source-specific checkpoint
}
<span class="boring">}</span></code></pre>
<p>Using <code>Arc&lt;[u8]&gt;</code> allows zero-copy sharing of checkpoint data across the pipeline without repeated allocations.</p>
<h2 id="commit-policy-4"><a class="header" href="#commit-policy-4">Commit Policy</a></h2>
<p>When multiple sinks are configured, the commit policy determines when checkpoints advance:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Policy</th><th>Behavior</th></tr>
</thead>
<tbody>
<tr><td><code>all</code></td><td>Every sink must acknowledge</td></tr>
<tr><td><code>required</code></td><td>Only <code>required: true</code> sinks must acknowledge</td></tr>
<tr><td><code>quorum</code></td><td>At least N sinks must acknowledge</td></tr>
</tbody>
</table>
</div>
<h3 id="configuration-8"><a class="header" href="#configuration-8">Configuration</a></h3>
<pre><code class="language-yaml">spec:
  batch:
    commit_policy: required  # or: all, quorum
    quorum: 2                # for quorum policy

  sinks:
    - type: kafka
      required: true  # Must succeed for checkpoint
      config: { ... }
    
    - type: redis
      required: false  # Best-effort, doesn't block checkpoint
      config: { ... }
</code></pre>
<h3 id="commit-logic"><a class="header" href="#commit-logic">Commit Logic</a></h3>
<p>The coordinator tracks acknowledgments from each sink and only advances the checkpoint when the policy is satisfied:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Simplified commit logic
let required_acks = sinks.iter().filter(|s| s.required).count();
let actual_acks = batch.acknowledgments.iter().filter(|a| a.success).count();

if actual_acks &gt;= required_acks {
    checkpoint_store.put(&amp;key, batch.last_checkpoint).await?;
} else {
    warn!("commit policy not satisfied; checkpoint NOT advanced");
}
<span class="boring">}</span></code></pre>
<h2 id="batching-and-checkpoints"><a class="header" href="#batching-and-checkpoints">Batching and Checkpoints</a></h2>
<p>Checkpoints are saved at batch boundaries, not per-event. This provides:</p>
<ul>
<li><strong>Efficiency</strong>: Fewer checkpoint writes</li>
<li><strong>Atomicity</strong>: Batch success or failure is all-or-nothing</li>
<li><strong>Transaction preservation</strong>: <code>respect_source_tx: true</code> keeps source transactions in single batches</li>
</ul>
<h3 id="batch-configuration"><a class="header" href="#batch-configuration">Batch Configuration</a></h3>
<pre><code class="language-yaml">spec:
  batch:
    max_events: 1000      # Flush after N events
    max_bytes: 8388608    # Flush after 8MB
    max_ms: 200           # Flush after 200ms
    respect_source_tx: true  # Never split source transactions
    max_inflight: 1       # Concurrent batches in flight
</code></pre>
<h3 id="checkpoint-timing-in-batches"><a class="header" href="#checkpoint-timing-in-batches">Checkpoint Timing in Batches</a></h3>
<p>Within a batch:</p>
<ol>
<li>Events are collected until a threshold is reached</li>
<li>Processors transform the batch</li>
<li>Sinks receive and deliver events</li>
<li>Sinks acknowledge success/failure</li>
<li>Commit policy is evaluated</li>
<li>If satisfied, checkpoint advances to the last eventâ€™s position</li>
</ol>
<h2 id="versioned-checkpoints"><a class="header" href="#versioned-checkpoints">Versioned Checkpoints</a></h2>
<p>The SQLite backend supports checkpoint versioning for:</p>
<ul>
<li><strong>Rollback</strong>: Return to a previous checkpoint position</li>
<li><strong>Audit</strong>: Track checkpoint progression over time</li>
<li><strong>Debugging</strong>: Understand checkpoint history during incident analysis</li>
</ul>
<h3 id="version-operations"><a class="header" href="#version-operations">Version Operations</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Store with versioning
let version = store.put_raw_versioned("pipeline-1", bytes).await?;

// Get specific version
let old_bytes = store.get_version_raw("pipeline-1", version - 1).await?;

// List all versions
let versions = store.list_versions("pipeline-1").await?;

// Rollback to previous version
store.rollback("pipeline-1", target_version).await?;
<span class="boring">}</span></code></pre>
<h3 id="version-metadata"><a class="header" href="#version-metadata">Version Metadata</a></h3>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct VersionInfo {
    pub version: u64,
    pub created_at: DateTime&lt;Utc&gt;,
    pub size_bytes: usize,
}
<span class="boring">}</span></code></pre>
<h2 id="schema-checkpoint-correlation"><a class="header" href="#schema-checkpoint-correlation">Schema-Checkpoint Correlation</a></h2>
<p>For replay scenarios, DeltaForge correlates schemas with checkpoints. When a schema is registered, it can optionally include the current checkpoint position:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>registry.register_with_checkpoint(
    tenant, db, table,
    &amp;fingerprint,
    &amp;schema_json,
    Some(checkpoint_bytes),  // Binlog position when schema was observed
).await?;
<span class="boring">}</span></code></pre>
<p>This enables:</p>
<ul>
<li><strong>Accurate replay</strong>: Events are interpreted with the schema active at their checkpoint position</li>
<li><strong>Schema time-travel</strong>: Find what schema was active at any checkpoint</li>
<li><strong>Coordinated rollback</strong>: Roll back both checkpoint and schema state together</li>
</ul>
<h2 id="operational-considerations"><a class="header" href="#operational-considerations">Operational Considerations</a></h2>
<h3 id="clean-shutdown"><a class="header" href="#clean-shutdown">Clean Shutdown</a></h3>
<p>Before maintenance, cleanly stop pipelines to flush checkpoints:</p>
<pre><code class="language-bash"># Pause ingestion
curl -X POST http://localhost:8080/pipelines/{name}/pause

# Wait for in-flight batches to complete
sleep 5

# Stop pipeline
curl -X POST http://localhost:8080/pipelines/{name}/stop
</code></pre>
<h3 id="checkpoint-inspection"><a class="header" href="#checkpoint-inspection">Checkpoint Inspection</a></h3>
<p>View current checkpoint state:</p>
<pre><code class="language-bash"># List all checkpoints
curl http://localhost:8080/checkpoints

# Get specific pipeline checkpoint
curl http://localhost:8080/checkpoints/{pipeline-name}
</code></pre>
<h3 id="monitoring-2"><a class="header" href="#monitoring-2">Monitoring</a></h3>
<p>Key metrics to monitor:</p>
<ul>
<li><code>deltaforge_checkpoint_lag_seconds</code>: Time since last checkpoint</li>
<li><code>deltaforge_checkpoint_bytes</code>: Size of last checkpoint</li>
<li><code>deltaforge_batch_commit_total</code>: Successful batch commits</li>
<li><code>deltaforge_batch_commit_failed_total</code>: Failed commits (policy not satisfied)</li>
</ul>
<h3 id="recovery-scenarios"><a class="header" href="#recovery-scenarios">Recovery Scenarios</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Scenario</th><th>Behavior</th></tr>
</thead>
<tbody>
<tr><td>Process crash</td><td>Resume from last checkpoint, replay events</td></tr>
<tr><td>Network partition (sink unreachable)</td><td>Retry delivery, checkpoint doesnâ€™t advance</td></tr>
<tr><td>Corrupt checkpoint file</td><td>Manual intervention required</td></tr>
<tr><td>Source unavailable at checkpoint</td><td>Retry connection with backoff</td></tr>
</tbody>
</table>
</div>
<h2 id="best-practices"><a class="header" href="#best-practices">Best Practices</a></h2>
<ol>
<li><strong>Use durable storage</strong> for production checkpoint backends (not in-memory)</li>
<li><strong>Monitor checkpoint lag</strong> to detect stuck pipelines</li>
<li><strong>Configure appropriate batch sizes</strong> â€” smaller batches mean more frequent checkpoints but more overhead</li>
<li><strong>Set <code>required: true</code></strong> only on sinks that must succeed for correctness</li>
<li><strong>Test recovery</strong> by killing pipelines and verifying no events are lost</li>
<li><strong>Back up checkpoint files</strong> if using file-based storage</li>
</ol>
<h2 id="future-enhancements"><a class="header" href="#future-enhancements">Future Enhancements</a></h2>
<p>Planned checkpoint improvements:</p>
<ul>
<li><strong>PostgreSQL backend</strong> for HA deployments with shared state</li>
<li><strong>S3/GCS backends</strong> for cloud-native deployments</li>
<li><strong>Distributed coordination</strong> for multi-instance leader election</li>
<li><strong>Checkpoint compression</strong> for large state</li>
<li><strong>Point-in-time recovery</strong> with event store integration</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="schema-registry-1"><a class="header" href="#schema-registry-1">Schema Registry</a></h1>
<p>DeltaForgeâ€™s schema registry tracks table schemas across time, enabling accurate event interpretation during replay and providing change detection for DDL operations.</p>
<h2 id="design-philosophy-source-owned-schemas"><a class="header" href="#design-philosophy-source-owned-schemas">Design Philosophy: Source-Owned Schemas</a></h2>
<p>DeltaForge takes a fundamentally different approach to schema handling than many CDC tools. Rather than normalizing all database schemas into a universal type system, <strong>each source defines and owns its schema semantics</strong>.</p>
<p>This means:</p>
<ul>
<li><strong>MySQL schemas capture MySQL semantics</strong> - column types like <code>bigint(20) unsigned</code>, <code>varchar(255)</code>, and <code>json</code> are preserved exactly as MySQL defines them</li>
<li><strong>PostgreSQL schemas capture PostgreSQL semantics</strong> - arrays, custom types, and pg-specific attributes remain intact</li>
<li><strong>No lossy normalization</strong> - you donâ€™t lose precision or database-specific information by forcing everything into a common format</li>
</ul>
<p>This design avoids the maintenance burden of keeping a universal type system synchronized across all databases, and it ensures that downstream consumers receive schemas that accurately reflect the source databaseâ€™s capabilities and constraints.</p>
<h2 id="the-sourceschema-trait"><a class="header" href="#the-sourceschema-trait">The SourceSchema Trait</a></h2>
<p>Every CDC source implements the <code>SourceSchema</code> trait, which provides a common interface for fingerprinting and column access while allowing source-specific schema representations:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub trait SourceSchema: Serialize + DeserializeOwned + Send + Sync {
    /// Source type identifier (e.g., "mysql", "postgres", "mongodb").
    fn source_kind(&amp;self) -&gt; &amp;'static str;

    /// Content-addressable fingerprint for change detection.
    /// Two schemas with the same fingerprint are identical.
    fn fingerprint(&amp;self) -&gt; String;

    /// Column/field names in ordinal order.
    fn column_names(&amp;self) -&gt; Vec&lt;&amp;str&gt;;

    /// Primary key column names.
    fn primary_key(&amp;self) -&gt; Vec&lt;&amp;str&gt;;

    /// Human-readable description.
    fn describe(&amp;self) -&gt; String;
}
<span class="boring">}</span></code></pre>
<h3 id="fingerprinting"><a class="header" href="#fingerprinting">Fingerprinting</a></h3>
<p>Schema fingerprints use SHA-256 hashing over JSON-serialized content to provide:</p>
<ul>
<li><strong>Stability</strong> - the same schema always produces the same fingerprint</li>
<li><strong>Change detection</strong> - any structural change produces a different fingerprint</li>
<li><strong>Content-addressability</strong> - fingerprints can be used as cache keys or deduplication identifiers</li>
</ul>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn compute_fingerprint&lt;T: Serialize&gt;(value: &amp;T) -&gt; String {
    let json = serde_json::to_vec(value).unwrap_or_default();
    let hash = Sha256::digest(&amp;json);
    format!("sha256:{}", hex::encode(hash))
}
<span class="boring">}</span></code></pre>
<p>The fingerprint only includes structurally significant fields. For MySQL, this means columns and primary key are included, but engine and charset are excluded since they donâ€™t affect how CDC events should be interpreted.</p>
<h2 id="mysql-schema-implementation"><a class="header" href="#mysql-schema-implementation">MySQL Schema Implementation</a></h2>
<p>The <code>MySqlTableSchema</code> struct captures comprehensive MySQL table metadata:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct MySqlTableSchema {
    /// Columns in ordinal order
    pub columns: Vec&lt;MySqlColumn&gt;,
    
    /// Primary key column names
    pub primary_key: Vec&lt;String&gt;,
    
    /// Storage engine (InnoDB, MyISAM, etc.)
    pub engine: Option&lt;String&gt;,
    
    /// Default charset
    pub charset: Option&lt;String&gt;,
    
    /// Default collation
    pub collation: Option&lt;String&gt;,
}

pub struct MySqlColumn {
    pub name: String,
    pub column_type: String,      // e.g., "bigint(20) unsigned"
    pub data_type: String,        // e.g., "bigint"
    pub nullable: bool,
    pub ordinal_position: u32,
    pub default_value: Option&lt;String&gt;,
    pub extra: Option&lt;String&gt;,    // e.g., "auto_increment"
    pub comment: Option&lt;String&gt;,
    pub char_max_length: Option&lt;i64&gt;,
    pub numeric_precision: Option&lt;i64&gt;,
    pub numeric_scale: Option&lt;i64&gt;,
}
<span class="boring">}</span></code></pre>
<p>Schema information is fetched from <code>INFORMATION_SCHEMA</code> at startup and cached for the pipelineâ€™s lifetime.</p>
<h2 id="schema-registry-architecture"><a class="header" href="#schema-registry-architecture">Schema Registry Architecture</a></h2>
<p>The schema registry serves three core functions:</p>
<ol>
<li><strong>Version tracking</strong> - maintains a history of schema versions per table</li>
<li><strong>Change detection</strong> - compares fingerprints to detect DDL changes</li>
<li><strong>Replay correlation</strong> - associates schemas with checkpoint positions for accurate replay</li>
</ol>
<h3 id="schema-versions"><a class="header" href="#schema-versions">Schema Versions</a></h3>
<p>Each registered schema version includes:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>version</code></td><td>Per-table version number (starts at 1)</td></tr>
<tr><td><code>hash</code></td><td>Content fingerprint for deduplication</td></tr>
<tr><td><code>schema_json</code></td><td>Full schema as JSON</td></tr>
<tr><td><code>registered_at</code></td><td>Registration timestamp</td></tr>
<tr><td><code>sequence</code></td><td>Global monotonic sequence number</td></tr>
<tr><td><code>checkpoint</code></td><td>Source checkpoint at registration time</td></tr>
</tbody>
</table>
</div>
<h3 id="sequence-numbers-for-replay"><a class="header" href="#sequence-numbers-for-replay">Sequence Numbers for Replay</a></h3>
<p>The registry maintains a global monotonic sequence counter. When a schema is registered, it receives the next sequence number. Events carry this sequence number, enabling the replay engine to look up the correct schema version:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// During replay: find schema active at event's sequence
let schema = registry.get_at_sequence(tenant, db, table, event.schema_sequence);
<span class="boring">}</span></code></pre>
<p>This ensures events are always interpreted with the schema that was active when they were produced, even if the table structure has since changed.</p>
<h3 id="checkpoint-correlation"><a class="header" href="#checkpoint-correlation">Checkpoint Correlation</a></h3>
<p>Schemas can be registered with an associated checkpoint, creating a correlation between schema versions and source positions:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>registry.register_with_checkpoint(
    tenant, db, table, 
    &amp;fingerprint, 
    &amp;schema_json,
    Some(checkpoint_bytes),  // Optional: binlog position when schema was observed
).await?;
<span class="boring">}</span></code></pre>
<p>This correlation supports scenarios like:</p>
<ul>
<li>Replaying events from a specific checkpoint with the correct schema</li>
<li>Determining which schema was active at a particular binlog position</li>
<li>Rolling back schema state along with checkpoint rollback</li>
</ul>
<h2 id="schema-loader"><a class="header" href="#schema-loader">Schema Loader</a></h2>
<p>The <code>MySqlSchemaLoader</code> handles schema discovery and caching:</p>
<h3 id="pattern-expansion"><a class="header" href="#pattern-expansion">Pattern Expansion</a></h3>
<p>Tables are specified using patterns that support wildcards:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Pattern</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>db.table</code></td><td>Exact match</td></tr>
<tr><td><code>db.*</code></td><td>All tables in database</td></tr>
<tr><td><code>db.prefix%</code></td><td>Tables starting with prefix</td></tr>
<tr><td><code>%.table</code></td><td>Table in any database</td></tr>
</tbody>
</table>
</div>
<h3 id="preloading"><a class="header" href="#preloading">Preloading</a></h3>
<p>At startup, the loader expands patterns and preloads all matching schemas:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let schema_loader = MySqlSchemaLoader::new(dsn, registry, tenant);
let tracked_tables = schema_loader.preload(&amp;["shop.orders", "shop.order_%"]).await?;
<span class="boring">}</span></code></pre>
<p>This ensures schemas are available before the first CDC event arrives.</p>
<h3 id="caching"><a class="header" href="#caching">Caching</a></h3>
<p>Loaded schemas are cached to avoid repeated <code>INFORMATION_SCHEMA</code> queries:</p>
<pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Fast path: return cached schema
if let Some(cached) = cache.get(&amp;(db, table)) {
    return Ok(cached.clone());
}

// Slow path: fetch from database, register, cache
let schema = fetch_schema(db, table).await?;
let version = registry.register(...).await?;
cache.insert((db, table), loaded_schema);
<span class="boring">}</span></code></pre>
<h2 id="ddl-handling"><a class="header" href="#ddl-handling">DDL Handling</a></h2>
<p>When the binlog contains DDL events, the schema loader responds:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>DDL Type</th><th>Action</th></tr>
</thead>
<tbody>
<tr><td><code>CREATE TABLE</code></td><td>Schema loaded on first row event</td></tr>
<tr><td><code>ALTER TABLE</code></td><td>Cache invalidated, reloaded on next row</td></tr>
<tr><td><code>DROP TABLE</code></td><td>Cache entry removed</td></tr>
<tr><td><code>TRUNCATE</code></td><td>No schema change</td></tr>
<tr><td><code>RENAME TABLE</code></td><td>Old removed, new loaded on first row</td></tr>
</tbody>
</table>
</div>
<p>DDL detection uses the <code>QueryEvent</code> type in the binlog. On DDL, the entire databaseâ€™s schema cache is invalidated since MySQL doesnâ€™t always specify the exact table in DDL events.</p>
<h2 id="api-endpoints"><a class="header" href="#api-endpoints">API Endpoints</a></h2>
<h3 id="reload-schemas"><a class="header" href="#reload-schemas">Reload Schemas</a></h3>
<p>Force reload schemas from the database:</p>
<pre><code class="language-bash">curl -X POST http://localhost:8080/pipelines/{name}/schemas/reload
</code></pre>
<p>This clears the cache and re-fetches schemas for all tracked tables.</p>
<h3 id="list-cached-schemas"><a class="header" href="#list-cached-schemas">List Cached Schemas</a></h3>
<p>View currently cached schemas:</p>
<pre><code class="language-bash">curl http://localhost:8080/pipelines/{name}/schemas
</code></pre>
<h2 id="limitations"><a class="header" href="#limitations">Limitations</a></h2>
<ul>
<li><strong>In-memory registry</strong> - Schema versions are lost on restart. Persistent backends (SQLite, then PostgreSQL for HA) are planned.</li>
<li><strong>No cross-pipeline sharing</strong> - Each pipeline maintains its own registry instance</li>
<li><strong>Pattern expansion at startup</strong> - New tables matching patterns require pipeline restart or reload</li>
</ul>
<h2 id="best-practices-1"><a class="header" href="#best-practices-1">Best Practices</a></h2>
<ol>
<li><strong>Use explicit table patterns</strong> in production to avoid accidentally capturing unwanted tables</li>
<li><strong>Monitor schema reload times</strong> - slow reloads may indicate overly broad patterns</li>
<li><strong>Trigger schema reload after DDL</strong> if your deployment process modifies schemas</li>
<li><strong>Include schema version in downstream events</strong> for consumers that need schema evolution awareness</li>
</ol>
<h2 id="troubleshooting-2"><a class="header" href="#troubleshooting-2">Troubleshooting</a></h2>
<h3 id="unknown-table_id-errors"><a class="header" href="#unknown-table_id-errors">Unknown table_id Errors</a></h3>
<pre><code>WARN write_rows for unknown table_id, table_id=42
</code></pre>
<p>The binlog contains row events for a table not in the table_map. This happens when:</p>
<ul>
<li>A table was created after the CDC stream started</li>
<li>Table patterns donâ€™t match the table</li>
</ul>
<p>Solution: Trigger a schema reload via the REST API.</p>
<h3 id="schema-fetch-returned-0-columns"><a class="header" href="#schema-fetch-returned-0-columns">Schema Fetch Returned 0 Columns</a></h3>
<pre><code>WARN schema fetch returned 0 columns, db=shop, table=orders
</code></pre>
<p>Usually indicates:</p>
<ul>
<li>Table doesnâ€™t exist</li>
<li>MySQL user lacks <code>SELECT</code> privilege on <code>INFORMATION_SCHEMA</code></li>
<li>Table was dropped between detection and schema load</li>
</ul>
<h3 id="slow-schema-loading"><a class="header" href="#slow-schema-loading">Slow Schema Loading</a></h3>
<pre><code>WARN slow schema fetch, db=shop, table=orders, ms=350
</code></pre>
<p>Consider:</p>
<ul>
<li>Narrowing table patterns to reduce the number of tables</li>
<li>Using exact table names instead of wildcards</li>
<li>Verifying network latency to the MySQL server</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="schema-sensing-2"><a class="header" href="#schema-sensing-2">Schema Sensing</a></h1>
<p>Schema sensing automatically infers and tracks schema structure from JSON event payloads. This complements the schema registry by discovering schema from data rather than database metadata.</p>
<h2 id="when-to-use-schema-sensing"><a class="header" href="#when-to-use-schema-sensing">When to Use Schema Sensing</a></h2>
<p>Schema sensing is useful when:</p>
<ul>
<li><strong>Source doesnâ€™t provide schema</strong>: Some sources emit JSON without metadata</li>
<li><strong>JSON columns</strong>: Database JSON/JSONB columns have dynamic structure</li>
<li><strong>Schema evolution tracking</strong>: Detect when payload structure changes over time</li>
<li><strong>Downstream integration</strong>: Generate JSON Schema for consumers</li>
<li><strong>Dynamic map keys</strong>: Session IDs, trace IDs, or other high-cardinality keys in JSON</li>
</ul>
<h2 id="how-it-works-1"><a class="header" href="#how-it-works-1">How It Works</a></h2>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Event     â”‚â”€â”€â”€â”€â–¶â”‚  Schema Sensor  â”‚â”€â”€â”€â”€â–¶â”‚  Inferred Schema â”‚
â”‚   Payload    â”‚     â”‚   (sampling)    â”‚     â”‚   + Fingerprint  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ Structure Cache â”‚
                     â”‚ + HC Classifier â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<ol>
<li><strong>Observation</strong>: Events flow through the sensor during batch processing</li>
<li><strong>Sampling</strong>: Not every event is fully analyzed (configurable rate)</li>
<li><strong>Deep inspection</strong>: Nested JSON structures are recursively analyzed</li>
<li><strong>High-cardinality detection</strong>: Dynamic map keys are classified and normalized</li>
<li><strong>Fingerprinting</strong>: Schema changes are detected via SHA-256 fingerprints</li>
<li><strong>Caching</strong>: Repeated structures skip full analysis for performance</li>
</ol>
<h2 id="high-cardinality-key-handling"><a class="header" href="#high-cardinality-key-handling">High-Cardinality Key Handling</a></h2>
<p>JSON payloads often contain dynamic keys like session IDs, trace IDs, or user-generated identifiers:</p>
<pre><code class="language-json">{
  "id": 1,
  "sessions": {
    "sess_abc123": {"user_id": 42, "started_at": 1700000000},
    "sess_xyz789": {"user_id": 43, "started_at": 1700000001}
  }
}
</code></pre>
<p>Without special handling, each unique key (<code>sess_abc123</code>, <code>sess_xyz789</code>) triggers a â€œschema evolutionâ€ event, causing:</p>
<ul>
<li>0% cache hit rate</li>
<li>Constant false evolution alerts</li>
<li>Unbounded schema growth</li>
</ul>
<h3 id="how-it-works-1-1"><a class="header" href="#how-it-works-1-1">How It Works</a></h3>
<p>DeltaForge uses probabilistic data structures (HyperLogLog, SpaceSaving) to classify fields:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Classification</th><th>Description</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td><strong>Stable fields</strong></td><td>Appear in most events</td><td><code>id</code>, <code>type</code>, <code>timestamp</code></td></tr>
<tr><td><strong>Dynamic fields</strong></td><td>Unique per event, high cardinality</td><td><code>sess_*</code>, <code>trace_*</code>, <code>uuid_*</code></td></tr>
</tbody>
</table>
</div>
<p>When dynamic fields are detected, the schema sensor:</p>
<ol>
<li><strong>Normalizes keys</strong>: Replaces <code>sess_abc123</code> with <code>&lt;dynamic&gt;</code> placeholder</li>
<li><strong>Uses adaptive hashing</strong>: Structure cache ignores dynamic key names</li>
<li><strong>Produces stable fingerprints</strong>: Same schema despite different keys</li>
</ol>
<h3 id="results"><a class="header" href="#results">Results</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Scenario</th><th>Without HC</th><th>With HC</th></tr>
</thead>
<tbody>
<tr><td>Nested dynamic keys</td><td>100% evolution rate</td><td>&lt;1% evolution rate</td></tr>
<tr><td>Top-level dynamic keys</td><td>0% cache hits</td><td>&gt;99% cache hits</td></tr>
<tr><td>Stable structs</td><td>Baseline</td><td>~20% overhead during warmup, then ~0%</td></tr>
</tbody>
</table>
</div>
<h2 id="configuration-9"><a class="header" href="#configuration-9">Configuration</a></h2>
<table>
<tr>
<td width="50%" valign="top">
<h3 id="example-2"><a class="header" href="#example-2">Example</a></h3>
<pre><code class="language-yaml">spec:
  schema_sensing:
    enabled: true
    
    deep_inspect:
      enabled: true
      max_depth: 3
      max_sample_size: 500
    
    sampling:
      warmup_events: 50
      sample_rate: 5
      structure_cache: true
      structure_cache_size: 50
    
    high_cardinality:
      enabled: true
      min_events: 100
      stable_threshold: 0.5
      min_dynamic_fields: 5
      confidence_threshold: 0.7
      reevaluate_interval: 10000
</code></pre>
</td>
<td width="50%" valign="top">
<h3 id="options"><a class="header" href="#options">Options</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Field</th><th>Type</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>enabled</code></td><td>bool</td><td><code>false</code></td><td>Enable schema sensing</td></tr>
<tr><td><strong>deep_inspect</strong></td><td></td><td></td><td></td></tr>
<tr><td><code>enabled</code></td><td>bool</td><td><code>false</code></td><td>Inspect nested JSON</td></tr>
<tr><td><code>max_depth</code></td><td>int</td><td><code>3</code></td><td>Max nesting depth</td></tr>
<tr><td><code>max_sample_size</code></td><td>int</td><td><code>500</code></td><td>Max events for deep analysis</td></tr>
<tr><td><strong>sampling</strong></td><td></td><td></td><td></td></tr>
<tr><td><code>warmup_events</code></td><td>int</td><td><code>50</code></td><td>Full analysis before sampling</td></tr>
<tr><td><code>sample_rate</code></td><td>int</td><td><code>5</code></td><td>After warmup, analyze 1 in N</td></tr>
<tr><td><code>structure_cache</code></td><td>bool</td><td><code>true</code></td><td>Cache structure hashes</td></tr>
<tr><td><code>structure_cache_size</code></td><td>int</td><td><code>50</code></td><td>Max cached per table</td></tr>
<tr><td><strong>high_cardinality</strong></td><td></td><td></td><td></td></tr>
<tr><td><code>enabled</code></td><td>bool</td><td><code>true</code></td><td>Detect dynamic map keys</td></tr>
<tr><td><code>min_events</code></td><td>int</td><td><code>100</code></td><td>Events before classification</td></tr>
<tr><td><code>stable_threshold</code></td><td>float</td><td><code>0.5</code></td><td>Frequency for stable fields</td></tr>
<tr><td><code>min_dynamic_fields</code></td><td>int</td><td><code>5</code></td><td>Min unique for map detection</td></tr>
<tr><td><code>confidence_threshold</code></td><td>float</td><td><code>0.7</code></td><td>Required confidence</td></tr>
<tr><td><code>reevaluate_interval</code></td><td>int</td><td><code>10000</code></td><td>Re-check interval (0=never)</td></tr>
</tbody>
</table>
</div>
</td>
</tr>

</table>

<h2 id="inferred-types"><a class="header" href="#inferred-types">Inferred Types</a></h2>
<p>Schema sensing infers these JSON types:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Type</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>null</code></td><td>JSON null value</td></tr>
<tr><td><code>boolean</code></td><td>true/false</td></tr>
<tr><td><code>integer</code></td><td>Whole numbers</td></tr>
<tr><td><code>number</code></td><td>Floating point numbers</td></tr>
<tr><td><code>string</code></td><td>Text values</td></tr>
<tr><td><code>array</code></td><td>JSON arrays (element types tracked)</td></tr>
<tr><td><code>object</code></td><td>Nested objects (fields recursively analyzed)</td></tr>
</tbody>
</table>
</div>
<p>For fields with varying types across events, all observed types are recorded.</p>
<h2 id="schema-evolution-1"><a class="header" href="#schema-evolution-1">Schema Evolution</a></h2>
<p>When schema structure changes, the sensor:</p>
<ol>
<li><strong>Detects change</strong>: Fingerprint differs from previous version</li>
<li><strong>Increments sequence</strong>: Monotonic version number increases</li>
<li><strong>Logs evolution</strong>: Emits structured log with old/new fingerprints</li>
<li><strong>Updates cache</strong>: New structure becomes current</li>
</ol>
<p>Evolution events are available via the REST API and can trigger alerts.</p>
<h2 id="stabilization"><a class="header" href="#stabilization">Stabilization</a></h2>
<p>After observing enough events, a schema â€œstabilizesâ€:</p>
<ul>
<li>Warmup phase completes</li>
<li>Structure stops changing</li>
<li>Sampling rate takes effect</li>
<li>Cache hit rate increases</li>
</ul>
<p>Stabilized schemas have <code>stabilized: true</code> in API responses.</p>
<h2 id="api-access"><a class="header" href="#api-access">API Access</a></h2>
<h3 id="list-inferred-schemas-1"><a class="header" href="#list-inferred-schemas-1">List Inferred Schemas</a></h3>
<pre><code class="language-bash">curl http://localhost:8080/pipelines/my-pipeline/sensing/schemas
</code></pre>
<h3 id="get-schema-details-1"><a class="header" href="#get-schema-details-1">Get Schema Details</a></h3>
<pre><code class="language-bash">curl http://localhost:8080/pipelines/my-pipeline/sensing/schemas/orders
</code></pre>
<h3 id="export-as-json-schema"><a class="header" href="#export-as-json-schema">Export as JSON Schema</a></h3>
<pre><code class="language-bash">curl http://localhost:8080/pipelines/my-pipeline/sensing/schemas/orders/json-schema
</code></pre>
<h3 id="cache-statistics"><a class="header" href="#cache-statistics">Cache Statistics</a></h3>
<pre><code class="language-bash">curl http://localhost:8080/pipelines/my-pipeline/sensing/stats
</code></pre>
<h3 id="dynamic-map-classifications"><a class="header" href="#dynamic-map-classifications">Dynamic Map Classifications</a></h3>
<pre><code class="language-bash">curl http://localhost:8080/pipelines/my-pipeline/sensing/schemas/orders/classifications
</code></pre>
<p>Response:</p>
<pre><code class="language-json">{
  "table": "orders",
  "paths": {
    "": {"stable_fields": ["id", "type", "timestamp"], "has_dynamic_fields": false},
    "sessions": {"stable_fields": [], "has_dynamic_fields": true, "unique_keys": 1523},
    "metadata": {"stable_fields": ["version"], "has_dynamic_fields": true, "unique_keys": 42}
  }
}
</code></pre>
<h2 id="drift-detection-1"><a class="header" href="#drift-detection-1">Drift Detection</a></h2>
<p>Schema sensing integrates with drift detection to compare:</p>
<ul>
<li><strong>Expected schema</strong>: From database metadata (schema registry)</li>
<li><strong>Observed schema</strong>: From event payloads (schema sensing)</li>
</ul>
<p>When mismatches occur, drift events are recorded:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Drift Type</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>unexpected_null</code></td><td>Non-nullable column has null values</td></tr>
<tr><td><code>type_mismatch</code></td><td>Observed type differs from declared type</td></tr>
<tr><td><code>undeclared_column</code></td><td>Field in data not in schema</td></tr>
<tr><td><code>missing_column</code></td><td>Schema field never seen in data</td></tr>
<tr><td><code>json_structure_change</code></td><td>JSON column structure changed</td></tr>
</tbody>
</table>
</div>
<p>Access drift data via:</p>
<pre><code class="language-bash">curl http://localhost:8080/pipelines/my-pipeline/drift
</code></pre>
<h2 id="performance-considerations-2"><a class="header" href="#performance-considerations-2">Performance Considerations</a></h2>
<h3 id="sampling-tradeoffs"><a class="header" href="#sampling-tradeoffs">Sampling Tradeoffs</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Setting</th><th>Effect</th></tr>
</thead>
<tbody>
<tr><td>Higher <code>warmup_events</code></td><td>Better initial accuracy, slower stabilization</td></tr>
<tr><td>Higher <code>sample_rate</code></td><td>Lower CPU usage, slower evolution detection</td></tr>
<tr><td>Larger <code>structure_cache_size</code></td><td>More memory, better hit rate</td></tr>
</tbody>
</table>
</div>
<h3 id="recommended-settings"><a class="header" href="#recommended-settings">Recommended Settings</a></h3>
<p><strong>High-throughput pipelines</strong> (&gt;10k events/sec):</p>
<pre><code class="language-yaml">sampling:
  warmup_events: 100
  sample_rate: 10
  structure_cache: true
  structure_cache_size: 100
high_cardinality:
  enabled: true
  min_events: 200
</code></pre>
<p><strong>Schema evolution monitoring</strong>:</p>
<pre><code class="language-yaml">sampling:
  warmup_events: 25
  sample_rate: 2
  structure_cache: true
high_cardinality:
  enabled: true
  min_events: 50
</code></pre>
<p><strong>Payloads with dynamic keys</strong> (session stores, feature flags):</p>
<pre><code class="language-yaml">sampling:
  structure_cache: true
  structure_cache_size: 50
high_cardinality:
  enabled: true
  min_events: 100
  min_dynamic_fields: 3
  stable_threshold: 0.5
</code></pre>
<p><strong>Development/debugging</strong>:</p>
<pre><code class="language-yaml">sampling:
  warmup_events: 10
  sample_rate: 1  # Analyze every event
high_cardinality:
  enabled: true
  min_events: 20  # Faster classification
</code></pre>
<h2 id="example-json-column-sensing"><a class="header" href="#example-json-column-sensing">Example: JSON Column Sensing</a></h2>
<p>For tables with JSON columns, sensing reveals the internal structure:</p>
<pre><code class="language-yaml"># Database schema shows: metadata JSON
# Sensing reveals:
fields:
  - name: "metadata.user_agent"
    types: ["string"]
    nullable: false
  - name: "metadata.ip_address"
    types: ["string"]
    nullable: true
  - name: "metadata.tags"
    types: ["array"]
    array_element_types: ["string"]
</code></pre>
<p>This enables downstream consumers to understand JSON column structure without manual documentation.</p>
<h2 id="metrics"><a class="header" href="#metrics">Metrics</a></h2>
<p>Schema sensing emits these Prometheus metrics:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Metric</th><th>Type</th><th>Labels</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>deltaforge_schema_events_total</code></td><td>Counter</td><td><code>table</code></td><td>Total events observed</td></tr>
<tr><td><code>deltaforge_schema_cache_hits_total</code></td><td>Counter</td><td><code>table</code></td><td>Structure cache hits</td></tr>
<tr><td><code>deltaforge_schema_cache_misses_total</code></td><td>Counter</td><td><code>table</code></td><td>Structure cache misses</td></tr>
<tr><td><code>deltaforge_schema_evolutions_total</code></td><td>Counter</td><td><code>table</code></td><td>Schema evolutions detected</td></tr>
<tr><td><code>deltaforge_schema_tables_total</code></td><td>Gauge</td><td>-</td><td>Tables with detected schemas</td></tr>
<tr><td><code>deltaforge_schema_dynamic_maps_total</code></td><td>Gauge</td><td>-</td><td>Paths classified as dynamic maps</td></tr>
<tr><td><code>deltaforge_schema_sensing_seconds</code></td><td>Histogram</td><td><code>table</code></td><td>Per-event sensing latency</td></tr>
</tbody>
</table>
</div>
<h3 id="example-queries"><a class="header" href="#example-queries">Example Queries</a></h3>
<pre><code class="language-promql"># Cache hit rate per table
sum(rate(deltaforge_schema_cache_hits_total[5m])) by (table)
/
sum(rate(deltaforge_schema_events_total[5m])) by (table)

# Schema evolution rate (should be near zero after warmup)
sum(rate(deltaforge_schema_evolutions_total[5m])) by (table)

# P99 sensing latency
histogram_quantile(0.99, rate(deltaforge_schema_sensing_seconds_bucket[5m]))
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="observability-playbook"><a class="header" href="#observability-playbook">Observability playbook</a></h1>
<p>DeltaForge already ships a Prometheus exporter, structured logging, and a panic hook. The runtime now emits source ingress counters, batching/processor histograms, and sink latency/throughput so operators can build production dashboards immediately. The tables below capture what is wired today and the remaining gaps to make the platform production ready for data and infra engineers.</p>
<h2 id="what-exists-today"><a class="header" href="#what-exists-today">What exists today</a></h2>
<ul>
<li>Prometheus endpoint served at <code>/metrics</code> (default <code>0.0.0.0:9000</code>) with descriptors for pipeline counts, source/sink counters, and a stage latency histogram. The recorder is installed automatically when metrics are enabled.</li>
<li>Structured logging via <code>tracing_subscriber</code> with JSON output by default, optional targets, and support for <code>RUST_LOG</code> overrides.</li>
<li>Panic hook increments a <code>deltaforge_panics_total</code> counter and logs captured panics before delegating to the default hook.</li>
</ul>
<h2 id="instrumentation-gaps-and-recommendations"><a class="header" href="#instrumentation-gaps-and-recommendations">Instrumentation gaps and recommendations</a></h2>
<p>The sections below call out concrete metrics and log events to add per component. All metrics should include <code>pipeline</code>, <code>tenant</code>, and component identifiers where applicable so users can aggregate across fleets.</p>
<h3 id="sources-mysqlpostgres"><a class="header" href="#sources-mysqlpostgres">Sources (MySQL/Postgres)</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Status</th><th>Metric/log</th><th>Rationale</th></tr>
</thead>
<tbody>
<tr><td>âœ… Implemented</td><td><code>deltaforge_source_events_total{pipeline,source,table}</code> counter increments when MySQL events are handed to the coordinator.</td><td>Surfaces ingress per table and pipeline.</td></tr>
<tr><td>âœ… Implemented</td><td><code>deltaforge_source_reconnects_total{pipeline,source}</code> counter when binlog reads reconnect.</td><td>Makes retry storms visible.</td></tr>
<tr><td>ğŸš§ Gap</td><td><code>deltaforge_source_lag_seconds{pipeline,source}</code> gauge based on binlog/WAL position vs. server time.</td><td>Alert when sources fall behind.</td></tr>
<tr><td>ğŸš§ Gap</td><td><code>deltaforge_source_idle_seconds{pipeline,source}</code> gauge updated when no events arrive within the inactivity window.</td><td>Catch stuck readers before downstream backlogs form.</td></tr>
</tbody>
</table>
</div>
<h3 id="coordinator-and-batching"><a class="header" href="#coordinator-and-batching">Coordinator and batching</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Status</th><th>Metric/log</th><th>Rationale</th></tr>
</thead>
<tbody>
<tr><td>âœ… Implemented</td><td><code>deltaforge_batch_events{pipeline}</code> and <code>deltaforge_batch_bytes{pipeline}</code> histograms in <code>Coordinator::process_deliver_and_maybe_commit</code>.</td><td>Tune batching policies with data.</td></tr>
<tr><td>âœ… Implemented</td><td><code>deltaforge_stage_latency_seconds{pipeline,stage,trigger}</code> histogram for processor stage.</td><td>Provides batch timing per trigger (timer/limits/shutdown).</td></tr>
<tr><td>âœ… Implemented</td><td><code>deltaforge_processor_latency_seconds{pipeline,processor}</code> histogram around every processor invocation.</td><td>Identify slow user functions.</td></tr>
<tr><td>ğŸš§ Gap</td><td><code>deltaforge_pipeline_channel_depth{pipeline}</code> gauge from <code>mpsc::Sender::capacity()</code>/<code>len()</code>.</td><td>Detect backpressure between sources and coordinator.</td></tr>
<tr><td>ğŸš§ Gap</td><td>Checkpoint outcome counters/logs (<code>deltaforge_checkpoint_success_total</code> / <code>_failure_total</code>).</td><td>Alert on persistence regressions and correlate to data loss risk.</td></tr>
</tbody>
</table>
</div>
<h3 id="sinks-kafkarediscustom"><a class="header" href="#sinks-kafkarediscustom">Sinks (Kafka/Redis/custom)</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Status</th><th>Metric/log</th><th>Rationale</th></tr>
</thead>
<tbody>
<tr><td>âœ… Implemented</td><td><code>deltaforge_sink_events_total{pipeline,sink}</code> counter and <code>deltaforge_sink_latency_seconds{pipeline,sink}</code> histogram around each send.</td><td>Throughput and responsiveness per sink.</td></tr>
<tr><td>âœ… Implemented</td><td><code>deltaforge_sink_batch_total{pipeline,sink}</code> counter for send.</td><td>Number of batches sent per sink.</td></tr>
<tr><td>ğŸš§ Gap</td><td>Error taxonomy in <code>deltaforge_sink_failures_total</code> (add <code>kind</code>/<code>details</code>).</td><td>Easier alerting on specific failure classes (auth, timeout, schema).</td></tr>
<tr><td>ğŸš§ Gap</td><td>Backpressure gauge for client buffers (rdkafka queue, Redis pipeline depth).</td><td>Early signal before errors occur.</td></tr>
<tr><td>ğŸš§ Gap</td><td>Drop/skip counters from processors/sinks.</td><td>Auditing and reconciliation.</td></tr>
</tbody>
</table>
</div>
<h3 id="control-plane-and-health-endpoints"><a class="header" href="#control-plane-and-health-endpoints">Control plane and health endpoints</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Need</th><th>Suggested metric/log</th><th>Rationale</th></tr>
</thead>
<tbody>
<tr><td>API request accounting</td><td><code>deltaforge_api_requests_total{route,method,status}</code> counter and latency histogram using Axum middleware.</td><td>Production-grade visibility of operator actions.</td></tr>
<tr><td>Ready/Liveness transitions</td><td>Logs with pipeline counts and per-pipeline status when readiness changes.</td><td>Explain probe failures in log aggregation.</td></tr>
<tr><td>Pipeline lifecycle</td><td>Counters for create/patch/stop actions with success/error labels; include tenant and caller metadata in logs.</td><td>Auditable control-plane operations.</td></tr>
</tbody>
</table>
</div>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="examples-2"><a class="header" href="#examples-2">Examples</a></h1>
<p>Complete pipeline configurations demonstrating common DeltaForge use cases. Each example is ready to run with minimal modifications.</p>
<h2 id="available-examples"><a class="header" href="#available-examples">Available Examples</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Example</th><th>Source</th><th>Sink(s)</th><th>Key Features</th></tr>
</thead>
<tbody>
<tr><td><a href="#mysql-to-redis">MySQL to Redis</a></td><td>MySQL</td><td>Redis</td><td>JavaScript processor, PII redaction</td></tr>
<tr><td><a href="#example-turso-to-kafka">Turso to Kafka</a></td><td>Turso/libSQL</td><td>Kafka</td><td>Native CDC, CloudEvents envelope</td></tr>
<tr><td><a href="#postgresql-to-nats">PostgreSQL to NATS</a></td><td>PostgreSQL</td><td>NATS</td><td>Logical replication, CloudEvents</td></tr>
<tr><td><a href="#multi-sink-fan-out-1">Multi-Sink Fan-Out</a></td><td>MySQL</td><td>Kafka + Redis + NATS</td><td>Multiple envelopes, selective checkpointing</td></tr>
<tr><td><a href="#event-filtering-with-javascript">Event Filtering</a></td><td>MySQL</td><td>Kafka</td><td>JavaScript filtering, PII redaction</td></tr>
<tr><td><a href="#schema-sensing-and-drift-detection">Schema Sensing</a></td><td>PostgreSQL</td><td>Kafka</td><td>JSON schema inference, drift detection</td></tr>
<tr><td><a href="#production-kafka-configuration">Production Kafka</a></td><td>PostgreSQL</td><td>Kafka</td><td>SASL/SSL auth, exactly-once, tuning</td></tr>
<tr><td><a href="#redis-cache-invalidation">Cache Invalidation</a></td><td>MySQL</td><td>Redis</td><td>CDC stream for cache invalidation workers</td></tr>
<tr><td><a href="#audit-trail-and-compliance-logging">Audit Trail</a></td><td>PostgreSQL</td><td>Kafka</td><td>Compliance logging, PII redaction</td></tr>
<tr><td><a href="#real-time-analytics-preprocessing-pipeline">Analytics Preprocessing</a></td><td>MySQL</td><td>Kafka + Redis</td><td>Metrics enrichment, analytics stream</td></tr>
<tr><td><a href="#outbox-pattern-1">Outbox Pattern</a></td><td>MySQL</td><td>Kafka</td><td>Transactional outbox, raw payload, per-aggregate routing</td></tr>
</tbody>
</table>
</div>
<h2 id="quick-start"><a class="header" href="#quick-start">Quick Start</a></h2>
<ol>
<li><strong>Set environment variables</strong> for your database and sink connections</li>
<li><strong>Copy the example</strong> to a <code>.yaml</code> file</li>
<li><strong>Run DeltaForge</strong>:
<pre><code class="language-bash">cargo run -p runner -- --config your-pipeline.yaml
</code></pre>
</li>
</ol>
<h2 id="examples-by-category"><a class="header" href="#examples-by-category">Examples by Category</a></h2>
<h3 id="getting-started-2"><a class="header" href="#getting-started-2">Getting Started</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Example</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><a href="#mysql-to-redis">MySQL to Redis</a></td><td>Simple pipeline with JavaScript transformation</td></tr>
<tr><td><a href="#example-turso-to-kafka">Turso to Kafka</a></td><td>Edge database to Kafka with CloudEvents</td></tr>
<tr><td><a href="#postgresql-to-nats">PostgreSQL to NATS</a></td><td>PostgreSQL logical replication to NATS</td></tr>
</tbody>
</table>
</div>
<h3 id="production-patterns"><a class="header" href="#production-patterns">Production Patterns</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Example</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><a href="#production-kafka-configuration">Production Kafka</a></td><td>Authentication, exactly-once, performance tuning</td></tr>
<tr><td><a href="#multi-sink-fan-out-1">Multi-Sink Fan-Out</a></td><td>Multiple sinks with different formats</td></tr>
<tr><td><a href="#redis-cache-invalidation">Cache Invalidation</a></td><td>CDC stream for cache invalidation</td></tr>
<tr><td><a href="#outbox-pattern-1">Outbox Pattern</a></td><td>Transactional outbox with raw payload delivery</td></tr>
</tbody>
</table>
</div>
<h3 id="data-processing"><a class="header" href="#data-processing">Data Processing</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Example</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><a href="#event-filtering-with-javascript">Event Filtering</a></td><td>Filter, drop, and redact events</td></tr>
<tr><td><a href="#schema-sensing-and-drift-detection">Schema Sensing</a></td><td>Automatic JSON schema discovery</td></tr>
<tr><td><a href="#real-time-analytics-preprocessing-pipeline">Analytics Preprocessing</a></td><td>Prepare events for analytics platforms</td></tr>
</tbody>
</table>
</div>
<h3 id="compliance--auditing"><a class="header" href="#compliance--auditing">Compliance &amp; Auditing</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Example</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><a href="#audit-trail-and-compliance-logging">Audit Trail</a></td><td>SOC2/HIPAA/GDPR-compliant change tracking</td></tr>
</tbody>
</table>
</div>
<h2 id="examples-by-feature"><a class="header" href="#examples-by-feature">Examples by Feature</a></h2>
<h3 id="envelope-formats-1"><a class="header" href="#envelope-formats-1">Envelope Formats</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Format</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td>Native</td><td><a href="#mysql-to-redis">MySQL to Redis</a>, <a href="#event-filtering-with-javascript">Event Filtering</a>, <a href="#redis-cache-invalidation">Cache Invalidation</a></td></tr>
<tr><td>Debezium</td><td><a href="#schema-sensing-and-drift-detection">Schema Sensing</a>, <a href="#multi-sink-fan-out-1">Multi-Sink Fan-Out</a>, <a href="#production-kafka-configuration">Production Kafka</a></td></tr>
<tr><td>CloudEvents</td><td><a href="#example-turso-to-kafka">Turso to Kafka</a>, <a href="#postgresql-to-nats">PostgreSQL to NATS</a></td></tr>
</tbody>
</table>
</div>
<h3 id="javascript-processors"><a class="header" href="#javascript-processors">JavaScript Processors</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Use Case</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td>PII Redaction</td><td><a href="#mysql-to-redis">MySQL to Redis</a>, <a href="#audit-trail-and-compliance-logging">Audit Trail</a></td></tr>
<tr><td>Event Filtering</td><td><a href="#event-filtering-with-javascript">Event Filtering</a></td></tr>
<tr><td>Enrichment</td><td><a href="#example-turso-to-kafka">Turso to Kafka</a>, <a href="#real-time-analytics-preprocessing-pipeline">Analytics Preprocessing</a></td></tr>
<tr><td>Cache Key Generation</td><td><a href="#redis-cache-invalidation">Cache Invalidation</a></td></tr>
<tr><td>Audit Metadata</td><td><a href="#audit-trail-and-compliance-logging">Audit Trail</a></td></tr>
</tbody>
</table>
</div>
<h3 id="multi-sink-patterns"><a class="header" href="#multi-sink-patterns">Multi-Sink Patterns</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Pattern</th><th>Example</th></tr>
</thead>
<tbody>
<tr><td>Fan-out with different formats</td><td><a href="#multi-sink-fan-out-1">Multi-Sink Fan-Out</a></td></tr>
<tr><td>Primary + best-effort secondary</td><td><a href="#multi-sink-fan-out-1">Multi-Sink Fan-Out</a>, <a href="#real-time-analytics-preprocessing-pipeline">Analytics Preprocessing</a></td></tr>
</tbody>
</table>
</div>
<h2 id="customizing-examples"><a class="header" href="#customizing-examples">Customizing Examples</a></h2>
<h3 id="change-envelope-format"><a class="header" href="#change-envelope-format">Change Envelope Format</a></h3>
<p>All sinks support configurable envelopes. Add to any sink config:</p>
<pre><code class="language-yaml">sinks:
  - type: kafka
    config:
      # ... other config
      envelope:
        type: cloudevents          # or: native, debezium
        type_prefix: "com.example" # required for cloudevents
      encoding: json
</code></pre>
<p>See <a href="#envelopes-and-encodings">Envelopes and Encodings</a> for details.</p>
<h3 id="add-multiple-sinks"><a class="header" href="#add-multiple-sinks">Add Multiple Sinks</a></h3>
<p>Fan out to multiple destinations with different formats:</p>
<pre><code class="language-yaml">sinks:
  - type: kafka
    config:
      id: primary-kafka
      envelope:
        type: debezium
      required: true    # Must succeed for checkpoint
  - type: redis
    config:
      id: cache-redis
      envelope:
        type: native
      required: false   # Best-effort, won't block checkpoint
</code></pre>
<p>See <a href="sinks/README.html">Sinks documentation</a> for multi-sink patterns.</p>
<h3 id="enable-schema-sensing"><a class="header" href="#enable-schema-sensing">Enable Schema Sensing</a></h3>
<p>Automatically discover JSON structure in your data:</p>
<pre><code class="language-yaml">schema_sensing:
  enabled: true
  deep_inspect:
    enabled: true
    max_depth: 3
  sampling:
    warmup_events: 100
    sample_rate: 10
</code></pre>
<p>See <a href="#schema-sensing-2">Schema Sensing</a> for configuration options.</p>
<h2 id="more-resources"><a class="header" href="#more-resources">More Resources</a></h2>
<ul>
<li><a href="#configuration">Configuration Reference</a> - Full spec documentation</li>
<li><a href="#quickstart">Quickstart Guide</a> - Get running in minutes</li>
<li><a href="#troubleshooting-3">Troubleshooting</a> - Common issues and solutions</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="mysql-to-redis"><a class="header" href="#mysql-to-redis">MySQL to Redis</a></h1>
<p>This example streams MySQL binlog events into a Redis stream with an inline JavaScript transformation for PII redaction.</p>
<h2 id="overview-2"><a class="header" href="#overview-2">Overview</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Component</th><th>Configuration</th></tr>
</thead>
<tbody>
<tr><td><strong>Source</strong></td><td>MySQL binlog CDC</td></tr>
<tr><td><strong>Processor</strong></td><td>JavaScript email redaction</td></tr>
<tr><td><strong>Sink</strong></td><td>Redis Streams</td></tr>
<tr><td><strong>Envelope</strong></td><td>Native (configurable)</td></tr>
</tbody>
</table>
</div>
<h2 id="pipeline-configuration"><a class="header" href="#pipeline-configuration">Pipeline Configuration</a></h2>
<pre><code class="language-yaml">metadata:
  name: orders-mysql-to-redis
  tenant: acme

spec:
  source:
    type: mysql
    config:
      id: orders-mysql
      dsn: ${MYSQL_DSN}
      tables:
        - shop.orders

  processors:
    - type: javascript
      id: redact-email
      inline: |
        function processBatch(events) {
          return events.map((event) =&gt; {
            if (event.after &amp;&amp; event.after.email) {
              event.after.email = "[redacted]";
            }
            return event;
          });
        }
      limits:
        timeout_ms: 500

  sinks:
    - type: redis
      config:
        id: orders-redis
        uri: ${REDIS_URI}
        stream: orders
        envelope:
          type: native
        encoding: json
        required: true

  batch:
    max_events: 500
    max_bytes: 1048576
    max_ms: 1000

  commit_policy:
    mode: required
</code></pre>
<h2 id="running-the-example"><a class="header" href="#running-the-example">Running the Example</a></h2>
<h3 id="1-set-environment-variables"><a class="header" href="#1-set-environment-variables">1. Set Environment Variables</a></h3>
<pre><code class="language-bash">export MYSQL_DSN="mysql://user:password@localhost:3306/shop"
export REDIS_URI="redis://localhost:6379"
</code></pre>
<h3 id="2-start-deltaforge-1"><a class="header" href="#2-start-deltaforge-1">2. Start DeltaForge</a></h3>
<pre><code class="language-bash"># Save config as mysql-redis.yaml
cargo run -p runner -- --config mysql-redis.yaml
</code></pre>
<h3 id="3-insert-test-data"><a class="header" href="#3-insert-test-data">3. Insert Test Data</a></h3>
<pre><code class="language-sql">INSERT INTO shop.orders (email, total, status)
VALUES ('alice@example.com', 99.99, 'pending');
</code></pre>
<h3 id="4-verify-in-redis"><a class="header" href="#4-verify-in-redis">4. Verify in Redis</a></h3>
<pre><code class="language-bash">./dev.sh redis-read orders 10
</code></pre>
<p>You should see the event with the email redacted:</p>
<pre><code class="language-json">{
  "before": null,
  "after": {
    "id": 1,
    "email": "[redacted]",
    "total": 99.99,
    "status": "pending"
  },
  "op": "c",
  "ts_ms": 1700000000000
}
</code></pre>
<h2 id="variations"><a class="header" href="#variations">Variations</a></h2>
<h3 id="with-debezium-envelope"><a class="header" href="#with-debezium-envelope">With Debezium Envelope</a></h3>
<p>For Kafka Connect compatibility downstream:</p>
<pre><code class="language-yaml">sinks:
  - type: redis
    config:
      id: orders-redis
      uri: ${REDIS_URI}
      stream: orders
      envelope:
        type: debezium
</code></pre>
<h3 id="multi-sink-fan-out"><a class="header" href="#multi-sink-fan-out">Multi-Sink Fan-Out</a></h3>
<p>Add Kafka alongside Redis for durability:</p>
<pre><code class="language-yaml">sinks:
  - type: kafka
    config:
      id: orders-kafka
      brokers: ${KAFKA_BROKERS}
      topic: orders
      envelope:
        type: debezium
      required: true    # Critical path

  - type: redis
    config:
      id: orders-redis
      uri: ${REDIS_URI}
      stream: orders
      envelope:
        type: native
      required: false   # Best-effort
</code></pre>
<p>With this configuration, checkpoints only wait for Kafka. Redis failures wonâ€™t block the pipeline.</p>
<h3 id="with-schema-sensing"><a class="header" href="#with-schema-sensing">With Schema Sensing</a></h3>
<p>Automatically track schema changes:</p>
<pre><code class="language-yaml">spec:
  # ... source and sinks config ...

  schema_sensing:
    enabled: true
    deep_inspect:
      enabled: true
      max_depth: 3
    sampling:
      warmup_events: 100
      sample_rate: 10
</code></pre>
<h2 id="key-concepts-demonstrated"><a class="header" href="#key-concepts-demonstrated">Key Concepts Demonstrated</a></h2>
<ul>
<li><strong>JavaScript Processors</strong>: Transform events in-flight with custom logic</li>
<li><strong>PII Redaction</strong>: Mask sensitive data before it reaches downstream systems</li>
<li><strong>Envelope Configuration</strong>: Choose output format based on consumer needs</li>
<li><strong>Commit Policy</strong>: Control checkpoint behavior with <code>required</code> flag</li>
</ul>
<h2 id="related-documentation"><a class="header" href="#related-documentation">Related Documentation</a></h2>
<ul>
<li><a href="#mysql-source">MySQL Source</a> - Prerequisites and configuration</li>
<li><a href="#redis-sink">Redis Sink</a> - Connection options and batching</li>
<li><a href="#envelopes-and-encodings">Envelopes</a> - Output format options</li>
<li><a href="#configuration">Configuration Reference</a> - Full spec documentation</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="example-turso-to-kafka"><a class="header" href="#example-turso-to-kafka">Example: Turso to Kafka</a></h1>
<p>This example demonstrates streaming changes from a Turso database to Kafka with schema sensing enabled.</p>
<h2 id="use-case"><a class="header" href="#use-case">Use Case</a></h2>
<p>You have a Turso database (or local libSQL) and want to:</p>
<ul>
<li>Stream table changes to a Kafka topic</li>
<li>Automatically detect schema structure from JSON payloads</li>
<li>Transform events with JavaScript before publishing</li>
</ul>
<h2 id="pipeline-configuration-1"><a class="header" href="#pipeline-configuration-1">Pipeline Configuration</a></h2>
<pre><code class="language-yaml">apiVersion: deltaforge/v1
kind: Pipeline
metadata:
  name: turso2kafka
  tenant: acme

spec:
  source:
    type: turso
    config:
      id: turso-main
      # Local libSQL for development
      url: "http://127.0.0.1:8080"
      # For Turso cloud:
      # url: "libsql://your-db.turso.io"
      # auth_token: "${TURSO_AUTH_TOKEN}"
      tables: ["users", "orders", "order_items"]
      poll_interval_ms: 1000
      cdc_mode: auto

  processors:
    - type: javascript
      id: enrich
      inline: |
        function processBatch(events) {
          return events.map(event =&gt; {
            // Add custom metadata to events
            event.source_type = "turso";
            event.processed_at = new Date().toISOString();
            return event;
          });
        }

  sinks:
    - type: kafka
      config:
        id: kafka-main
        brokers: "${KAFKA_BROKERS}"
        topic: turso.changes
        required: true
        exactly_once: false
        client_conf:
          message.timeout.ms: "5000"
          acks: "all"

  batch:
    max_events: 100
    max_bytes: 1048576
    max_ms: 500
    respect_source_tx: false
    max_inflight: 2

  commit_policy:
    mode: required

  schema_sensing:
    enabled: true
    deep_inspect:
      enabled: true
      max_depth: 3
    sampling:
      warmup_events: 50
      sample_rate: 5
      structure_cache: true
</code></pre>
<h2 id="running-the-example-1"><a class="header" href="#running-the-example-1">Running the Example</a></h2>
<h3 id="1-start-infrastructure"><a class="header" href="#1-start-infrastructure">1. Start Infrastructure</a></h3>
<pre><code class="language-bash"># Start Kafka and other services
./dev.sh up

# Create the target topic
./dev.sh k-create turso.changes 6
</code></pre>
<h3 id="2-start-local-libsql-optional"><a class="header" href="#2-start-local-libsql-optional">2. Start Local libSQL (Optional)</a></h3>
<p>For local development without Turso cloud:</p>
<pre><code class="language-bash"># Using sqld (libSQL server)
sqld --http-listen-addr 127.0.0.1:8080

# Or with Docker
docker run -p 8080:8080 ghcr.io/libsql/sqld:latest
</code></pre>
<h3 id="3-create-test-tables"><a class="header" href="#3-create-test-tables">3. Create Test Tables</a></h3>
<pre><code class="language-sql">CREATE TABLE users (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  name TEXT NOT NULL,
  email TEXT UNIQUE,
  metadata TEXT  -- JSON column
);

CREATE TABLE orders (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  user_id INTEGER NOT NULL,
  total REAL NOT NULL,
  status TEXT DEFAULT 'pending',
  created_at TEXT DEFAULT CURRENT_TIMESTAMP
);
</code></pre>
<h3 id="4-run-deltaforge"><a class="header" href="#4-run-deltaforge">4. Run DeltaForge</a></h3>
<pre><code class="language-bash"># Save config as turso-kafka.yaml
cargo run -p runner -- --config turso-kafka.yaml
</code></pre>
<h3 id="5-insert-test-data"><a class="header" href="#5-insert-test-data">5. Insert Test Data</a></h3>
<pre><code class="language-sql">INSERT INTO users (name, email, metadata) 
VALUES ('Alice', 'alice@example.com', '{"role": "admin", "tags": ["vip"]}');

INSERT INTO orders (user_id, total, status) 
VALUES (1, 99.99, 'completed');
</code></pre>
<h3 id="6-verify-events-in-kafka"><a class="header" href="#6-verify-events-in-kafka">6. Verify Events in Kafka</a></h3>
<pre><code class="language-bash">./dev.sh k-consume turso.changes --from-beginning
</code></pre>
<p>You should see events like:</p>
<pre><code class="language-json">{
  "event_id": "550e8400-e29b-41d4-a716-446655440000",
  "tenant_id": "acme",
  "table": "users",
  "op": "insert",
  "after": {
    "id": 1,
    "name": "Alice",
    "email": "alice@example.com",
    "metadata": "{\"role\": \"admin\", \"tags\": [\"vip\"]}"
  },
  "source_type": "turso",
  "processed_at": "2025-01-15T10:30:00.000Z",
  "timestamp": "2025-01-15T10:30:00.123Z"
}
</code></pre>
<h2 id="monitoring-3"><a class="header" href="#monitoring-3">Monitoring</a></h2>
<h3 id="check-pipeline-status"><a class="header" href="#check-pipeline-status">Check Pipeline Status</a></h3>
<pre><code class="language-bash">curl http://localhost:8080/pipelines/turso2kafka
</code></pre>
<h3 id="view-inferred-schemas"><a class="header" href="#view-inferred-schemas">View Inferred Schemas</a></h3>
<pre><code class="language-bash"># List all inferred schemas
curl http://localhost:8080/pipelines/turso2kafka/sensing/schemas

# Get details for users table
curl http://localhost:8080/pipelines/turso2kafka/sensing/schemas/users

# Export as JSON Schema
curl http://localhost:8080/pipelines/turso2kafka/sensing/schemas/users/json-schema
</code></pre>
<h3 id="check-drift-detection"><a class="header" href="#check-drift-detection">Check Drift Detection</a></h3>
<pre><code class="language-bash">curl http://localhost:8080/pipelines/turso2kafka/drift
</code></pre>
<h2 id="turso-cloud-configuration"><a class="header" href="#turso-cloud-configuration">Turso Cloud Configuration</a></h2>
<p>For production with Turso cloud:</p>
<pre><code class="language-yaml">source:
  type: turso
  config:
    id: turso-prod
    url: "libsql://mydb-myorg.turso.io"
    auth_token: "${TURSO_AUTH_TOKEN}"
    tables: ["*"]
    cdc_mode: native
    poll_interval_ms: 1000
    native_cdc:
      level: data
</code></pre>
<p>Set the auth token via environment variable:</p>
<pre><code class="language-bash">export TURSO_AUTH_TOKEN="your-token-here"
</code></pre>
<h2 id="notes-4"><a class="header" href="#notes-4">Notes</a></h2>
<ul>
<li><strong>CDC Mode</strong>: <code>auto</code> tries native CDC first, then falls back to triggers or polling</li>
<li><strong>Poll Interval</strong>: Lower values reduce latency but increase database load</li>
<li><strong>Schema Sensing</strong>: Automatically discovers JSON structure in text columns</li>
<li><strong>Exactly Once</strong>: Set to <code>false</code> for higher throughput; use <code>true</code> if Kafka cluster supports EOS</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="postgresql-to-nats"><a class="header" href="#postgresql-to-nats">PostgreSQL to NATS</a></h1>
<p>This one streams PostgreSQL logical replication changes to NATS JetStream with CloudEvents envelope format, for serverless architectures for example.</p>
<h2 id="overview-3"><a class="header" href="#overview-3">Overview</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Component</th><th>Configuration</th></tr>
</thead>
<tbody>
<tr><td><strong>Source</strong></td><td>PostgreSQL logical replication</td></tr>
<tr><td><strong>Processor</strong></td><td>None (passthrough)</td></tr>
<tr><td><strong>Sink</strong></td><td>NATS JetStream</td></tr>
<tr><td><strong>Envelope</strong></td><td>CloudEvents 1.0</td></tr>
</tbody>
</table>
</div>
<h2 id="use-case-1"><a class="header" href="#use-case-1">Use Case</a></h2>
<p>You have a PostgreSQL database and want to:</p>
<ul>
<li>Stream changes to NATS for event-driven microservices</li>
<li>Use CloudEvents format for AWS Lambda, Azure Functions, or Knative</li>
<li>Leverage JetStream for durable, replay-capable event streams</li>
</ul>
<h2 id="pipeline-configuration-2"><a class="header" href="#pipeline-configuration-2">Pipeline Configuration</a></h2>
<pre><code class="language-yaml">apiVersion: deltaforge/v1
kind: Pipeline
metadata:
  name: users-postgres-to-nats
  tenant: acme

spec:
  source:
    type: postgres
    config:
      id: users-postgres
      dsn: ${POSTGRES_DSN}
      slot: deltaforge_users
      publication: users_pub
      tables:
        - public.users
        - public.profiles
        - public.user_sessions
      start_position: earliest

  sinks:
    - type: nats
      config:
        id: users-nats
        url: ${NATS_URL}
        subject: users.events
        stream: USERS
        envelope:
          type: cloudevents
          type_prefix: "com.acme.users"
        encoding: json
        required: true
        send_timeout_secs: 5
        batch_timeout_secs: 30

  batch:
    max_events: 500
    max_ms: 500
    respect_source_tx: true

  commit_policy:
    mode: required
</code></pre>
<h2 id="prerequisites-2"><a class="header" href="#prerequisites-2">Prerequisites</a></h2>
<h3 id="postgresql-setup"><a class="header" href="#postgresql-setup">PostgreSQL Setup</a></h3>
<pre><code class="language-sql">-- Enable logical replication (postgresql.conf)
-- wal_level = logical

-- Create publication for the tables
CREATE PUBLICATION users_pub FOR TABLE users, profiles, user_sessions;

-- Verify publication
SELECT * FROM pg_publication_tables WHERE pubname = 'users_pub';
</code></pre>
<h3 id="nats-jetstream-setup"><a class="header" href="#nats-jetstream-setup">NATS JetStream Setup</a></h3>
<pre><code class="language-bash"># Start NATS with JetStream enabled
./dev.sh up

# Create the stream
./dev.sh nats-stream-add USERS 'users.&gt;'
</code></pre>
<h2 id="running-the-example-2"><a class="header" href="#running-the-example-2">Running the Example</a></h2>
<h3 id="1-set-environment-variables-1"><a class="header" href="#1-set-environment-variables-1">1. Set Environment Variables</a></h3>
<pre><code class="language-bash">export POSTGRES_DSN="postgres://user:password@localhost:5432/mydb"
export NATS_URL="nats://localhost:4222"
</code></pre>
<h3 id="2-start-deltaforge-2"><a class="header" href="#2-start-deltaforge-2">2. Start DeltaForge</a></h3>
<pre><code class="language-bash">cargo run -p runner -- --config postgres-nats.yaml
</code></pre>
<h3 id="3-insert-test-data-1"><a class="header" href="#3-insert-test-data-1">3. Insert Test Data</a></h3>
<pre><code class="language-sql">INSERT INTO users (name, email, created_at)
VALUES ('Alice', 'alice@example.com', NOW());

UPDATE users SET email = 'alice.new@example.com' WHERE name = 'Alice';
</code></pre>
<h3 id="4-verify-in-nats"><a class="header" href="#4-verify-in-nats">4. Verify in NATS</a></h3>
<pre><code class="language-bash">./dev.sh nats-sub 'users.&gt;'
</code></pre>
<p>You should see CloudEvents formatted messages:</p>
<pre><code class="language-json">{
  "specversion": "1.0",
  "id": "550e8400-e29b-41d4-a716-446655440000",
  "source": "deltaforge/users-postgres/public.users",
  "type": "com.acme.users.created",
  "time": "2025-01-15T10:30:00.000Z",
  "datacontenttype": "application/json",
  "subject": "public.users",
  "data": {
    "before": null,
    "after": {
      "id": 1,
      "name": "Alice",
      "email": "alice@example.com",
      "created_at": "2025-01-15T10:30:00.000Z"
    },
    "op": "c"
  }
}
</code></pre>
<h2 id="variations-1"><a class="header" href="#variations-1">Variations</a></h2>
<h3 id="with-debezium-envelope-1"><a class="header" href="#with-debezium-envelope-1">With Debezium Envelope</a></h3>
<p>For compatibility with existing Debezium consumers:</p>
<pre><code class="language-yaml">sinks:
  - type: nats
    config:
      id: users-nats
      url: ${NATS_URL}
      subject: users.events
      stream: USERS
      envelope:
        type: debezium
</code></pre>
<h3 id="with-authentication"><a class="header" href="#with-authentication">With Authentication</a></h3>
<pre><code class="language-yaml">sinks:
  - type: nats
    config:
      id: users-nats
      url: ${NATS_URL}
      subject: users.events
      stream: USERS
      envelope:
        type: cloudevents
        type_prefix: "com.acme.users"
      credentials_file: /path/to/nats.creds
      # Or use username/password:
      # username: ${NATS_USER}
      # password: ${NATS_PASS}
</code></pre>
<h3 id="starting-from-latest"><a class="header" href="#starting-from-latest">Starting from Latest</a></h3>
<p>Skip existing data and only capture new changes:</p>
<pre><code class="language-yaml">source:
  type: postgres
  config:
    id: users-postgres
    dsn: ${POSTGRES_DSN}
    slot: deltaforge_users
    publication: users_pub
    tables:
      - public.users
    start_position: latest
</code></pre>
<h2 id="key-concepts-demonstrated-1"><a class="header" href="#key-concepts-demonstrated-1">Key Concepts Demonstrated</a></h2>
<ul>
<li><strong>PostgreSQL Logical Replication</strong>: Production-ready CDC with slot management</li>
<li><strong>CloudEvents Format</strong>: Standard envelope for cloud-native event routing</li>
<li><strong>JetStream Durability</strong>: Replay-capable event streams with consumer acknowledgment</li>
<li><strong>Transaction Preservation</strong>: <code>respect_source_tx: true</code> keeps related changes together</li>
</ul>
<h2 id="related-documentation-1"><a class="header" href="#related-documentation-1">Related Documentation</a></h2>
<ul>
<li><a href="#postgresql-source">PostgreSQL Source</a> - Replication setup and configuration</li>
<li><a href="#nats-sink">NATS Sink</a> - JetStream configuration and authentication</li>
<li><a href="#envelopes-and-encodings">Envelopes</a> - Output format options</li>
<li><a href="#configuration">Configuration Reference</a> - Full spec documentation</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="multi-sink-fan-out-1"><a class="header" href="#multi-sink-fan-out-1">Multi-Sink Fan-Out</a></h1>
<p>This example demonstrates streaming changes to multiple destinations simultaneously, each with a different envelope format tailored to its consumers.</p>
<h2 id="overview-4"><a class="header" href="#overview-4">Overview</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Component</th><th>Configuration</th></tr>
</thead>
<tbody>
<tr><td><strong>Source</strong></td><td>MySQL binlog CDC</td></tr>
<tr><td><strong>Processor</strong></td><td>JavaScript enrichment</td></tr>
<tr><td><strong>Sinks</strong></td><td>Kafka (Debezium) + Redis (Native) + NATS (CloudEvents)</td></tr>
<tr><td><strong>Pattern</strong></td><td>Fan-out with format adaptation</td></tr>
</tbody>
</table>
</div>
<h2 id="use-case-2"><a class="header" href="#use-case-2">Use Case</a></h2>
<p>You have a MySQL database and need to:</p>
<ul>
<li>Send to Kafka Connect (requires Debezium format)</li>
<li>Populate a Redis cache (native format for efficiency)</li>
<li>Trigger serverless functions via NATS (CloudEvents format)</li>
<li>Handle sink failures independently without blocking the pipeline</li>
</ul>
<h2 id="pipeline-configuration-3"><a class="header" href="#pipeline-configuration-3">Pipeline Configuration</a></h2>
<pre><code class="language-yaml">apiVersion: deltaforge/v1
kind: Pipeline
metadata:
  name: orders-fan-out
  tenant: acme

spec:
  source:
    type: mysql
    config:
      id: orders-mysql
      dsn: ${MYSQL_DSN}
      tables:
        - shop.orders
        - shop.order_items

  processors:
    - type: javascript
      id: enrich
      inline: |
        function processBatch(events) {
          return events.map(event =&gt; {
            // Add routing hints via tags (tags is a valid Event field)
            event.tags = event.tags || [];
            
            if (event.table.includes('orders')) {
              event.tags.push('entity:order');
              // High-value orders get priority tag
              if (event.after &amp;&amp; event.after.total &gt; 1000) {
                event.tags.push('priority:high');
              }
            }
            
            event.tags.push('enriched');
            return event;
          });
        }
      limits:
        timeout_ms: 500

  sinks:
    # Primary: Kafka for data warehouse / Kafka Connect
    # Uses Debezium format for ecosystem compatibility
    - type: kafka
      config:
        id: warehouse-kafka
        brokers: ${KAFKA_BROKERS}
        topic: orders.cdc
        envelope:
          type: debezium
        encoding: json
        required: true           # Must succeed for checkpoint
        exactly_once: false
        client_conf:
          acks: "all"

    # Secondary: Redis for real-time cache
    # Uses native format for minimal overhead
    - type: redis
      config:
        id: cache-redis
        uri: ${REDIS_URI}
        stream: orders:cache
        envelope:
          type: native
        encoding: json
        required: false          # Best-effort, won't block pipeline

    # Tertiary: NATS for serverless triggers
    # Uses CloudEvents for Lambda/Functions compatibility
    - type: nats
      config:
        id: serverless-nats
        url: ${NATS_URL}
        subject: orders.events
        stream: ORDERS
        envelope:
          type: cloudevents
          type_prefix: "com.acme.orders"
        encoding: json
        required: false          # Best-effort

  batch:
    max_events: 500
    max_bytes: 1048576
    max_ms: 500
    respect_source_tx: true

  commit_policy:
    mode: required    # Only wait for required sinks (Kafka)
</code></pre>
<h2 id="how-it-works-2"><a class="header" href="#how-it-works-2">How It Works</a></h2>
<h3 id="checkpoint-behavior"><a class="header" href="#checkpoint-behavior">Checkpoint Behavior</a></h3>
<p>With <code>commit_policy.mode: required</code>:</p>
<pre><code>Source Event
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Parallel Sink Delivery                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Kafka           â”‚ Redis           â”‚ NATS            â”‚
â”‚ required: true  â”‚ required: false â”‚ required: false â”‚
â”‚                 â”‚                 â”‚                 â”‚
â”‚ âœ“ Must succeed  â”‚ âœ“ Best-effort   â”‚ âœ“ Best-effort   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚               â”‚
         â–¼                 â”‚               â”‚
    Checkpoint &lt;â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
    advances                               â”‚
    (even if Redis/NATS fail)              â”‚
</code></pre>
<h3 id="output-formats"><a class="header" href="#output-formats">Output Formats</a></h3>
<p><strong>Kafka (Debezium):</strong></p>
<pre><code class="language-json">{
  "payload": {
    "before": null,
    "after": {"id": 1, "total": 1500.00, "status": "pending"},
    "source": {"connector": "mysql", "db": "shop", "table": "orders"},
    "op": "c",
    "ts_ms": 1700000000000
  }
}
</code></pre>
<p><strong>Redis (Native):</strong></p>
<pre><code class="language-json">{
  "before": null,
  "after": {"id": 1, "total": 1500.00, "status": "pending"},
  "source": {"connector": "mysql", "db": "shop", "table": "orders"},
  "op": "c",
  "ts_ms": 1700000000000
}
</code></pre>
<p><strong>NATS (CloudEvents):</strong></p>
<pre><code class="language-json">{
  "specversion": "1.0",
  "id": "evt-123",
  "source": "deltaforge/orders-mysql/shop.orders",
  "type": "com.acme.orders.created",
  "time": "2025-01-15T10:30:00.000Z",
  "data": {
    "before": null,
    "after": {"id": 1, "total": 1500.00, "status": "pending"},
    "op": "c"
  }
}
</code></pre>
<h2 id="running-the-example-3"><a class="header" href="#running-the-example-3">Running the Example</a></h2>
<h3 id="1-set-environment-variables-2"><a class="header" href="#1-set-environment-variables-2">1. Set Environment Variables</a></h3>
<pre><code class="language-bash">export MYSQL_DSN="mysql://user:password@localhost:3306/shop"
export KAFKA_BROKERS="localhost:9092"
export REDIS_URI="redis://localhost:6379"
export NATS_URL="nats://localhost:4222"
</code></pre>
<h3 id="2-start-infrastructure"><a class="header" href="#2-start-infrastructure">2. Start Infrastructure</a></h3>
<pre><code class="language-bash">./dev.sh up
./dev.sh k-create orders.cdc 6
./dev.sh nats-stream-add ORDERS 'orders.&gt;'
</code></pre>
<h3 id="3-start-deltaforge"><a class="header" href="#3-start-deltaforge">3. Start DeltaForge</a></h3>
<pre><code class="language-bash">cargo run -p runner -- --config fan-out.yaml
</code></pre>
<h3 id="4-insert-test-data"><a class="header" href="#4-insert-test-data">4. Insert Test Data</a></h3>
<pre><code class="language-sql">INSERT INTO shop.orders (customer_id, total, status)
VALUES (1, 1500.00, 'pending');
</code></pre>
<h3 id="5-verify-each-sink"><a class="header" href="#5-verify-each-sink">5. Verify Each Sink</a></h3>
<pre><code class="language-bash"># Kafka
./dev.sh k-consume orders.cdc --from-beginning

# Redis
./dev.sh redis-read orders:cache 10

# NATS
./dev.sh nats-sub 'orders.&gt;'
</code></pre>
<h2 id="variations-2"><a class="header" href="#variations-2">Variations</a></h2>
<h3 id="quorum-mode"><a class="header" href="#quorum-mode">Quorum Mode</a></h3>
<p>Require 2 of 3 sinks to succeed:</p>
<pre><code class="language-yaml">sinks:
  - type: kafka
    config:
      id: kafka-1
      required: true   # Counts toward quorum
  - type: redis
    config:
      id: redis-1
      required: true   # Counts toward quorum
  - type: nats
    config:
      id: nats-1
      required: true   # Counts toward quorum

commit_policy:
  mode: quorum
  quorum: 2            # Any 2 must succeed
</code></pre>
<h3 id="all-or-nothing"><a class="header" href="#all-or-nothing">All-or-Nothing</a></h3>
<p>Require all sinks to succeed (strongest consistency):</p>
<pre><code class="language-yaml">commit_policy:
  mode: all
</code></pre>
<blockquote>
<p>âš ï¸ <strong>Warning</strong>: <code>mode: all</code> means any sink failure blocks the entire pipeline. Use only when all destinations are equally critical.</p>
</blockquote>
<h2 id="key-concepts-demonstrated-2"><a class="header" href="#key-concepts-demonstrated-2">Key Concepts Demonstrated</a></h2>
<ul>
<li><strong>Multi-Sink Fan-Out</strong>: Single source to multiple destinations</li>
<li><strong>Format Adaptation</strong>: Different envelope per consumer requirement</li>
<li><strong>Selective Checkpointing</strong>: <code>required</code> flag controls which sinks gate progress</li>
<li><strong>Failure Isolation</strong>: Non-critical sinks donâ€™t block the pipeline</li>
<li><strong>Tag-Based Enrichment</strong>: Use <code>event.tags</code> for routing metadata</li>
</ul>
<blockquote>
<p><strong>Processor Constraints</strong>: JavaScript processors can only modify <code>event.before</code>, <code>event.after</code>, and <code>event.tags</code>. Arbitrary top-level fields would be lost during serialization.</p>
</blockquote>
<h2 id="related-documentation-2"><a class="header" href="#related-documentation-2">Related Documentation</a></h2>
<ul>
<li><a href="sinks/README.html">Sinks Overview</a> - Multi-sink patterns and commit policies</li>
<li><a href="#envelopes-and-encodings">Envelopes</a> - Output format options</li>
<li><a href="#commit-policy">Commit Policy</a> - Checkpoint gating modes</li>
<li><a href="#configuration">Configuration Reference</a> - Full spec documentation</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="event-filtering-with-javascript"><a class="header" href="#event-filtering-with-javascript">Event Filtering with JavaScript</a></h1>
<p>This example demonstrates using JavaScript processors to filter and selectively drop events before they reach sinks.</p>
<h2 id="overview-5"><a class="header" href="#overview-5">Overview</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Component</th><th>Configuration</th></tr>
</thead>
<tbody>
<tr><td><strong>Source</strong></td><td>MySQL binlog CDC</td></tr>
<tr><td><strong>Processor</strong></td><td>JavaScript filter + redaction</td></tr>
<tr><td><strong>Sink</strong></td><td>Kafka</td></tr>
<tr><td><strong>Pattern</strong></td><td>Event filtering and transformation</td></tr>
</tbody>
</table>
</div>
<h2 id="use-case-3"><a class="header" href="#use-case-3">Use Case</a></h2>
<p>You have a MySQL database and want to:</p>
<ul>
<li>Filter out events from certain tables or with specific conditions</li>
<li>Drop low-value events to reduce downstream load</li>
<li>Redact sensitive fields conditionally</li>
</ul>
<h2 id="pipeline-configuration-4"><a class="header" href="#pipeline-configuration-4">Pipeline Configuration</a></h2>
<pre><code class="language-yaml">apiVersion: deltaforge/v1
kind: Pipeline
metadata:
  name: filtered-orders
  tenant: acme

spec:
  source:
    type: mysql
    config:
      id: orders-mysql
      dsn: ${MYSQL_DSN}
      tables:
        - shop.orders
        - shop.order_items
        - shop.audit_log        # We'll filter most of these out

  processors:
    - type: javascript
      id: filter-and-redact
      inline: |
        function processBatch(events) {
          return events
            // 1. Filter out audit_log events except errors
            .filter(event =&gt; {
              if (event.table === 'shop.audit_log') {
                // Only keep error-level audit events
                return event.after &amp;&amp; event.after.level === 'error';
              }
              return true;
            })
            
            // 2. Filter out soft-deleted records
            .filter(event =&gt; {
              if (event.after &amp;&amp; event.after.deleted_at !== null) {
                return false;  // Drop soft-deleted records
              }
              return true;
            })
            
            // 3. Filter out test/staging data
            .filter(event =&gt; {
              if (event.after &amp;&amp; event.after.email) {
                // Drop test accounts
                if (event.after.email.endsWith('@test.local')) {
                  return false;
                }
              }
              return true;
            })
            
            // 4. Transform remaining events
            .map(event =&gt; {
              // Redact PII for non-admin tables
              if (event.after) {
                if (event.after.email) {
                  event.after.email = maskEmail(event.after.email);
                }
                if (event.after.phone) {
                  event.after.phone = '[redacted]';
                }
              }
              
              // Add filter tag (tags is a valid Event field)
              event.tags = (event.tags || []).concat(['filtered']);
              
              return event;
            });
        }
        
        // helper: mask email keeping domain
        function maskEmail(email) {
          const [local, domain] = email.split('@');
          if (!domain) return '[invalid-email]';
          const masked = local.charAt(0) + '***' + local.charAt(local.length - 1);
          return masked + '@' + domain;
        }
      limits:
        timeout_ms: 1000
        mem_mb: 128

  sinks:
    - type: kafka
      config:
        id: filtered-kafka
        brokers: ${KAFKA_BROKERS}
        topic: orders.filtered
        envelope:
          type: native
        encoding: json
        required: true

  batch:
    max_events: 500
    max_ms: 500
    respect_source_tx: true

  commit_policy:
    mode: required
</code></pre>
<h2 id="filter-patterns"><a class="header" href="#filter-patterns">Filter Patterns</a></h2>
<h3 id="drop-events-return-empty-array-element"><a class="header" href="#drop-events-return-empty-array-element">Drop Events (Return Empty Array Element)</a></h3>
<pre><code class="language-javascript">.filter(event =&gt; {
  // return false to drop the event
  if (event.table === 'internal_logs') {
    return false;
  }
  return true;
})
</code></pre>
<h3 id="conditional-field-based-filtering"><a class="header" href="#conditional-field-based-filtering">Conditional Field-Based Filtering</a></h3>
<pre><code class="language-javascript">.filter(event =&gt; {
  // drop events where status is 'draft'
  if (event.after &amp;&amp; event.after.status === 'draft') {
    return false;
  }
  return true;
})
</code></pre>
<h3 id="drop-by-operation-type"><a class="header" href="#drop-by-operation-type">Drop by Operation Type</a></h3>
<pre><code class="language-javascript">.filter(event =&gt; {
  // only keep inserts and updates, drop deletes
  return event.op === 'c' || event.op === 'u';
})
</code></pre>
<h3 id="sample-events-rate-limiting"><a class="header" href="#sample-events-rate-limiting">Sample Events (Rate Limiting)</a></h3>
<pre><code class="language-javascript">// keep only 10% of events (for high-volume tables)
.filter(event =&gt; {
  if (event.table === 'high_volume_table') {
    return Math.random() &lt; 0.1;
  }
  return true;
})
</code></pre>
<h2 id="running-the-example-4"><a class="header" href="#running-the-example-4">Running the Example</a></h2>
<h3 id="1-set-environment-variables-3"><a class="header" href="#1-set-environment-variables-3">1. Set Environment Variables</a></h3>
<pre><code class="language-bash">export MYSQL_DSN="mysql://user:password@localhost:3306/shop"
export KAFKA_BROKERS="localhost:9092"
</code></pre>
<h3 id="2-start-deltaforge-3"><a class="header" href="#2-start-deltaforge-3">2. Start DeltaForge</a></h3>
<pre><code class="language-bash">cargo run -p runner -- --config filtered-orders.yaml
</code></pre>
<h3 id="3-insert-test-data-2"><a class="header" href="#3-insert-test-data-2">3. Insert Test Data</a></h3>
<pre><code class="language-sql">-- This will be captured and transformed
INSERT INTO shop.orders (customer_email, total, status)
VALUES ('alice@example.com', 99.99, 'pending');

-- This will be filtered out (test account)
INSERT INTO shop.orders (customer_email, total, status)
VALUES ('test@test.local', 50.00, 'pending');

-- This will be filtered out (soft-deleted)
INSERT INTO shop.orders (customer_email, total, status, deleted_at)
VALUES ('bob@example.com', 75.00, 'pending', NOW());

-- Audit log - will be filtered out (not error level)
INSERT INTO shop.audit_log (level, message)
VALUES ('info', 'User logged in');

-- Audit log - will be kept (error level)
INSERT INTO shop.audit_log (level, message)
VALUES ('error', 'Payment failed');
</code></pre>
<h3 id="4-verify-filtered-output"><a class="header" href="#4-verify-filtered-output">4. Verify Filtered Output</a></h3>
<pre><code class="language-bash">./dev.sh k-consume orders.filtered --from-beginning
</code></pre>
<p>You should only see:</p>
<ul>
<li>Aliceâ€™s order (with masked email: <code>a***e@example.com</code>)</li>
<li>The error-level audit log entry</li>
</ul>
<h2 id="performance-considerations-3"><a class="header" href="#performance-considerations-3">Performance Considerations</a></h2>
<blockquote>
<p><strong>Tip</strong>: Filtering early reduces downstream load. If youâ€™re filtering out 50% of events, your sinks process half the data.</p>
</blockquote>
<pre><code class="language-yaml">processors:
  - type: javascript
    id: filter
    inline: |
      function processBatch(events) {
        // Filter FIRST, then transform
        return events
          .filter(e =&gt; shouldKeep(e))  // Reduces array size
          .map(e =&gt; transform(e));      // Processes fewer events
      }
    limits:
      timeout_ms: 500    # Increase if filtering logic is complex
      mem_mb: 128        # Increase for large batches
</code></pre>
<h2 id="key-concepts-demonstrated-3"><a class="header" href="#key-concepts-demonstrated-3">Key Concepts Demonstrated</a></h2>
<ul>
<li><strong>Event Filtering</strong>: Drop events before they reach sinks</li>
<li><strong>Conditional Logic</strong>: Filter based on table, operation, or field values</li>
<li><strong>PII Redaction</strong>: Mask sensitive data in remaining events</li>
<li><strong>Sampling</strong>: Rate-limit high-volume event streams</li>
</ul>
<blockquote>
<p><strong>Processor Constraints</strong>: JavaScript processors can only modify <code>event.before</code>, <code>event.after</code>, and <code>event.tags</code>. Arbitrary top-level fields like <code>event.filtered_at</code> would be lost during serialization.</p>
</blockquote>
<h2 id="related-documentation-3"><a class="header" href="#related-documentation-3">Related Documentation</a></h2>
<ul>
<li><a href="#processors">Processors</a> - JavaScript processor configuration</li>
<li><a href="#envelopes-and-encodings">Envelopes</a> - Output format options</li>
<li><a href="#configuration">Configuration Reference</a> - Full spec documentation</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="schema-sensing-and-drift-detection"><a class="header" href="#schema-sensing-and-drift-detection">Schema Sensing and Drift Detection</a></h1>
<p>This example demonstrates DeltaForgeâ€™s automatic schema inference for JSON columns and drift detection capabilities.</p>
<h2 id="overview-6"><a class="header" href="#overview-6">Overview</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Component</th><th>Configuration</th></tr>
</thead>
<tbody>
<tr><td><strong>Source</strong></td><td>PostgreSQL logical replication</td></tr>
<tr><td><strong>Processor</strong></td><td>None</td></tr>
<tr><td><strong>Sink</strong></td><td>Kafka</td></tr>
<tr><td><strong>Feature</strong></td><td>Schema sensing with deep JSON inspection</td></tr>
</tbody>
</table>
</div>
<h2 id="use-case-4"><a class="header" href="#use-case-4">Use Case</a></h2>
<p>You have a PostgreSQL database with JSON/JSONB columns and want to:</p>
<ul>
<li>Automatically discover the structure of JSON payloads</li>
<li>Detect when JSON schemas change over time (drift)</li>
<li>Export inferred schemas as JSON Schema for downstream validation</li>
<li>Monitor schema evolution without manual tracking</li>
</ul>
<h2 id="pipeline-configuration-5"><a class="header" href="#pipeline-configuration-5">Pipeline Configuration</a></h2>
<pre><code class="language-yaml">apiVersion: deltaforge/v1
kind: Pipeline
metadata:
  name: products-with-sensing
  tenant: acme

spec:
  source:
    type: postgres
    config:
      id: products-postgres
      dsn: ${POSTGRES_DSN}
      slot: deltaforge_products
      publication: products_pub
      tables:
        - public.products
        - public.product_variants
      start_position: earliest

  sinks:
    - type: kafka
      config:
        id: products-kafka
        brokers: ${KAFKA_BROKERS}
        topic: products.changes
        envelope:
          type: debezium
        encoding: json
        required: true

  batch:
    max_events: 500
    max_ms: 1000
    respect_source_tx: true

  commit_policy:
    mode: required

  # Schema sensing configuration
  schema_sensing:
    enabled: true
    
    deep_inspect:
      enabled: true
      max_depth: 5           # How deep to traverse nested JSON
      max_sample_size: 500   # Events to analyze for deep inspection
    
    sampling:
      warmup_events: 100     # Analyze every event during warmup
      sample_rate: 20        # After warmup, analyze 1 in 20 events
      structure_cache: true  # Cache seen structures to avoid re-analysis
      structure_cache_size: 100
    
    tracking:
      detect_drift: true     # Enable drift detection
      drift_threshold: 0.1   # Alert if &gt;10% of events have new fields
    
    output:
      emit_schemas: true     # Include schema info in API responses
      json_schema_format: draft-07
</code></pre>
<h2 id="table-structure"><a class="header" href="#table-structure">Table Structure</a></h2>
<pre><code class="language-sql">CREATE TABLE products (
  id SERIAL PRIMARY KEY,
  name TEXT NOT NULL,
  sku TEXT UNIQUE,
  price DECIMAL(10,2),
  
  -- JSON columns that schema sensing will analyze
  metadata JSONB,          -- Product attributes, tags, etc.
  specifications JSONB,    -- Technical specs
  
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE TABLE product_variants (
  id SERIAL PRIMARY KEY,
  product_id INTEGER REFERENCES products(id),
  variant_name TEXT,
  
  -- Nested JSON with variable structure
  attributes JSONB,        -- Color, size, material, etc.
  pricing JSONB,           -- Regional pricing, discounts
  
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Create publication
CREATE PUBLICATION products_pub FOR TABLE products, product_variants;
</code></pre>
<h2 id="sample-data"><a class="header" href="#sample-data">Sample Data</a></h2>
<pre><code class="language-sql">-- Insert products with JSON metadata
INSERT INTO products (name, sku, price, metadata, specifications) VALUES
(
  'Wireless Headphones',
  'WH-1000',
  299.99,
  '{
    "brand": "AudioTech",
    "category": "Electronics",
    "tags": ["wireless", "bluetooth", "noise-canceling"],
    "ratings": {"average": 4.5, "count": 1250}
  }',
  '{
    "battery_life_hours": 30,
    "driver_size_mm": 40,
    "frequency_response": {"min_hz": 20, "max_hz": 20000},
    "connectivity": ["bluetooth", "aux"]
  }'
);

-- Insert variant with nested attributes
INSERT INTO product_variants (product_id, variant_name, attributes, pricing) VALUES
(
  1,
  'Midnight Black',
  '{
    "color": {"name": "Midnight Black", "hex": "#1a1a2e"},
    "material": "Premium Plastic",
    "weight_grams": 250
  }',
  '{
    "base_price": 299.99,
    "regional": {
      "US": {"price": 299.99, "currency": "USD"},
      "EU": {"price": 279.99, "currency": "EUR"}
    },
    "discounts": [
      {"code": "SAVE20", "percent": 20, "expires": "2025-12-31"}
    ]
  }'
);
</code></pre>
<h2 id="running-the-example-5"><a class="header" href="#running-the-example-5">Running the Example</a></h2>
<h3 id="1-set-environment-variables-4"><a class="header" href="#1-set-environment-variables-4">1. Set Environment Variables</a></h3>
<pre><code class="language-bash">export POSTGRES_DSN="postgres://user:password@localhost:5432/shop"
export KAFKA_BROKERS="localhost:9092"
</code></pre>
<h3 id="2-start-deltaforge-4"><a class="header" href="#2-start-deltaforge-4">2. Start DeltaForge</a></h3>
<pre><code class="language-bash">cargo run -p runner -- --config products-sensing.yaml
</code></pre>
<h3 id="3-insert-data-and-let-sensing-analyze"><a class="header" href="#3-insert-data-and-let-sensing-analyze">3. Insert Data and Let Sensing Analyze</a></h3>
<pre><code class="language-sql">-- Insert several products to build schema profile
INSERT INTO products (name, sku, price, metadata, specifications) VALUES
('Smart Watch', 'SW-200', 399.99, 
 '{"brand": "TechWear", "tags": ["fitness", "smart"]}',
 '{"battery_days": 7, "water_resistant": true}');
</code></pre>
<h2 id="using-the-schema-sensing-api"><a class="header" href="#using-the-schema-sensing-api">Using the Schema Sensing API</a></h2>
<h3 id="list-inferred-schemas-2"><a class="header" href="#list-inferred-schemas-2">List Inferred Schemas</a></h3>
<pre><code class="language-bash">curl http://localhost:8080/pipelines/products-with-sensing/sensing/schemas
</code></pre>
<p>Response:</p>
<pre><code class="language-json">{
  "schemas": [
    {
      "table": "public.products",
      "columns": {
        "metadata": {
          "type": "object",
          "inferred_at": "2025-01-15T10:30:00Z",
          "sample_count": 150
        },
        "specifications": {
          "type": "object",
          "inferred_at": "2025-01-15T10:30:00Z",
          "sample_count": 150
        }
      }
    }
  ]
}
</code></pre>
<h3 id="get-detailed-schema-for-a-table"><a class="header" href="#get-detailed-schema-for-a-table">Get Detailed Schema for a Table</a></h3>
<pre><code class="language-bash">curl http://localhost:8080/pipelines/products-with-sensing/sensing/schemas/public.products
</code></pre>
<p>Response:</p>
<pre><code class="language-json">{
  "table": "public.products",
  "json_columns": {
    "metadata": {
      "inferred_schema": {
        "type": "object",
        "properties": {
          "brand": {"type": "string"},
          "category": {"type": "string"},
          "tags": {"type": "array", "items": {"type": "string"}},
          "ratings": {
            "type": "object",
            "properties": {
              "average": {"type": "number"},
              "count": {"type": "integer"}
            }
          }
        }
      },
      "sample_count": 150,
      "last_updated": "2025-01-15T10:35:00Z"
    }
  }
}
</code></pre>
<h3 id="export-as-json-schema-1"><a class="header" href="#export-as-json-schema-1">Export as JSON Schema</a></h3>
<pre><code class="language-bash">curl http://localhost:8080/pipelines/products-with-sensing/sensing/schemas/public.products/json-schema
</code></pre>
<p>Response:</p>
<pre><code class="language-json">{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "public.products.metadata",
  "type": "object",
  "properties": {
    "brand": {"type": "string"},
    "category": {"type": "string"},
    "tags": {
      "type": "array",
      "items": {"type": "string"}
    },
    "ratings": {
      "type": "object",
      "properties": {
        "average": {"type": "number"},
        "count": {"type": "integer"}
      }
    }
  }
}
</code></pre>
<h3 id="check-drift-detection-1"><a class="header" href="#check-drift-detection-1">Check Drift Detection</a></h3>
<pre><code class="language-bash">curl http://localhost:8080/pipelines/products-with-sensing/drift
</code></pre>
<p>Response:</p>
<pre><code class="language-json">{
  "drift_detected": true,
  "tables": {
    "public.products": {
      "metadata": {
        "new_fields": ["promotion", "seasonal"],
        "removed_fields": [],
        "type_changes": [],
        "drift_percentage": 0.15,
        "first_seen": "2025-01-15T11:00:00Z"
      }
    }
  }
}
</code></pre>
<h3 id="get-sensing-statistics"><a class="header" href="#get-sensing-statistics">Get Sensing Statistics</a></h3>
<pre><code class="language-bash">curl http://localhost:8080/pipelines/products-with-sensing/sensing/stats
</code></pre>
<p>Response:</p>
<pre><code class="language-json">{
  "total_events_analyzed": 1500,
  "total_events_sampled": 250,
  "cache_hits": 1250,
  "cache_misses": 250,
  "tables_tracked": 2,
  "json_columns_tracked": 4
}
</code></pre>
<h2 id="performance-tuning"><a class="header" href="#performance-tuning">Performance Tuning</a></h2>
<blockquote>
<p><strong>Performance tip</strong>: Schema sensing can be CPU-intensive. Tune based on your throughput needs.</p>
</blockquote>
<h3 id="high-throughput-configuration"><a class="header" href="#high-throughput-configuration">High-Throughput Configuration</a></h3>
<pre><code class="language-yaml">schema_sensing:
  enabled: true
  deep_inspect:
    enabled: true
    max_depth: 3           # Limit depth for faster processing
    max_sample_size: 200   # Fewer samples
  sampling:
    warmup_events: 50      # Shorter warmup
    sample_rate: 100       # Analyze 1 in 100 events
    structure_cache: true
    structure_cache_size: 200  # Larger cache
</code></pre>
<h3 id="developmentdebugging-configuration"><a class="header" href="#developmentdebugging-configuration">Development/Debugging Configuration</a></h3>
<pre><code class="language-yaml">schema_sensing:
  enabled: true
  deep_inspect:
    enabled: true
    max_depth: 10          # Full depth
    max_sample_size: 1000  # More samples
  sampling:
    warmup_events: 500     # Longer warmup
    sample_rate: 1         # Analyze every event
</code></pre>
<h2 id="key-concepts-demonstrated-4"><a class="header" href="#key-concepts-demonstrated-4">Key Concepts Demonstrated</a></h2>
<ul>
<li><strong>Automatic Schema Inference</strong>: Discover JSON structure without manual definition</li>
<li><strong>Deep JSON Inspection</strong>: Traverse nested objects and arrays</li>
<li><strong>Drift Detection</strong>: Alert when schemas change unexpectedly</li>
<li><strong>JSON Schema Export</strong>: Generate standard schemas for validation</li>
<li><strong>Sampling Strategy</strong>: Balance accuracy vs. performance</li>
</ul>
<h2 id="related-documentation-4"><a class="header" href="#related-documentation-4">Related Documentation</a></h2>
<ul>
<li><a href="#schema-sensing-2">Schema Sensing</a> - Detailed schema sensing documentation</li>
<li><a href="#postgresql-source">PostgreSQL Source</a> - Replication setup</li>
<li><a href="#schema-sensing">Configuration Reference</a> - Full sensing options</li>
<li><a href="#rest-api-reference">API Reference</a> - All sensing endpoints</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="production-kafka-configuration"><a class="header" href="#production-kafka-configuration">Production Kafka Configuration</a></h1>
<p>This example demonstrates a production-ready Kafka sink configuration with authentication, high availability settings, and some  performance tuning.</p>
<h2 id="overview-7"><a class="header" href="#overview-7">Overview</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Component</th><th>Configuration</th></tr>
</thead>
<tbody>
<tr><td><strong>Source</strong></td><td>PostgreSQL logical replication</td></tr>
<tr><td><strong>Processor</strong></td><td>None</td></tr>
<tr><td><strong>Sink</strong></td><td>Kafka with SASL/SSL authentication</td></tr>
<tr><td><strong>Pattern</strong></td><td>Production-grade reliability</td></tr>
</tbody>
</table>
</div>
<h2 id="use-case-5"><a class="header" href="#use-case-5">Use Case</a></h2>
<p>Youâ€™re deploying DeltaForge to production and need:</p>
<ul>
<li>Secure authentication (SASL/SCRAM or mTLS)</li>
<li>High availability with proper acknowledgment settings</li>
<li>Optimal batching and compression for throughput</li>
<li>Exactly-once semantics for critical data</li>
</ul>
<h2 id="pipeline-configuration-6"><a class="header" href="#pipeline-configuration-6">Pipeline Configuration</a></h2>
<pre><code class="language-yaml">apiVersion: deltaforge/v1
kind: Pipeline
metadata:
  name: orders-to-kafka-prod
  tenant: acme

spec:
  source:
    type: postgres
    config:
      id: orders-postgres
      dsn: ${POSTGRES_DSN}
      slot: deltaforge_orders
      publication: orders_pub
      tables:
        - public.orders
        - public.order_items
        - public.payments
      start_position: earliest

  sinks:
    - type: kafka
      config:
        id: orders-kafka
        brokers: ${KAFKA_BROKERS}
        topic: orders.cdc.events
        envelope:
          type: debezium
        encoding: json
        required: true
        
        # enable exactly-once semantics
        exactly_once: true
        
        # timeout for individual sends
        send_timeout_secs: 60
        
        # librdkafka client configuration
        client_conf:
          # security - SASL/SCRAM authentication
          security.protocol: "SASL_SSL"
          sasl.mechanism: "SCRAM-SHA-512"
          sasl.username: "${KAFKA_USERNAME}"
          sasl.password: "${KAFKA_PASSWORD}"
          
          # SSL/TLS configuration
          ssl.ca.location: "/etc/ssl/certs/kafka-ca.pem"
          ssl.endpoint.identification.algorithm: "https"
          
          # reliability - wait for all replicas
          acks: "all"
          
          # idempotence (required for exactly-once)
          enable.idempotence: "true"
          
          # retries and timeouts
          retries: "2147483647"
          retry.backoff.ms: "100"
          delivery.timeout.ms: "300000"
          request.timeout.ms: "30000"
          
          # kafka batching for throughput
          batch.size: "65536"
          linger.ms: "10"
          
          # compression
          compression.type: "lz4"
          
          # buffer management
          queue.buffering.max.messages: "100000"
          queue.buffering.max.kbytes: "1048576"

  batch:
    max_events: 1000
    max_bytes: 1048576
    max_ms: 100
    respect_source_tx: true
    max_inflight: 4

  commit_policy:
    mode: required
</code></pre>
<h2 id="security-configurations"><a class="header" href="#security-configurations">Security Configurations</a></h2>
<h3 id="saslscram-usernamepassword"><a class="header" href="#saslscram-usernamepassword">SASL/SCRAM (Username/Password)</a></h3>
<pre><code class="language-yaml">client_conf:
  security.protocol: "SASL_SSL"
  sasl.mechanism: "SCRAM-SHA-512"
  sasl.username: "${KAFKA_USERNAME}"
  sasl.password: "${KAFKA_PASSWORD}"
  ssl.ca.location: "/etc/ssl/certs/ca.pem"
</code></pre>
<h3 id="mtls-mutual-tls"><a class="header" href="#mtls-mutual-tls">mTLS (Mutual TLS)</a></h3>
<pre><code class="language-yaml">client_conf:
  security.protocol: "SSL"
  ssl.ca.location: "/etc/ssl/certs/kafka-ca.pem"
  ssl.certificate.location: "/etc/ssl/certs/client.pem"
  ssl.key.location: "/etc/ssl/private/client.key"
  ssl.key.password: "${SSL_KEY_PASSWORD}"
</code></pre>
<h3 id="aws-msk-with-iam"><a class="header" href="#aws-msk-with-iam">AWS MSK with IAM</a></h3>
<pre><code class="language-yaml">client_conf:
  security.protocol: "SASL_SSL"
  sasl.mechanism: "AWS_MSK_IAM"
  sasl.jaas.config: "software.amazon.msk.auth.iam.IAMLoginModule required;"
  sasl.client.callback.handler.class: "software.amazon.msk.auth.iam.IAMClientCallbackHandler"
</code></pre>
<h3 id="confluent-cloud"><a class="header" href="#confluent-cloud">Confluent Cloud</a></h3>
<pre><code class="language-yaml">client_conf:
  security.protocol: "SASL_SSL"
  sasl.mechanism: "PLAIN"
  sasl.username: "${CONFLUENT_API_KEY}"
  sasl.password: "${CONFLUENT_API_SECRET}"
</code></pre>
<h2 id="performance-tuning-1"><a class="header" href="#performance-tuning-1">Performance Tuning</a></h2>
<h3 id="high-throughput"><a class="header" href="#high-throughput">High Throughput</a></h3>
<p>Optimize for maximum events per second:</p>
<pre><code class="language-yaml">client_conf:
  acks: "1"                    # Leader-only ack (faster, less safe)
  batch.size: "131072"         # 128KB batches
  linger.ms: "50"              # Wait longer to fill batches
  compression.type: "lz4"      # Fast compression
  
batch:
  max_events: 5000             # Larger batches
  max_ms: 200                  # More time to accumulate
  max_inflight: 8              # More concurrent requests
</code></pre>
<h3 id="low-latency"><a class="header" href="#low-latency">Low Latency</a></h3>
<p>Optimize for minimal delay:</p>
<pre><code class="language-yaml">client_conf:
  acks: "1"                    # Don't wait for replicas
  batch.size: "16384"          # Smaller batches
  linger.ms: "0"               # Send immediately
  compression.type: "none"     # Skip compression
  
batch:
  max_events: 100              # Smaller batches
  max_ms: 10                   # Flush quickly
  max_inflight: 2              # Limit in-flight
</code></pre>
<h3 id="maximum-durability"><a class="header" href="#maximum-durability">Maximum Durability</a></h3>
<p>Optimize for zero data loss:</p>
<pre><code class="language-yaml">client_conf:
  acks: "all"                  # All replicas must ack
  enable.idempotence: "true"   # Prevent duplicates
  max.in.flight.requests.per.connection: "5"  # Required for idempotence
  retries: "2147483647"        # Infinite retries
  
exactly_once: true             # Transactional producer

batch:
  respect_source_tx: true      # Preserve source transactions
  max_inflight: 1              # Strict ordering
</code></pre>
<h2 id="running-the-example-6"><a class="header" href="#running-the-example-6">Running the Example</a></h2>
<h3 id="1-set-environment-variables-5"><a class="header" href="#1-set-environment-variables-5">1. Set Environment Variables</a></h3>
<pre><code class="language-bash">export POSTGRES_DSN="postgres://user:password@localhost:5432/orders"
export KAFKA_BROKERS="kafka1:9093,kafka2:9093,kafka3:9093"
export KAFKA_USERNAME="deltaforge"
export KAFKA_PASSWORD="secret"
</code></pre>
<h3 id="2-create-kafka-topic"><a class="header" href="#2-create-kafka-topic">2. Create Kafka Topic</a></h3>
<pre><code class="language-bash">kafka-topics.sh --create \
  --topic orders.cdc.events \
  --partitions 12 \
  --replication-factor 3 \
  --config min.insync.replicas=2 \
  --bootstrap-server ${KAFKA_BROKERS}
</code></pre>
<h3 id="3-start-deltaforge-1"><a class="header" href="#3-start-deltaforge-1">3. Start DeltaForge</a></h3>
<pre><code class="language-bash">cargo run -p runner --release -- --config kafka-prod.yaml
</code></pre>
<h2 id="monitoring-4"><a class="header" href="#monitoring-4">Monitoring</a></h2>
<h3 id="key-metrics-to-watch"><a class="header" href="#key-metrics-to-watch">Key Metrics to Watch</a></h3>
<ul>
<li><code>deltaforge_sink_events_sent_total</code> â€” Events delivered</li>
<li><code>deltaforge_sink_send_latency_seconds</code> â€” Delivery latency</li>
<li><code>deltaforge_sink_errors_total</code> â€” Delivery failures</li>
<li><code>deltaforge_checkpoint_lag_events</code> â€” Events pending checkpoint</li>
</ul>
<h3 id="health-check"><a class="header" href="#health-check">Health Check</a></h3>
<pre><code class="language-bash">curl http://localhost:8080/health
</code></pre>
<h3 id="pipeline-status"><a class="header" href="#pipeline-status">Pipeline Status</a></h3>
<pre><code class="language-bash">curl http://localhost:8080/pipelines/orders-to-kafka-prod
</code></pre>
<h2 id="key-concepts-demonstrated-5"><a class="header" href="#key-concepts-demonstrated-5">Key Concepts Demonstrated</a></h2>
<ul>
<li><strong>SASL/SSL Authentication</strong>: Secure broker connections</li>
<li><strong>Exactly-Once Semantics</strong>: Transactional producer for no duplicates</li>
<li><strong>Acknowledgment Modes</strong>: Trade-off between durability and latency</li>
<li><strong>Batching &amp; Compression</strong>: Optimize throughput</li>
<li><strong>Production Tuning</strong>: Real-world configuration patterns</li>
</ul>
<h2 id="related-documentation-5"><a class="header" href="#related-documentation-5">Related Documentation</a></h2>
<ul>
<li><a href="#kafka-sink">Kafka Sink</a> - Full configuration reference</li>
<li><a href="#envelopes-and-encodings">Envelopes</a> - Output format options</li>
<li><a href="#observability-playbook">Observability</a> - Metrics and monitoring</li>
<li><a href="#configuration">Configuration Reference</a> - Full spec documentation</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="redis-cache-invalidation"><a class="header" href="#redis-cache-invalidation">Redis Cache Invalidation</a></h1>
<p>This example demonstrates streaming database changes to a Redis stream where a worker can consume them to invalidate cache entries, ensuring cache consistency without polling.</p>
<h2 id="overview-8"><a class="header" href="#overview-8">Overview</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Component</th><th>Configuration</th></tr>
</thead>
<tbody>
<tr><td><strong>Source</strong></td><td>MySQL binlog CDC</td></tr>
<tr><td><strong>Processor</strong></td><td>JavaScript cache key generator</td></tr>
<tr><td><strong>Sink</strong></td><td>Redis Streams</td></tr>
<tr><td><strong>Pattern</strong></td><td>CDC-driven cache invalidation</td></tr>
</tbody>
</table>
</div>
<h2 id="use-case-6"><a class="header" href="#use-case-6">Use Case</a></h2>
<p>You have a MySQL database with Redis caching and want to:</p>
<ul>
<li>Stream change events that trigger cache invalidation</li>
<li>Avoid stale cache issues without TTL-based expiration</li>
<li>Generate cache keys matching your applicationâ€™s key format</li>
</ul>
<h2 id="architecture-1"><a class="header" href="#architecture-1">Architecture</a></h2>
<pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MySQL  â”‚â”€â”€â”€â”€&gt;â”‚ DeltaForge  â”‚â”€â”€â”€â”€&gt;â”‚ Redis Stream   â”‚â”€â”€â”€â”€&gt;â”‚ Worker  â”‚
â”‚         â”‚     â”‚             â”‚     â”‚ (invalidations)â”‚     â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                                                                â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
                                    â”‚  Redis Cache  â”‚&lt;â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚  (DEL keys)   â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<h2 id="pipeline-configuration-7"><a class="header" href="#pipeline-configuration-7">Pipeline Configuration</a></h2>
<pre><code class="language-yaml">apiVersion: deltaforge/v1
kind: Pipeline
metadata:
  name: cache-invalidation
  tenant: acme

spec:
  source:
    type: mysql
    config:
      id: app-mysql
      dsn: ${MYSQL_DSN}
      tables:
        - app.users
        - app.products
        - app.orders
        - app.inventory

  processors:
    - type: javascript
      id: generate-cache-keys
      inline: |
        function processBatch(events) {
          return events.map(event =&gt; {
            const keys = generateCacheKeys(event);
            const strategy = getStrategy(event);
            
            // Store cache keys in event.after metadata
            // (we can modify event.after since it's a JSON Value)
            if (event.after) {
              event.after._cache_keys = keys;
              event.after._invalidation_strategy = strategy;
            } else if (event.before) {
              // For deletes, add to before
              event.before._cache_keys = keys;
              event.before._invalidation_strategy = strategy;
            }
            
            // Add tags for routing/filtering
            event.tags = (event.tags || []).concat([
              'cache:invalidate',
              `strategy:${strategy}`,
              `keys:${keys.length}`
            ]);
            
            return event;
          });
        }
        
        function generateCacheKeys(event) {
          const table = event.table.split('.')[1];
          const keys = [];
          const record = event.after || event.before;
          
          if (!record || !record.id) return keys;
          const id = record.id;
          
          switch (table) {
            case 'users':
              keys.push(`user:${id}`);
              if (record.email) {
                keys.push(`user:email:${record.email}`);
              }
              // If email changed, invalidate old email key
              if (event.before &amp;&amp; event.before.email &amp;&amp; 
                  event.before.email !== record.email) {
                keys.push(`user:email:${event.before.email}`);
              }
              keys.push(`user:${id}:orders`);
              keys.push(`user:${id}:profile`);
              break;
              
            case 'products':
              keys.push(`product:${id}`);
              keys.push(`product:${id}:details`);
              if (record.category_id) {
                keys.push(`category:${record.category_id}:products`);
              }
              break;
              
            case 'orders':
              keys.push(`order:${id}`);
              if (record.user_id) {
                keys.push(`user:${record.user_id}:orders`);
              }
              break;
              
            case 'inventory':
              keys.push(`inventory:${record.product_id}`);
              keys.push(`product:${record.product_id}:stock`);
              break;
          }
          
          return keys;
        }
        
        function getStrategy(event) {
          const table = event.table.split('.')[1];
          if (table === 'inventory') return 'immediate';
          if (event.op === 'd') return 'immediate';
          return 'batched';
        }
      limits:
        timeout_ms: 500
        mem_mb: 128

  sinks:
    - type: redis
      config:
        id: invalidation-stream
        uri: ${REDIS_URI}
        stream: cache:invalidations
        envelope:
          type: native
        encoding: json
        required: true

  batch:
    max_events: 100
    max_ms: 100
    respect_source_tx: true

  commit_policy:
    mode: required
</code></pre>
<h2 id="javascript-processor-constraints"><a class="header" href="#javascript-processor-constraints">JavaScript Processor Constraints</a></h2>
<blockquote>
<p><strong>Important</strong>: The processor stores cache keys in <code>event.after._cache_keys</code> (or <code>event.before</code> for deletes) because we can only modify existing Event fields. Arbitrary top-level fields like <code>event.cache_keys</code> would be lost.</p>
</blockquote>
<h2 id="cache-worker-consumer"><a class="header" href="#cache-worker-consumer">Cache Worker (Consumer)</a></h2>
<pre><code class="language-javascript">// cache-invalidation-worker.js
const Redis = require('ioredis');

const redis = new Redis(process.env.REDIS_URI);
const streamKey = 'cache:invalidations';
const consumerGroup = 'cache-workers';
const consumerId = `worker-${process.pid}`;

async function processInvalidations() {
  try {
    await redis.xgroup('CREATE', streamKey, consumerGroup, '0', 'MKSTREAM');
  } catch (e) {
    if (!e.message.includes('BUSYGROUP')) throw e;
  }

  console.log(`Starting cache invalidation worker: ${consumerId}`);

  while (true) {
    try {
      const results = await redis.xreadgroup(
        'GROUP', consumerGroup, consumerId,
        'COUNT', 100, 'BLOCK', 5000,
        'STREAMS', streamKey, '&gt;'
      );

      if (!results) continue;

      for (const [stream, messages] of results) {
        for (const [id, fields] of messages) {
          const event = JSON.parse(fields[1]);
          await invalidateKeys(event);
          await redis.xack(streamKey, consumerGroup, id);
        }
      }
    } catch (error) {
      console.error('Worker error:', error);
      await new Promise(r =&gt; setTimeout(r, 1000));
    }
  }
}

async function invalidateKeys(event) {
  // Get cache keys from the event payload
  const record = event.after || event.before;
  const cacheKeys = record?._cache_keys || [];
  
  if (cacheKeys.length === 0) return;

  for (const key of cacheKeys) {
    if (key.includes('*')) {
      await scanAndDelete(key);
    } else {
      await redis.del(key);
    }
  }

  console.log(`Invalidated ${cacheKeys.length} keys for ${event.table}`);
}

async function scanAndDelete(pattern) {
  let cursor = '0';
  do {
    const [nextCursor, keys] = await redis.scan(cursor, 'MATCH', pattern, 'COUNT', 100);
    cursor = nextCursor;
    if (keys.length &gt; 0) {
      await redis.del(...keys);
    }
  } while (cursor !== '0');
}

processInvalidations().catch(console.error);
</code></pre>
<h2 id="running-the-example-7"><a class="header" href="#running-the-example-7">Running the Example</a></h2>
<h3 id="1-set-environment-variables-6"><a class="header" href="#1-set-environment-variables-6">1. Set Environment Variables</a></h3>
<pre><code class="language-bash">export MYSQL_DSN="mysql://user:password@localhost:3306/app"
export REDIS_URI="redis://localhost:6379"
</code></pre>
<h3 id="2-start-deltaforge-5"><a class="header" href="#2-start-deltaforge-5">2. Start DeltaForge</a></h3>
<pre><code class="language-bash">cargo run -p runner -- --config cache-invalidation.yaml
</code></pre>
<h3 id="3-start-workers"><a class="header" href="#3-start-workers">3. Start Worker(s)</a></h3>
<pre><code class="language-bash">node cache-invalidation-worker.js
</code></pre>
<h3 id="4-test-invalidation"><a class="header" href="#4-test-invalidation">4. Test Invalidation</a></h3>
<pre><code class="language-sql">UPDATE app.users SET email = 'alice.new@example.com' WHERE id = 1;
</code></pre>
<p>Worker output:</p>
<pre><code>Invalidated 5 keys for app.users
</code></pre>
<h2 id="key-concepts-demonstrated-6"><a class="header" href="#key-concepts-demonstrated-6">Key Concepts Demonstrated</a></h2>
<ul>
<li><strong>CDC to Stream</strong>: DeltaForge captures changes and writes to Redis Streams</li>
<li><strong>Custom Key Generation</strong>: Processor computes cache keys for downstream worker</li>
<li><strong>Consumer Groups</strong>: Scalable worker processing with acknowledgments</li>
<li><strong>Before/After Diffing</strong>: Compute invalidation keys for both old and new values</li>
</ul>
<blockquote>
<p><strong>Note</strong>: DeltaForge streams events to Redis; a separate worker (shown above) consumes them and performs the actual cache invalidation.</p>
</blockquote>
<h2 id="related-documentation-6"><a class="header" href="#related-documentation-6">Related Documentation</a></h2>
<ul>
<li><a href="#mysql-source">MySQL Source</a> - Binlog configuration</li>
<li><a href="#redis-sink">Redis Sink</a> - Stream configuration</li>
<li><a href="#processors">Processors</a> - JavaScript processor options</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="audit-trail-and-compliance-logging"><a class="header" href="#audit-trail-and-compliance-logging">Audit Trail and Compliance Logging</a></h1>
<p>This example demonstrates building an immutable audit trail for compliance requirements (SOC2, HIPAA, GDPR) by capturing all database changes with full context.</p>
<h2 id="overview-9"><a class="header" href="#overview-9">Overview</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Component</th><th>Configuration</th></tr>
</thead>
<tbody>
<tr><td><strong>Source</strong></td><td>PostgreSQL logical replication</td></tr>
<tr><td><strong>Processor</strong></td><td>JavaScript audit enrichment</td></tr>
<tr><td><strong>Sink</strong></td><td>Kafka (durable audit log)</td></tr>
<tr><td><strong>Pattern</strong></td><td>Compliance-grade change tracking</td></tr>
</tbody>
</table>
</div>
<h2 id="use-case-7"><a class="header" href="#use-case-7">Use Case</a></h2>
<p>You need to meet compliance requirements and want to:</p>
<ul>
<li>Capture every change to sensitive tables with before/after values</li>
<li>Add audit metadata (classification, retention, regulations)</li>
<li>Redact sensitive fields while preserving change records</li>
<li>Store immutable audit logs for required retention periods</li>
</ul>
<h2 id="pipeline-configuration-8"><a class="header" href="#pipeline-configuration-8">Pipeline Configuration</a></h2>
<pre><code class="language-yaml">apiVersion: deltaforge/v1
kind: Pipeline
metadata:
  name: compliance-audit-trail
  tenant: acme

spec:
  source:
    type: postgres
    config:
      id: app-postgres
      dsn: ${POSTGRES_DSN}
      slot: deltaforge_audit
      publication: audit_pub
      tables:
        - public.users
        - public.user_profiles
        - public.payment_methods
        - public.transactions
        - public.roles
        - public.permissions
        - public.user_roles
      start_position: earliest

  processors:
    - type: javascript
      id: audit-enrichment
      inline: |
        function processBatch(events) {
          return events.map(event =&gt; {
            const table = event.table.split('.')[1];
            
            // Build audit tags (event.tags is a valid Event field)
            const auditTags = [
              'audited',
              `sensitivity:${classifySensitivity(table)}`,
              `classification:${getDataClassification(table)}`,
              `retention:${getRetentionPeriod(table)}d`
            ];
            
            // Add regulation tags
            for (const reg of getApplicableRegulations(table)) {
              auditTags.push(`regulation:${reg}`);
            }
            
            // Track changed fields for updates
            if (event.before &amp;&amp; event.after) {
              const changed = detectChangedFields(event.before, event.after);
              for (const field of changed) {
                auditTags.push(`changed:${field}`);
              }
            }
            
            event.tags = (event.tags || []).concat(auditTags);
            
            // Redact sensitive fields in before/after
            if (event.before) {
              event.before = sanitizeRecord(event.before, table);
            }
            if (event.after) {
              event.after = sanitizeRecord(event.after, table);
            }
            
            return event;
          });
        }
        
        function classifySensitivity(table) {
          const highSensitivity = ['users', 'payment_methods', 'transactions'];
          const mediumSensitivity = ['user_profiles', 'user_roles'];
          
          if (highSensitivity.includes(table)) return 'HIGH';
          if (mediumSensitivity.includes(table)) return 'MEDIUM';
          return 'LOW';
        }
        
        function sanitizeRecord(record, table) {
          if (!record) return null;
          const sanitized = { ...record };
          
          const sensitiveFields = {
            'users': ['password_hash', 'ssn', 'tax_id'],
            'payment_methods': ['card_number', 'cvv', 'account_number'],
            'user_profiles': ['date_of_birth']
          };
          
          const fieldsToMask = sensitiveFields[table] || [];
          
          for (const field of fieldsToMask) {
            if (sanitized[field] !== undefined) {
              sanitized[`_${field}_redacted`] = true;
              sanitized[field] = '[REDACTED]';
            }
          }
          
          return sanitized;
        }
        
        function detectChangedFields(before, after) {
          const changed = [];
          const allKeys = new Set([...Object.keys(before), ...Object.keys(after)]);
          for (const key of allKeys) {
            if (JSON.stringify(before[key]) !== JSON.stringify(after[key])) {
              changed.push(key);
            }
          }
          return changed;
        }
        
        function getRetentionPeriod(table) {
          if (['transactions', 'payment_methods'].includes(table)) return 2555;
          if (['users', 'user_profiles'].includes(table)) return 2190;
          return 1095;
        }
        
        function getDataClassification(table) {
          if (['payment_methods', 'transactions'].includes(table)) return 'PCI';
          if (['users', 'user_profiles'].includes(table)) return 'PII';
          return 'INTERNAL';
        }
        
        function getApplicableRegulations(table) {
          const regs = [];
          if (['users', 'user_profiles'].includes(table)) regs.push('GDPR', 'CCPA');
          if (['payment_methods', 'transactions'].includes(table)) regs.push('PCI-DSS', 'SOX');
          return regs;
        }
      limits:
        timeout_ms: 1000
        mem_mb: 256

  sinks:
    - type: kafka
      config:
        id: audit-kafka
        brokers: ${KAFKA_BROKERS}
        topic: audit.trail.events
        envelope:
          type: debezium
        encoding: json
        required: true
        exactly_once: true
        client_conf:
          acks: "all"
          enable.idempotence: "true"
          compression.type: "gzip"

  batch:
    max_events: 500
    max_bytes: 1048576
    max_ms: 1000
    respect_source_tx: true

  commit_policy:
    mode: required
</code></pre>
<h2 id="javascript-processor-constraints-1"><a class="header" href="#javascript-processor-constraints-1">JavaScript Processor Constraints</a></h2>
<blockquote>
<p><strong>Important</strong>: The JavaScript processor can only modify fields that exist on DeltaForgeâ€™s <code>Event</code> struct. You can:</p>
<ul>
<li>Modify <code>event.before</code> and <code>event.after</code> values (JSON objects)</li>
<li>Set <code>event.tags</code> (array of strings)</li>
<li>Filter out events (return empty array)</li>
</ul>
<p>You <strong>cannot</strong> add arbitrary top-level fields like <code>event.audit</code> or <code>event.metadata</code> - they will be lost during serialization.
This limitation will be addressed soon.</p>
</blockquote>
<p>This example uses <code>event.tags</code> to store audit metadata as key:value strings that downstream systems can parse.</p>
<h2 id="postgresql-setup-1"><a class="header" href="#postgresql-setup-1">PostgreSQL Setup</a></h2>
<pre><code class="language-sql">-- Create publication for audited tables
CREATE PUBLICATION audit_pub FOR TABLE 
  public.users,
  public.user_profiles,
  public.payment_methods,
  public.transactions,
  public.roles,
  public.permissions,
  public.user_roles
WITH (publish = 'insert, update, delete');

-- Enable REPLICA IDENTITY FULL to capture before values on updates
ALTER TABLE public.users REPLICA IDENTITY FULL;
ALTER TABLE public.user_profiles REPLICA IDENTITY FULL;
ALTER TABLE public.payment_methods REPLICA IDENTITY FULL;
ALTER TABLE public.transactions REPLICA IDENTITY FULL;
</code></pre>
<h2 id="sample-audit-event-output"><a class="header" href="#sample-audit-event-output">Sample Audit Event Output</a></h2>
<p>With Debezium envelope:</p>
<pre><code class="language-json">{
  "payload": {
    "before": {
      "id": 42,
      "email": "alice@old-domain.com",
      "name": "Alice Smith",
      "password_hash": "[REDACTED]",
      "_password_hash_redacted": true
    },
    "after": {
      "id": 42,
      "email": "alice@new-domain.com",
      "name": "Alice Smith",
      "password_hash": "[REDACTED]",
      "_password_hash_redacted": true
    },
    "source": {
      "version": "0.1.0",
      "connector": "postgres",
      "name": "app-postgres",
      "ts_ms": 1705312199123,
      "db": "app",
      "schema": "public",
      "table": "users"
    },
    "op": "u",
    "ts_ms": 1705312200000,
    "tags": [
      "audited",
      "sensitivity:HIGH",
      "classification:PII",
      "retention:2190d",
      "regulation:GDPR",
      "regulation:CCPA",
      "changed:email"
    ]
  }
}
</code></pre>
<h2 id="parsing-audit-tags"><a class="header" href="#parsing-audit-tags">Parsing Audit Tags</a></h2>
<p>Downstream consumers can parse the structured tags:</p>
<pre><code class="language-javascript">// Parse audit tags into an object
function parseAuditTags(tags) {
  const audit = {};
  for (const tag of tags || []) {
    if (tag.includes(':')) {
      const [key, value] = tag.split(':', 2);
      if (audit[key]) {
        if (!Array.isArray(audit[key])) {
          audit[key] = [audit[key]];
        }
        audit[key].push(value);
      } else {
        audit[key] = value;
      }
    } else {
      audit[tag] = true;
    }
  }
  return audit;
}

// Example: parseAuditTags(event.tags)
// Returns: {
//   audited: true,
//   sensitivity: "HIGH",
//   classification: "PII",
//   retention: "2190d",
//   regulation: ["GDPR", "CCPA"],
//   changed: "email"
// }
</code></pre>
<h2 id="kafka-topic-configuration"><a class="header" href="#kafka-topic-configuration">Kafka Topic Configuration</a></h2>
<pre><code class="language-bash">kafka-topics.sh --create \
  --topic audit.trail.events \
  --partitions 12 \
  --replication-factor 3 \
  --config retention.ms=189216000000 \
  --config cleanup.policy=delete \
  --config min.insync.replicas=2 \
  --config compression.type=gzip \
  --bootstrap-server ${KAFKA_BROKERS}
</code></pre>
<h2 id="running-the-example-8"><a class="header" href="#running-the-example-8">Running the Example</a></h2>
<h3 id="1-set-environment-variables-7"><a class="header" href="#1-set-environment-variables-7">1. Set Environment Variables</a></h3>
<pre><code class="language-bash">export POSTGRES_DSN="postgres://user:password@localhost:5432/app"
export KAFKA_BROKERS="kafka1:9092,kafka2:9092,kafka3:9092"
</code></pre>
<h3 id="2-start-deltaforge-6"><a class="header" href="#2-start-deltaforge-6">2. Start DeltaForge</a></h3>
<pre><code class="language-bash">cargo run -p runner -- --config audit-trail.yaml
</code></pre>
<h3 id="3-verify-audit-events"><a class="header" href="#3-verify-audit-events">3. Verify Audit Events</a></h3>
<pre><code class="language-bash">./dev.sh k-consume audit.trail.events --from-beginning
</code></pre>
<h2 id="querying-audit-data"><a class="header" href="#querying-audit-data">Querying Audit Data</a></h2>
<pre><code class="language-sql">-- Find high-sensitivity changes
SELECT * FROM audit_events 
WHERE ARRAY_CONTAINS(payload.tags, 'sensitivity:HIGH');

-- Find all email changes
SELECT * FROM audit_events
WHERE ARRAY_CONTAINS(payload.tags, 'changed:email');

-- Find PCI-regulated changes
SELECT * FROM audit_events
WHERE ARRAY_CONTAINS(payload.tags, 'regulation:PCI-DSS');
</code></pre>
<h2 id="key-concepts-demonstrated-7"><a class="header" href="#key-concepts-demonstrated-7">Key Concepts Demonstrated</a></h2>
<ul>
<li><strong>Full Change Capture</strong>: Before and after values with REPLICA IDENTITY FULL</li>
<li><strong>PII Redaction</strong>: Sensitive fields masked, presence tracked via <code>_field_redacted</code></li>
<li><strong>Tag-Based Metadata</strong>: Audit info stored in <code>event.tags</code> as parseable strings</li>
<li><strong>Immutable Storage</strong>: Exactly-once delivery to append-only Kafka log</li>
<li><strong>Compliance Tagging</strong>: Retention periods, classifications, regulations as tags</li>
</ul>
<h2 id="related-documentation-7"><a class="header" href="#related-documentation-7">Related Documentation</a></h2>
<ul>
<li><a href="#postgresql-source">PostgreSQL Source</a> - Logical replication setup</li>
<li><a href="#kafka-sink">Kafka Sink</a> - Exactly-once and durability settings</li>
<li><a href="#processors">Processors</a> - JavaScript processor constraints</li>
<li><a href="#envelopes-and-encodings">Envelopes</a> - Output format options</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="real-time-analytics-preprocessing-pipeline"><a class="header" href="#real-time-analytics-preprocessing-pipeline">Real-Time Analytics Preprocessing Pipeline</a></h1>
<p>This example demonstrates preparing CDC events for real-time analytics by enriching events with dimensions, metrics, and routing tags (pre-processing).</p>
<h2 id="overview-10"><a class="header" href="#overview-10">Overview</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Component</th><th>Configuration</th></tr>
</thead>
<tbody>
<tr><td><strong>Source</strong></td><td>MySQL binlog CDC</td></tr>
<tr><td><strong>Processor</strong></td><td>JavaScript analytics enrichment</td></tr>
<tr><td><strong>Sinks</strong></td><td>Kafka (stream processing) + Redis (real-time counters)</td></tr>
<tr><td><strong>Pattern</strong></td><td>Analytics-ready event preparation</td></tr>
</tbody>
</table>
</div>
<h2 id="use-case-8"><a class="header" href="#use-case-8">Use Case</a></h2>
<p>You have an e-commerce MySQL database and want to:</p>
<ul>
<li>Stream order events to real-time dashboards</li>
<li>Feed a worker that maintains live counters in Redis</li>
<li>Prepare events for stream processing (Flink, Spark Streaming)</li>
</ul>
<h2 id="pipeline-configuration-9"><a class="header" href="#pipeline-configuration-9">Pipeline Configuration</a></h2>
<pre><code class="language-yaml">apiVersion: deltaforge/v1
kind: Pipeline
metadata:
  name: ecommerce-analytics
  tenant: acme

spec:
  source:
    type: mysql
    config:
      id: ecommerce-mysql
      dsn: ${MYSQL_DSN}
      tables:
        - shop.orders
        - shop.order_items
        - shop.payments
        - shop.cart_events

  processors:
    - type: javascript
      id: analytics-enrichment
      inline: |
        function processBatch(events) {
          return events.map(event =&gt; {
            const table = event.table.split('.')[1];
            const record = event.after || event.before;
            
            // Add analytics metadata to event.after
            // (we can modify event.after since it's a JSON Value)
            if (event.after) {
              event.after._analytics = {
                event_type: `${table}.${mapOperation(event.op)}`,
                dimensions: extractDimensions(table, record, event),
                metrics: extractMetrics(table, record)
              };
            }
            
            // Add routing tags
            event.tags = generateTags(table, record, event);
            
            return event;
          });
        }
        
        function mapOperation(op) {
          return { 'c': 'created', 'u': 'updated', 'd': 'deleted', 'r': 'snapshot' }[op] || op;
        }
        
        function extractDimensions(table, record, event) {
          if (!record) return {};
          
          const dims = {
            hour_of_day: new Date(event.timestamp).getUTCHours(),
            day_of_week: new Date(event.timestamp).getUTCDay()
          };
          
          switch (table) {
            case 'orders':
              dims.customer_id = record.customer_id;
              dims.status = record.status;
              dims.channel = record.channel || 'web';
              break;
            case 'order_items':
              dims.order_id = record.order_id;
              dims.product_id = record.product_id;
              break;
            case 'payments':
              dims.order_id = record.order_id;
              dims.payment_method = record.method;
              break;
          }
          
          return dims;
        }
        
        function extractMetrics(table, record) {
          if (!record) return {};
          
          const metrics = { event_count: 1 };
          
          switch (table) {
            case 'orders':
              metrics.order_total = parseFloat(record.total) || 0;
              metrics.item_count = parseInt(record.item_count) || 0;
              break;
            case 'order_items':
              metrics.quantity = parseInt(record.quantity) || 1;
              metrics.line_total = parseFloat(record.line_total) || 0;
              break;
            case 'payments':
              metrics.payment_amount = parseFloat(record.amount) || 0;
              break;
          }
          
          return metrics;
        }
        
        function generateTags(table, record, event) {
          const tags = [table, event.op];
          
          if (!record) return tags;
          
          if (table === 'orders' &amp;&amp; record.total &gt; 500) {
            tags.push('high_value');
          }
          if (record.status) {
            tags.push(`status:${record.status}`);
          }
          
          return tags;
        }
      limits:
        timeout_ms: 500
        mem_mb: 256

  sinks:
    - type: kafka
      config:
        id: analytics-kafka
        brokers: ${KAFKA_BROKERS}
        topic: analytics.events
        envelope:
          type: native
        encoding: json
        required: true
        client_conf:
          compression.type: "lz4"

    - type: redis
      config:
        id: realtime-redis
        uri: ${REDIS_URI}
        stream: analytics:realtime
        envelope:
          type: native
        encoding: json
        required: false

  batch:
    max_events: 500
    max_bytes: 1048576
    max_ms: 100
    respect_source_tx: false

  commit_policy:
    mode: required
</code></pre>
<h2 id="javascript-processor-constraints-2"><a class="header" href="#javascript-processor-constraints-2">JavaScript Processor Constraints</a></h2>
<blockquote>
<p><strong>Important</strong>: Analytics metadata is stored in <code>event.after._analytics</code> because the processor can only modify existing Event fields (<code>before</code>, <code>after</code>, <code>tags</code>). Arbitrary top-level fields would be lost during serialization.</p>
</blockquote>
<h2 id="sample-event-output"><a class="header" href="#sample-event-output">Sample Event Output</a></h2>
<pre><code class="language-json">{
  "before": null,
  "after": {
    "id": 98765,
    "customer_id": 12345,
    "total": 299.99,
    "status": "pending",
    "channel": "mobile",
    "_analytics": {
      "event_type": "orders.created",
      "dimensions": {
        "hour_of_day": 10,
        "day_of_week": 3,
        "customer_id": 12345,
        "status": "pending",
        "channel": "mobile"
      },
      "metrics": {
        "event_count": 1,
        "order_total": 299.99,
        "item_count": 3
      }
    }
  },
  "source": {
    "connector": "mysql",
    "db": "shop",
    "table": "orders"
  },
  "op": "c",
  "ts_ms": 1705312200000,
  "tags": ["orders", "c", "status:pending"]
}
</code></pre>
<h2 id="redis-counter-worker"><a class="header" href="#redis-counter-worker">Redis Counter Worker</a></h2>
<pre><code class="language-javascript">const Redis = require('ioredis');
const redis = new Redis(process.env.REDIS_URI);

async function processAnalyticsEvents() {
  let lastId = '0';
  
  while (true) {
    const results = await redis.xread(
      'COUNT', 100, 'BLOCK', 1000,
      'STREAMS', 'analytics:realtime', lastId
    );
    
    if (!results) continue;
    
    for (const [stream, messages] of results) {
      for (const [id, fields] of messages) {
        const event = JSON.parse(fields[1]);
        await updateCounters(event);
        lastId = id;
      }
    }
  }
}

async function updateCounters(event) {
  const pipe = redis.pipeline();
  const now = new Date();
  const hourKey = `${now.getUTCFullYear()}:${now.getUTCMonth()+1}:${now.getUTCDate()}:${now.getUTCHours()}`;
  const dayKey = `${now.getUTCFullYear()}:${now.getUTCMonth()+1}:${now.getUTCDate()}`;
  
  // Get analytics from event.after
  const analytics = event.after?._analytics || {};
  const eventType = analytics.event_type || `${event.table}.${event.op}`;
  const metrics = analytics.metrics || {};
  const dimensions = analytics.dimensions || {};
  
  // Event counts
  pipe.hincrby(`stats:events:${hourKey}`, eventType, 1);
  
  // Order-specific counters
  if (eventType === 'orders.created') {
    pipe.incr(`stats:orders:count:${hourKey}`);
    pipe.incrbyfloat(`stats:orders:revenue:${hourKey}`, metrics.order_total || 0);
    
    if (dimensions.channel) {
      pipe.hincrby(`stats:orders:channel:${dayKey}`, dimensions.channel, 1);
    }
  }
  
  // High-value alerts
  if (event.tags?.includes('high_value')) {
    pipe.lpush('alerts:high_value_orders', JSON.stringify({
      order_id: event.after?.id,
      total: metrics.order_total
    }));
    pipe.ltrim('alerts:high_value_orders', 0, 99);
  }
  
  pipe.expire(`stats:events:${hourKey}`, 90000);
  await pipe.exec();
}

processAnalyticsEvents().catch(console.error);
</code></pre>
<h2 id="running-the-example-9"><a class="header" href="#running-the-example-9">Running the Example</a></h2>
<h3 id="1-set-environment-variables-8"><a class="header" href="#1-set-environment-variables-8">1. Set Environment Variables</a></h3>
<pre><code class="language-bash">export MYSQL_DSN="mysql://user:password@localhost:3306/shop"
export KAFKA_BROKERS="localhost:9092"
export REDIS_URI="redis://localhost:6379"
</code></pre>
<h3 id="2-create-kafka-topic-1"><a class="header" href="#2-create-kafka-topic-1">2. Create Kafka Topic</a></h3>
<pre><code class="language-bash">./dev.sh k-create analytics.events 12
</code></pre>
<h3 id="3-start-deltaforge-2"><a class="header" href="#3-start-deltaforge-2">3. Start DeltaForge</a></h3>
<pre><code class="language-bash">cargo run -p runner -- --config analytics-pipeline.yaml
</code></pre>
<h3 id="4-start-counter-worker"><a class="header" href="#4-start-counter-worker">4. Start Counter Worker</a></h3>
<pre><code class="language-bash">node realtime-counter-worker.js
</code></pre>
<h2 id="key-concepts-demonstrated-8"><a class="header" href="#key-concepts-demonstrated-8">Key Concepts Demonstrated</a></h2>
<ul>
<li><strong>Event Enrichment</strong>: Add dimensions/metrics in <code>event.after._analytics</code></li>
<li><strong>Tag-Based Filtering</strong>: Use <code>event.tags</code> for high-value order detection</li>
<li><strong>Multi-Sink Fan-Out</strong>: Kafka for stream processing + Redis for worker consumption</li>
<li><strong>Worker Pattern</strong>: Separate worker consumes Redis stream to update counters</li>
</ul>
<h2 id="related-documentation-8"><a class="header" href="#related-documentation-8">Related Documentation</a></h2>
<ul>
<li><a href="#mysql-source">MySQL Source</a> - Binlog configuration</li>
<li><a href="#kafka-sink">Kafka Sink</a> - Producer settings</li>
<li><a href="#redis-sink">Redis Sink</a> - Stream configuration</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="outbox-pattern-1"><a class="header" href="#outbox-pattern-1">Outbox Pattern</a></h1>
<p>This example demonstrates the transactional outbox pattern - writing business data and a domain event in the same database transaction, then streaming the event to Kafka via DeltaForge.</p>
<h2 id="overview-11"><a class="header" href="#overview-11">Overview</a></h2>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Component</th><th>Configuration</th></tr>
</thead>
<tbody>
<tr><td><strong>Source</strong></td><td>MySQL binlog CDC</td></tr>
<tr><td><strong>Processor</strong></td><td>Outbox (extract + route)</td></tr>
<tr><td><strong>Sink</strong></td><td>Kafka (per-aggregate topics)</td></tr>
<tr><td><strong>Pattern</strong></td><td>Transactional outbox with raw payload delivery</td></tr>
</tbody>
</table>
</div>
<h2 id="use-case-9"><a class="header" href="#use-case-9">Use Case</a></h2>
<p>Your application writes orders and needs to publish <code>OrderCreated</code>, <code>OrderShipped</code>, etc. events to Kafka. You want:</p>
<ul>
<li><strong>Atomicity</strong>: event published if and only if the transaction commits</li>
<li><strong>Clean payloads</strong>: consumers receive the applicationâ€™s JSON, not CDC envelopes</li>
<li><strong>Per-aggregate routing</strong>: events land on <code>Order.OrderCreated</code>, <code>Payment.PaymentReceived</code>, etc.</li>
<li><strong>Zero polling</strong>: DeltaForge tails the binlog, no application-side outbox relay</li>
</ul>
<h2 id="database-setup"><a class="header" href="#database-setup">Database Setup</a></h2>
<pre><code class="language-sql">-- Business table
CREATE TABLE orders (
  id         INT AUTO_INCREMENT PRIMARY KEY,
  customer   VARCHAR(64),
  total      DECIMAL(10,2),
  status     VARCHAR(32),
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Outbox table (BLACKHOLE = binlog only, no disk storage)
CREATE TABLE outbox (
  id             INT AUTO_INCREMENT PRIMARY KEY,
  aggregate_type VARCHAR(64),
  aggregate_id   VARCHAR(64),
  event_type     VARCHAR(64),
  payload        JSON
) ENGINE=BLACKHOLE;
</code></pre>
<h2 id="application-code"><a class="header" href="#application-code">Application Code</a></h2>
<p>Write both in the same transaction:</p>
<pre><code class="language-sql">BEGIN;

INSERT INTO orders (customer, total, status)
VALUES ('alice', 149.99, 'pending');

INSERT INTO outbox (aggregate_type, aggregate_id, event_type, payload)
VALUES (
  'Order',
  LAST_INSERT_ID(),
  'OrderCreated',
  JSON_OBJECT('customer', 'alice', 'total', 149.99, 'status', 'pending')
);

COMMIT;
</code></pre>
<p>If the transaction rolls back, neither the order nor the event exist. If it commits, both do.</p>
<h2 id="pipeline-configuration-10"><a class="header" href="#pipeline-configuration-10">Pipeline Configuration</a></h2>
<pre><code class="language-yaml">apiVersion: deltaforge/v1
kind: Pipeline
metadata:
  name: orders-outbox
  tenant: acme

spec:
  source:
    type: mysql
    config:
      id: orders-mysql
      dsn: ${MYSQL_DSN}
      tables:
        - shop.orders
        - shop.outbox
      outbox:
        tables: ["shop.outbox"]

  processors:
    - type: outbox
      topic: "${aggregate_type}.${event_type}"
      default_topic: events.unrouted
      raw_payload: true

  sinks:
    - type: kafka
      config:
        id: events-kafka
        brokers: ${KAFKA_BROKERS}
        topic: "cdc.${source.db}.${source.table}"
        envelope:
          type: debezium
        encoding: json
        required: true

  batch:
    max_events: 500
    max_ms: 500
    respect_source_tx: true

  commit_policy:
    mode: required
</code></pre>
<h3 id="what-lands-on-kafka"><a class="header" href="#what-lands-on-kafka">What lands on Kafka</a></h3>
<p><strong>Outbox event</strong> â€“&gt; topic <code>Order.OrderCreated</code>:</p>
<pre><code class="language-json">{"customer": "alice", "total": 149.99, "status": "pending"}
</code></pre>
<p>Raw payload, no envelope. Kafka headers carry metadata: <code>df-aggregate-type: Order</code>, <code>df-aggregate-id: 1</code>, <code>df-event-type: OrderCreated</code>.</p>
<p><strong>CDC event</strong> â€“&gt; topic <code>cdc.shop.orders</code> (from sinkâ€™s topic template):</p>
<pre><code class="language-json">{
  "payload": {
    "before": null,
    "after": {"id": 1, "customer": "alice", "total": 149.99, "status": "pending"},
    "source": {"connector": "mysql", "db": "shop", "table": "orders"},
    "op": "c",
    "ts_ms": 1700000000000
  }
}
</code></pre>
<p>Full Debezium envelope. Both flow through the same pipeline - the outbox processor only touches events tagged as outbox.</p>
<h2 id="running-the-example-10"><a class="header" href="#running-the-example-10">Running the Example</a></h2>
<h3 id="1-start-infrastructure-1"><a class="header" href="#1-start-infrastructure-1">1. Start Infrastructure</a></h3>
<pre><code class="language-bash">./dev.sh up
./dev.sh k-create Order.OrderCreated 3
./dev.sh k-create Order.OrderShipped 3
./dev.sh k-create cdc.shop.orders 3
</code></pre>
<h3 id="2-set-environment-variables"><a class="header" href="#2-set-environment-variables">2. Set Environment Variables</a></h3>
<pre><code class="language-bash">export MYSQL_DSN="mysql://deltaforge:dfpw@localhost:3306/shop"
export KAFKA_BROKERS="localhost:9092"
</code></pre>
<h3 id="3-start-deltaforge-3"><a class="header" href="#3-start-deltaforge-3">3. Start DeltaForge</a></h3>
<pre><code class="language-bash">cargo run -p runner -- --config outbox.yaml
</code></pre>
<h3 id="4-insert-test-data-1"><a class="header" href="#4-insert-test-data-1">4. Insert Test Data</a></h3>
<pre><code class="language-sql">BEGIN;
INSERT INTO shop.orders (customer, total, status) VALUES ('alice', 149.99, 'pending');
INSERT INTO shop.outbox (aggregate_type, aggregate_id, event_type, payload)
VALUES ('Order', LAST_INSERT_ID(), 'OrderCreated',
        '{"customer":"alice","total":149.99,"status":"pending"}');
COMMIT;
</code></pre>
<h3 id="5-verify"><a class="header" href="#5-verify">5. Verify</a></h3>
<pre><code class="language-bash"># Outbox event (raw payload, per-aggregate topic)
./dev.sh k-inspect Order.OrderCreated

# CDC event (Debezium envelope, per-table topic)
./dev.sh k-inspect cdc.shop.orders
</code></pre>
<h2 id="variations-3"><a class="header" href="#variations-3">Variations</a></h2>
<h3 id="postgresql-with-wal-messages"><a class="header" href="#postgresql-with-wal-messages">PostgreSQL with WAL Messages</a></h3>
<p>PostgreSQL doesnâ€™t need an outbox table - write directly to the WAL:</p>
<pre><code class="language-sql">BEGIN;
INSERT INTO orders (customer, total, status) VALUES ('alice', 149.99, 'pending');
SELECT pg_logical_emit_message(
  true, 'outbox',
  '{"aggregate_type":"Order","aggregate_id":"1","event_type":"OrderCreated","payload":{"customer":"alice","total":149.99}}'
);
COMMIT;
</code></pre>
<pre><code class="language-yaml">source:
  type: postgres
  config:
    id: orders-pg
    dsn: ${POSTGRES_DSN}
    slot: deltaforge_orders
    publication: orders_pub
    tables: [public.orders]
    outbox:
      prefixes: [outbox]
</code></pre>
<p>No table, no index, no vacuum - just a WAL entry.</p>
<h3 id="multiple-outbox-channels"><a class="header" href="#multiple-outbox-channels">Multiple Outbox Channels</a></h3>
<p>Route order events and payment events to different topic hierarchies:</p>
<pre><code class="language-yaml">processors:
  - type: outbox
    tables: [orders_outbox]
    topic: "orders.${event_type}"
    raw_payload: true

  - type: outbox
    tables: [payments_outbox]
    topic: "payments.${event_type}"
    raw_payload: true
    columns:
      payload: data
</code></pre>
<h3 id="additional-headers-for-tracing"><a class="header" href="#additional-headers-for-tracing">Additional Headers for Tracing</a></h3>
<p>Forward trace and correlation IDs as Kafka headers:</p>
<pre><code class="language-yaml">processors:
  - type: outbox
    topic: "${aggregate_type}.${event_type}"
    raw_payload: true
    additional_headers:
      x-trace-id: trace_id
      x-correlation-id: correlation_id
</code></pre>
<pre><code class="language-sql">INSERT INTO outbox (aggregate_type, aggregate_id, event_type, payload, trace_id, correlation_id)
VALUES ('Order', '42', 'OrderCreated', '{"total":99.99}', 'abc-123', 'req-456');
</code></pre>
<h3 id="migrating-from-debezium-1"><a class="header" href="#migrating-from-debezium-1">Migrating from Debezium</a></h3>
<p>If your outbox table uses Debeziumâ€™s column names (<code>aggregatetype</code>, <code>aggregateid</code>, <code>type</code>):</p>
<pre><code class="language-yaml">processors:
  - type: outbox
    topic: "${aggregatetype}.${type}"
    raw_payload: true
    columns:
      aggregate_type: aggregatetype
      aggregate_id: aggregateid
      event_type: type
    additional_headers:
      x-trace-id: traceid
      x-tenant: tenant
</code></pre>
<p>Column mappings control header extraction. The topic template uses raw column names directly.</p>
<h2 id="key-concepts-demonstrated-9"><a class="header" href="#key-concepts-demonstrated-9">Key Concepts Demonstrated</a></h2>
<ul>
<li><strong>Transactional outbox</strong>: atomicity without distributed transactions</li>
<li><strong>BLACKHOLE engine</strong>: binlog-only storage for zero-cost outbox tables on MySQL</li>
<li><strong>Raw payload delivery</strong>: <code>raw_payload: true</code> bypasses envelope wrapping - consumers get exactly what the application wrote</li>
<li><strong>Mixed pipeline</strong>: outbox events and CDC events coexist in the same pipeline with different serialization</li>
<li><strong>Per-aggregate routing</strong>: topic template routes events by <code>aggregate_type</code> and <code>event_type</code></li>
</ul>
<h2 id="related-documentation-9"><a class="header" href="#related-documentation-9">Related Documentation</a></h2>
<ul>
<li><a href="#outbox-pattern">Outbox Pattern</a> - Full outbox reference with configuration details</li>
<li><a href="#mysql-source">MySQL Source</a> - Binlog prerequisites</li>
<li><a href="#postgresql-source">PostgreSQL Source</a> - WAL message setup</li>
<li><a href="#dynamic-routing">Dynamic Routing</a> - Template syntax for topic/key resolution</li>
<li><a href="#envelopes-and-encodings">Envelopes</a> - Native, Debezium, CloudEvents formats</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="troubleshooting-3"><a class="header" href="#troubleshooting-3">Troubleshooting</a></h1>
<p>Common issues and quick checks when running DeltaForge.</p>
<ul>
<li>ğŸ©º <strong>Health-first</strong>: start with <code>/healthz</code> and <code>/readyz</code> to pinpoint failing components.</li>
</ul>
<h2 id="runner-fails-to-start"><a class="header" href="#runner-fails-to-start">Runner fails to start</a></h2>
<ul>
<li>Confirm the config path passed to <code>--config</code> exists and is readable.</li>
<li>Validate YAML syntax and that required fields like <code>metadata.name</code> and <code>spec.source</code> are present.</li>
<li>Ensure environment variables referenced in the spec are set (<code>dsn</code>, <code>brokers</code>, <code>uri</code>, etc.).</li>
</ul>
<h2 id="pipelines-remain-unready"><a class="header" href="#pipelines-remain-unready">Pipelines remain unready</a></h2>
<ul>
<li>Check the <code>/readyz</code> endpoint for per-pipeline status and error messages.</li>
<li>Verify upstream credentials allow replication (MySQL binlog). Other engines are experimental unless explicitly documented.</li>
<li>Inspect sink connectivity; a required sink that cannot connect will block checkpoints.</li>
</ul>
<h2 id="slow-throughput"><a class="header" href="#slow-throughput">Slow throughput</a></h2>
<ul>
<li>Increase <code>batch.max_events</code> or <code>batch.max_bytes</code> to reduce flush frequency.</li>
<li>Adjust <code>max_inflight</code> to allow more concurrent batches if sinks can handle parallelism.</li>
<li>Reduce processor work or add guardrails (<code>limits</code>) to prevent slow JavaScript from stalling the pipeline.</li>
</ul>
<h2 id="checkpoints-not-advancing"><a class="header" href="#checkpoints-not-advancing">Checkpoints not advancing</a></h2>
<ul>
<li>Review the commit policy: <code>mode: all</code> or <code>required</code> sinks that are unavailable will block progress.</li>
<li>Look for sink-specific errors (for example, Kafka broker unreachability or Redis backpressure).</li>
<li>Pause and resume the pipeline to force a clean restart after addressing the underlying issue.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<p align="center">
  <img src="assets/deltaforge-dev.png" width="250" alt="DeltaForge">
</p>

<h1 id="development-guide"><a class="header" href="#development-guide">Development Guide</a></h1>
<p>Use this guide to build, test, and extend DeltaForge. It covers local workflows, optional dependency containers, and how to work with Docker images.</p>
<p>All contributions are welcome and highly appreciated.</p>
<h2 id="local-prerequisites"><a class="header" href="#local-prerequisites">Local prerequisites</a></h2>
<ul>
<li>Rust toolchain 1.89+ (install via <a href="https://rustup.rs"><code>rustup</code></a>).</li>
<li>Optional: Docker or Podman for running the dev dependency stack and the container image.</li>
</ul>
<h2 id="workspace-layout"><a class="header" href="#workspace-layout">Workspace layout</a></h2>
<ul>
<li><code>crates/deltaforge-core</code> : shared event model, pipeline engine, and checkpointing primitives.</li>
<li><code>crates/deltaforge-config</code> : YAML config parsing, environment variable expansion, and pipeline spec types.</li>
<li><code>crates/sources</code> : database CDC readers (MySQL binlog, Postgres logical replication) implemented as pluggable sources.</li>
<li><code>crates/processors</code> : JavaScript-based processors and support code for transforming batches.</li>
<li><code>crates/sinks</code> : sink implementations (Kafka producer, Redis streams, NATS JetStream) plus sink utilities.</li>
<li><code>crates/rest-api</code> : HTTP control plane with health/readiness and pipeline lifecycle endpoints.</li>
<li><code>crates/runner</code> : CLI entrypoint that wires the runtime, metrics, and control plane together.</li>
</ul>
<p>Use these crate boundaries as reference points when adding new sources, sinks, or pipeline behaviors.</p>
<h2 id="start-dev-dependencies"><a class="header" href="#start-dev-dependencies">Start dev dependencies</a></h2>
<p>Bring up the optional backing services (MySQL, Kafka, Redis) with Docker Compose:</p>
<pre><code class="language-bash">docker compose -f docker-compose.dev.yml up -d
</code></pre>
<p>Each service is exposed on localhost for local runs (<code>5432</code>, <code>3306</code>, <code>9092</code>, <code>6379</code>). The MySQL container seeds demo data from <code>./init-scripts</code> and configures binlog settings required for CDC.</p>
<p>Prefer the convenience <code>dev.sh</code> wrapper to keep common tasks consistent:</p>
<pre><code class="language-bash">./dev.sh up     # start the dependency stack
./dev.sh down   # stop and remove it
./dev.sh ps     # see container status
</code></pre>
<h2 id="build-and-test-locally"><a class="header" href="#build-and-test-locally">Build and test locally</a></h2>
<p>Run the usual Rust workflow from the repo root:</p>
<pre><code class="language-bash">cargo fmt --all
cargo clippy --workspace --all-targets --all-features
cargo test --workspace
</code></pre>
<p>Or use the helper script for a single command that mirrors CI expectations:</p>
<pre><code class="language-bash">./dev.sh build         # build project (debug)
./dev.sh build-release # build project (release)
./dev.sh run           # run with examples/dev.yaml
./dev.sh fmt           # format code
./dev.sh lint          # clippy with warnings as errors
./dev.sh test          # full test suite
./dev.sh check         # fmt --check + clippy + tests (mirrors CI)
./dev.sh cov           # generate coverage report
</code></pre>
<h2 id="docker-images"><a class="header" href="#docker-images">Docker images</a></h2>
<h3 id="use-pre-built-images"><a class="header" href="#use-pre-built-images">Use pre-built images</a></h3>
<p>Multi-arch images (amd64/arm64) are published to GHCR and Docker Hub:</p>
<pre><code class="language-bash"># Minimal (~57MB, scratch-based, no shell)
docker pull ghcr.io/vnvo/deltaforge:latest
docker pull vnvohub/deltaforge:latest

# Debug (~140MB, includes shell for troubleshooting)
docker pull ghcr.io/vnvo/deltaforge:latest-debug
docker pull vnvohub/deltaforge:latest-debug
</code></pre>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Variant</th><th>Size</th><th>Base</th><th>Use case</th></tr>
</thead>
<tbody>
<tr><td><code>latest</code></td><td>~57MB</td><td>scratch</td><td>Production</td></tr>
<tr><td><code>latest-debug</code></td><td>~140MB</td><td>debian-slim</td><td>Troubleshooting, has shell</td></tr>
</tbody>
</table>
</div>
<h3 id="build-locally"><a class="header" href="#build-locally">Build locally</a></h3>
<p>Two Dockerfiles are provided:</p>
<pre><code class="language-bash"># Minimal image (~57MB)
docker build -t deltaforge:local .

# Debug image (~140MB, includes shell)
docker build -t deltaforge:local-debug -f Dockerfile.debug .
</code></pre>
<p>Or use the dev helper:</p>
<pre><code class="language-bash">./dev.sh docker            # build minimal image
./dev.sh docker-debug      # build debug image
./dev.sh docker-test       # test minimal image runs
./dev.sh docker-test-debug # test debug image runs
./dev.sh docker-all        # build and test all variants
./dev.sh docker-shell      # open shell in debug container
</code></pre>
<h3 id="build-multi-arch-locally"><a class="header" href="#build-multi-arch-locally">Build multi-arch locally</a></h3>
<p>To build for both amd64 and arm64:</p>
<pre><code class="language-bash">./dev.sh docker-multi-setup  # create buildx builder (once)
./dev.sh docker-multi        # build both architectures
</code></pre>
<p>Note: Multi-arch builds use QEMU emulation and take ~30-35 minutes. The images are not loaded locally - use <code>--push</code> to push to a registry.</p>
<h3 id="run-the-image"><a class="header" href="#run-the-image">Run the image</a></h3>
<p>Run the container by mounting your pipeline specs and exposing the API and metrics ports:</p>
<pre><code class="language-bash">docker run --rm \
  -p 8080:8080 -p 9000:9000 \
  -v $(pwd)/examples/dev.yaml:/etc/deltaforge/pipeline.yaml:ro \
  -v deltaforge-checkpoints:/app/data \
  ghcr.io/vnvo/deltaforge:latest \
  --config /etc/deltaforge/pipeline.yaml
</code></pre>
<p>Notes:</p>
<ul>
<li>The container listens on <code>0.0.0.0:8080</code> for the control plane API with metrics on <code>:9000</code>.</li>
<li>Checkpoints are written to <code>/app/data/df_checkpoints.json</code>; mount a volume to persist them across restarts.</li>
<li>Environment variables inside the YAML are expanded before parsing.</li>
<li>Pass any other runner flags as needed (e.g., <code>--api-addr</code> or <code>--metrics-addr</code>).</li>
</ul>
<h3 id="debug-a-running-container"><a class="header" href="#debug-a-running-container">Debug a running container</a></h3>
<p>Use the debug image to troubleshoot:</p>
<pre><code class="language-bash"># Run with shell access
docker run --rm -it --entrypoint /bin/bash ghcr.io/vnvo/deltaforge:latest-debug

# Exec into a running container
docker exec -it &lt;container_id&gt; /bin/bash
</code></pre>
<h2 id="dev-helper-commands"><a class="header" href="#dev-helper-commands">Dev helper commands</a></h2>
<p>The <code>dev.sh</code> script provides shortcuts for common tasks:</p>
<pre><code class="language-bash">./dev.sh help  # show all commands
</code></pre>
<h3 id="infrastructure"><a class="header" href="#infrastructure">Infrastructure</a></h3>
<pre><code class="language-bash">./dev.sh up           # start MySQL, Kafka, Redis
./dev.sh down         # stop and remove containers
./dev.sh ps           # list running services
</code></pre>
<h3 id="kafka-2"><a class="header" href="#kafka-2">Kafka</a></h3>
<pre><code class="language-bash">./dev.sh k-list                         # list topics
./dev.sh k-create &lt;topic&gt;               # create topic
./dev.sh k-consume &lt;topic&gt; --from-beginning
./dev.sh k-produce &lt;topic&gt;              # interactive producer
</code></pre>
<h3 id="redis-2"><a class="header" href="#redis-2">Redis</a></h3>
<pre><code class="language-bash">./dev.sh redis-cli                      # open redis-cli
./dev.sh redis-read &lt;stream&gt;            # read from stream
</code></pre>
<h3 id="database-shells"><a class="header" href="#database-shells">Database shells</a></h3>
<pre><code class="language-bash">./dev.sh pg-sh       # psql into Postgres
./dev.sh mysql-sh    # mysql into MySQL
</code></pre>
<h3 id="documentation"><a class="header" href="#documentation">Documentation</a></h3>
<pre><code class="language-bash">./dev.sh docs        # serve docs locally (opens browser)
./dev.sh docs-build  # build docs
</code></pre>
<h3 id="pre-release-checks"><a class="header" href="#pre-release-checks">Pre-release checks</a></h3>
<pre><code class="language-bash">./dev.sh release-check  # run all checks + build all Docker variants
</code></pre>
<h2 id="contributing"><a class="header" href="#contributing">Contributing</a></h2>
<ol>
<li>Fork the repository</li>
<li>Create a branch from <code>main</code> (e.g., <code>feature/new-sink</code>, <code>fix/checkpoint-bug</code>)</li>
<li>Make your changes</li>
<li>Run <code>./dev.sh check</code> to ensure CI will pass</li>
<li>Submit a PR against <code>main</code></li>
</ol>
<h2 id="things-to-remember"><a class="header" href="#things-to-remember">Things to Remember</a></h2>
<h3 id="tests"><a class="header" href="#tests">Tests</a></h3>
<p>There a few <code>#[ignore]</code> tests, run them when making deep changes to the sources, pipeline coordination and anything with impact on core functionality.</p>
<h3 id="logging-hygiene"><a class="header" href="#logging-hygiene">Logging hygiene</a></h3>
<ul>
<li>Include <code>pipeline</code>, <code>tenant</code>, <code>source_id</code>/<code>sink_id</code>, and <code>batch_id</code> fields on all warnings/errors to make traces joinable in log aggregation tools.</li>
<li>Normalize retry/backoff logs so they include the attempt count and sleep duration; consider a structured <code>reason</code> field alongside error details for dashboards.</li>
<li>Add info-level summaries on interval (e.g., every N batches) reporting batches processed, average batch size, lag, and sink latency percentiles pulled from the metrics registry to create human-friendly breadcrumbs.</li>
<li>Add metrics in a backward-compatible way: prefer new metric names over redefining existing ones to avoid breaking dashboards. Validate cardinality (bounded label sets) before merging.</li>
<li>Gate noisy logs behind levels (<code>debug</code> for per-event traces, <code>info</code> for batch summaries, <code>warn</code>/<code>error</code> for retries and failures).</li>
<li>Exercise the new metrics in integration tests by asserting counters change when sending synthetic events through pipelines.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="roadmap"><a class="header" href="#roadmap">Roadmap</a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr-ef4e11c1.min.js"></script>
        <script src="mark-09e88c2c.min.js"></script>
        <script src="searcher-c2a407aa.js"></script>

        <script src="clipboard-1626706a.min.js"></script>
        <script src="highlight-abc7f01d.js"></script>
        <script src="book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
