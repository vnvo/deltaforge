<!DOCTYPE HTML>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>DeltaForge User Guide</title>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="Change Data Capture pipelines with DeltaForge: configuration, runtime behavior, and deployment tips.">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <base href="">

        <link rel="stylesheet" href="book.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <link rel="shortcut icon" href="favicon.png">

        <!-- Font Awesome -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme -->
        

        

        <!-- Fetch Clipboard.js from CDN but have a local fallback -->
        <script src="https://cdn.jsdelivr.net/clipboard.js/1.6.1/clipboard.min.js"></script>
        <script>
            if (typeof Clipboard == 'undefined') {
                document.write(unescape("%3Cscript src='clipboard.min.js'%3E%3C/script%3E"));
            }
        </script>

        <!-- Fetch JQuery from CDN but have a local fallback -->
        <script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
        <script>
            if (typeof jQuery == 'undefined') {
                document.write(unescape("%3Cscript src='jquery.js'%3E%3C/script%3E"));
            }
        </script>

        <!-- Fetch store.js from local - TODO add CDN when 2.x.x is available on cdnjs -->
        <script src="store.js"></script>

    </head>
    <body class="light">
        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme = store.get('mdbook-theme');
            if (theme === null || theme === undefined) { theme = 'light'; }
            $('body').removeClass().addClass(theme);
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var sidebar = store.get('mdbook-sidebar');
            if (sidebar === "hidden") { $("html").addClass("sidebar-hidden") }
            else if (sidebar === "visible") { $("html").addClass("sidebar-visible") }
        </script>

        <div id="sidebar" class="sidebar">
            <ul class="chapter"><li><a href="introduction.html"><strong>1.</strong> Introduction</a></li><li><a href="cdc.html"><strong>2.</strong> Change Data Capture</a></li><li><a href="quickstart.html"><strong>3.</strong> Quickstart</a></li><li><a href="configuration.html"><strong>4.</strong> Configuration</a></li><li><a href="pipelines.html"><strong>5.</strong> Pipelines</a></li><li><a href="sources/README.html"><strong>6.</strong> Sources</a></li><li><ul class="section"><li><a href="sources/mysql.html"><strong>6.1.</strong> MySQL</a></li></ul></li><li><a href="sinks/README.html"><strong>7.</strong> Sinks</a></li><li><ul class="section"><li><a href="sinks/redis.html"><strong>7.1.</strong> Redis</a></li><li><a href="sinks/kafka.html"><strong>7.2.</strong> Kafka</a></li></ul></li><li><a href="checkpoints.html"><strong>8.</strong> Checkpoints</a></li><li><a href="examples/README.html"><strong>9.</strong> Examples</a></li><li><ul class="section"><li><a href="examples/mysql_to_redis.html"><strong>9.1.</strong> MySQL to Redis</a></li></ul></li><li><a href="troubleshooting.html"><strong>10.</strong> Troubleshooting</a></li><li><a href="roadmap.html"><strong>11.</strong> Roadmap</a></li></ul>
        </div>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page" tabindex="-1">
                
                <div id="menu-bar" class="menu-bar">
                    <div class="left-buttons">
                        <i id="sidebar-toggle" class="fa fa-bars" title="Toggle sidebar"></i>
                        <i id="theme-toggle" class="fa fa-paint-brush" title="Change theme"></i>
                    </div>

                    <h1 class="menu-title">DeltaForge User Guide</h1>

                    <div class="right-buttons">
                        <a href="print.html">
                            <i id="print-button" class="fa fa-print" title="Print this book"></i>
                        </a>
                    </div>
                </div>

                <div id="content" class="content">
                    <a class="header" href="print.html#introduction" id="introduction"><h1>Introduction</h1></a>
<p>DeltaForge is a modular, config-driven Change Data Capture (CDC) micro-framework. It streams database changes into downstream systems like Kafka and Redis while giving users control over how each event is processed. Pipelines are defined declaratively in YAML, making it straightforward to onboard new use cases without custom code.</p>
<ul>
<li>‚ö° <strong>Powered by Rust</strong>: for predictable performance, security and reliable operations.</li>
<li>üîå <strong>Pluggable sources &amp; sinks</strong>: with first-class MySQL binlog CDC today and more planned for upcoming interations.</li>
<li>üß© <strong>Config-driven pipelines</strong>: specs capture sources, processors, sinks, batching, and commit policy.</li>
<li>üì¶ <strong>Built-in checkpointing and batching</strong>: to keep sinks consistent.</li>
<li>üõ†Ô∏è <strong>Operational guardrails</strong>: structured logging, metrics, and health endpoints.</li>
</ul>
<p>DeltaForge is designed to enable:</p>
<ul>
<li>Efficient and Fast incremental data movement for modern event-driven architectures</li>
<li>Compact and Inline ETL, making your data processing stack smaller and more cost effective.</li>
</ul>
<hr />
<p><strong>DeltaForge is <em>not</em> a DAG-based stream processor; it is a focused CDC engine meant to replace tools like Debezium when you need a lighter, cloud-friendly and more customizable runtime.</strong></p>
<hr />
<p>This documentation will walk you through installation, configuration, and usage.</p>
<a class="header" href="print.html#change-data-capture-cdc" id="change-data-capture-cdc"><h1>Change Data Capture (CDC)</h1></a>
<p>Change Data Capture (CDC) is the practice of streaming database mutations as ordered events so downstream systems can stay in sync without periodic full loads. DeltaForge focuses on <strong>row-level</strong> CDC from a data source to keep consumers accurate and latency-aware.</p>
<a class="header" href="print.html#why-cdc-matters" id="why-cdc-matters"><h2>Why CDC matters</h2></a>
<ul>
<li>‚è±Ô∏è <strong>Low latency propagation</strong>: pushes inserts, updates, and deletes in near real time instead of relying on slow batch ETL windows.</li>
<li>üìâ <strong>Reduced load on primaries</strong>: avoids repeated full-table scans by tailing the transaction log.</li>
<li>üßÆ <strong>Deterministic replay</strong>: ordered events allow consumers to reconstruct state or power exactly-once sinks with checkpointing.</li>
<li>üîÑ <strong>Polyglot delivery</strong>: the same change feed can serve caches, queues, warehouses, and search indexes.</li>
</ul>
<a class="header" href="print.html#how-cdc-works-in-practice" id="how-cdc-works-in-practice"><h2>How CDC works in practice</h2></a>
<ol>
<li>ü™™ <strong>Identify change boundaries</strong>: capture table, primary key, and operation type (insert/update/delete) for each row-level event.</li>
<li>üßæ <strong>Read from the transaction log</strong>: follow the data source (or source of truth) to ensure changes reflect committed transactions.</li>
<li>üß∞ <strong>Transform and route</strong>: enrich or filter events before delivering to sinks like Redis or Kafka.</li>
<li>üõ°Ô∏è <strong>Guard with checkpoints</strong>: store offsets so pipelines can resume safely after restarts.</li>
</ol>
<a class="header" href="print.html#when-to-use-cdc" id="when-to-use-cdc"><h2>When to use CDC</h2></a>
<p>Choose CDC when you need:</p>
<ul>
<li>üî• <strong>Real-time cache population</strong> to keep Redis aligned with MySQL writes.</li>
<li>üß† <strong>Event-driven integrations</strong> that react to specific table mutations.</li>
<li>üóÉÔ∏è <strong>Incremental warehouse loads</strong> without locking source tables.</li>
<li>üîÄ <strong>Fan-out to multiple sinks</strong> while preserving ordering and idempotency.</li>
</ul>
<a class="header" href="print.html#cdc-and-deltaforge" id="cdc-and-deltaforge"><h2>CDC and DeltaForge</h2></a>
<ul>
<li>üß≠ <strong>Config-driven flows</strong>: YAML specs declare which tables to watch, how to shape payloads, and where to send them.</li>
<li>üß± <strong>Operational safety</strong>: checkpoints, backpressure-aware batching, and observability reduce drift and replay risk.</li>
<li>üß© <strong>Composable processors</strong>: filters, mappers, and sink-specific serializers keep business logic close to the data stream.</li>
</ul>
<a class="header" href="print.html#considerations" id="considerations"><h2>Considerations</h2></a>
<ul>
<li>‚öñÔ∏è <strong>Schema evolution</strong>: plan for column additions or renames; prefer backward-compatible changes to avoid consumer breakage.</li>
<li>üß≤ <strong>Ordering and idempotency</strong>: downstream systems should handle retries and ensure only-once effects when possible.</li>
<li>üßÆ <strong>Data minimization</strong>: emit only the columns consumers need to reduce bandwidth and exposure of sensitive fields.</li>
<li>üìä <strong>Monitoring</strong>: track lag between source position and sink commits to detect regressions early.</li>
</ul>
<a class="header" href="print.html#quickstart" id="quickstart"><h1>Quickstart</h1></a>
<p>Follow these steps to run DeltaForge with a sample pipeline definition.</p>
<a class="header" href="print.html#1-prepare-a-pipeline-spec" id="1-prepare-a-pipeline-spec"><h2>1. Prepare a pipeline spec</h2></a>
<p>Create a YAML file that matches the <code>PipelineSpec</code> schema. Environment variables inside the YAML are expanded before parsing, so secrets and hostnames can be injected at runtime.</p>
<ul>
<li>üß© <strong>Config-first</strong>: describe the source, processors, and sinks without code changes.</li>
</ul>
<pre><code class="language-yaml">metadata:
  name: orders-mysql-to-kafka
  tenant: acme
spec:
  source:
    type: mysql
    config:
      id: orders-mysql
      dsn: ${MYSQL_DSN}
      tables:
        - shop.orders
  processors:
    - type: javascript
      id: transform
      inline: |
        function process(batch) {
          return batch;
        }
  sinks:
    - type: kafka
      config:
        id: orders-kafka
        brokers: ${KAFKA_BROKERS}
        topic: orders
        required: true
  batch:
    max_events: 500
    max_bytes: 1048576
    max_ms: 1000
</code></pre>
<a class="header" href="print.html#2-start-the-runner" id="2-start-the-runner"><h2>2. Start the runner</h2></a>
<p>Point the runner at a file or directory containing one or more specs. Directories are walked recursively.</p>
<pre><code class="language-bash">cargo run -p runner -- --config ./pipelines
</code></pre>
<p>Optional flags:</p>
<ul>
<li><code>--api-addr 0.0.0.0:8080</code> ‚Äî REST control plane for pipeline lifecycle operations.</li>
<li><code>--metrics-addr 0.0.0.0:9095</code> ‚Äî Prometheus metrics endpoint.</li>
</ul>
<a class="header" href="print.html#3-inspect-health-and-readiness" id="3-inspect-health-and-readiness"><h2>3. Inspect health and readiness</h2></a>
<p>Use the REST endpoints to verify the runtime is healthy and discover pipeline states:</p>
<ul>
<li><code>GET /healthz</code> ‚Äî liveness probe.</li>
<li><code>GET /readyz</code> ‚Äî readiness with pipeline status and specs.</li>
<li><code>GET /pipelines</code> ‚Äî list all pipelines with their live configuration.</li>
</ul>
<a class="header" href="print.html#4-iterate-safely" id="4-iterate-safely"><h2>4. Iterate safely</h2></a>
<p>Pause, resume, or stop pipelines through the control plane while tweaking batch thresholds, sink requirements, or processor code. DeltaForge will restart pipelines automatically when specs change.</p>
<a class="header" href="print.html#configuration" id="configuration"><h1>Configuration</h1></a>
<p>DeltaForge pipelines are defined as YAML documents that map directly to the <code>PipelineSpec</code> schema. Environment variables inside the YAML are expanded before parsing, so you can keep secrets like DSNs or broker lists outside of version control. You can also inline values directly‚Äîplaceholders are optional‚Äîbut <code>${VAR_NAME}</code> entries make it easy to switch between local dev secrets and production credentials without editing the files.</p>
<ul>
<li>üîë <strong>Flexible substitution</strong>: environment variables extend placeholders; hardcoded values remain supported when you prefer explicit config.</li>
</ul>
<a class="header" href="print.html#loading-configuration" id="loading-configuration"><h2>Loading configuration</h2></a>
<ul>
<li>Pass a path with <code>--config &lt;path&gt;</code> when starting the runner.</li>
<li>The path may be a single file or a directory. Directories are walked recursively, and every file is parsed as a pipeline spec.</li>
<li>Multiple specs can be loaded at once; each spawns its own pipeline.</li>
</ul>
<pre><code class="language-bash">cargo run -p runner -- --config ./pipelines \
  --api-addr 0.0.0.0:8080 \
  --metrics-addr 0.0.0.0:9095
</code></pre>
<a class="header" href="print.html#pipelinespec-structure" id="pipelinespec-structure"><h2>PipelineSpec structure</h2></a>
<a class="header" href="print.html#metadata" id="metadata"><h3>metadata</h3></a>
<table><thead><tr><th> field </th><th> type </th><th> description </th></tr></thead><tbody>
<tr><td> <code>name</code> </td><td> string (required) </td><td> Unique pipeline identifier used in API routes and metrics labels. </td></tr>
<tr><td> <code>tenant</code> </td><td> string (required) </td><td> Business tenant or ownership label. </td></tr>
</tbody></table>
<a class="header" href="print.html#spec" id="spec"><h3>spec</h3></a>
<table><thead><tr><th> field </th><th> type </th><th> description </th></tr></thead><tbody>
<tr><td> <code>sharding</code> </td><td> object (optional) </td><td> Downstream partitioning hint with <code>mode</code>, optional <code>count</code>, and optional <code>key</code>. </td></tr>
<tr><td> <code>source</code> </td><td> object (required) </td><td> CDC source definition. See <a href="sources/README.md">Sources</a>. </td></tr>
<tr><td> <code>processors</code> </td><td> array (required, can be empty) </td><td> Ordered processors executed on each batch. See <a href="pipelines.md#processors">Processors</a>. </td></tr>
<tr><td> <code>sinks</code> </td><td> array (required, at least one) </td><td> One or more sinks that receive each batch. See <a href="sinks/README.md">Sinks</a>. </td></tr>
<tr><td> <code>connection_policy</code> </td><td> object (optional) </td><td> How the runtime establishes upstream connections (default mode, preferred replicas, and limits). </td></tr>
<tr><td> <code>batch</code> </td><td> object (optional) </td><td> Commit unit thresholds. See <a href="pipelines.md#batching">Batching</a>. </td></tr>
<tr><td> <code>commit_policy</code> </td><td> object (optional) </td><td> How sink acknowledgements gate checkpoints. See <a href="pipelines.md#commit-policy">Commit policy</a>. </td></tr>
</tbody></table>
<a class="header" href="print.html#example" id="example"><h3>Example</h3></a>
<pre><code class="language-yaml">metadata:
  name: orders-mysql-to-kafka
  tenant: acme
spec:
  sharding:
    mode: hash
    count: 4
    key: customer_id
  source:
    type: mysql
    config:
      id: orders-mysql
      dsn: ${MYSQL_DSN}
      tables:
        - shop.orders
  processors:
    - type: javascript
      id: transform
      inline: |
        function process(batch) {
          return batch;
        }
      limits:
        cpu_ms: 50
        mem_mb: 128
        timeout_ms: 500
  sinks:
    - type: kafka
      config:
        id: orders-kafka
        brokers: ${KAFKA_BROKERS}
        topic: orders
        required: true
        exactly_once: false
        client_conf:
          message.timeout.ms: &quot;5000&quot;
    - type: redis
      config:
        id: orders-redis
        uri: ${REDIS_URI}
        stream: orders
  batch:
    max_events: 500
    max_bytes: 1048576
    max_ms: 1000
    respect_source_tx: true
    max_inflight: 2
  commit_policy:
    mode: quorum
    quorum: 2
</code></pre>
<a class="header" href="print.html#pipelines" id="pipelines"><h1>Pipelines</h1></a>
<p>Each pipeline is created from a single <code>PipelineSpec</code>. The runtime spawns the source, processors, and sinks defined in the spec and coordinates them with batching and checkpointing.</p>
<ul>
<li>üîÑ <strong>Live control</strong>: pause, resume, or stop pipelines through the REST API without redeploying.</li>
<li>üì¶ <strong>Coordinated delivery</strong>: batching and commit policy keep sinks consistent even when multiple outputs are configured.</li>
</ul>
<a class="header" href="print.html#lifecycle-controls" id="lifecycle-controls"><h2>Lifecycle controls</h2></a>
<p>The REST API addresses pipelines by <code>metadata.name</code> and returns <code>PipeInfo</code> records containing the live spec and status.</p>
<ul>
<li><code>GET /healthz</code> ‚Äî liveness probe.</li>
<li><code>GET /readyz</code> ‚Äî readiness with pipeline states.</li>
<li><code>GET /pipelines</code> ‚Äî list pipelines.</li>
<li><code>POST /pipelines</code> ‚Äî create from a full spec.</li>
<li><code>PATCH /pipelines/{name}</code> ‚Äî merge a partial spec (for example, adjust batch thresholds) and restart the pipeline.</li>
<li><code>POST /pipelines/{name}/pause</code> ‚Äî pause ingestion and coordination.</li>
<li><code>POST /pipelines/{name}/resume</code> ‚Äî resume a paused pipeline.</li>
<li><code>POST /pipelines/{name}/stop</code> ‚Äî stop a running pipeline.</li>
</ul>
<p>Pausing halts both source ingestion and the coordinator. Resuming re-enables both ends so buffered events can drain cleanly.</p>
<a class="header" href="print.html#processors" id="processors"><h2>Processors</h2></a>
<p>Processors run in the declared order for each batch. The built-in processor type is JavaScript, powered by <code>deno_core</code>.</p>
<ul>
<li><code>type: javascript</code>
<ul>
<li><code>id</code>: processor label.</li>
<li><code>inline</code>: JS source. Export a <code>process(batch)</code> function that returns the transformed batch.</li>
<li><code>limits</code> (optional): resource guardrails (<code>cpu_ms</code>, <code>mem_mb</code>, <code>timeout_ms</code>).</li>
</ul>
</li>
</ul>
<a class="header" href="print.html#batching" id="batching"><h2>Batching</h2></a>
<p>The coordinator builds batches using soft thresholds:</p>
<ul>
<li><code>max_events</code>: flush after this many events.</li>
<li><code>max_bytes</code>: flush after the serialized size reaches this limit.</li>
<li><code>max_ms</code>: flush after this much time has elapsed since the batch started.</li>
<li><code>respect_source_tx</code>: when true, never split a single source transaction across batches.</li>
<li><code>max_inflight</code>: cap the number of batches being processed concurrently.</li>
</ul>
<a class="header" href="print.html#commit-policy" id="commit-policy"><h2>Commit policy</h2></a>
<p>When multiple sinks are configured, checkpoints can wait for different acknowledgement rules:</p>
<ul>
<li><code>all</code>: every sink must acknowledge.</li>
<li><code>required</code> (default): only sinks marked <code>required: true</code> must acknowledge; others are best-effort.</li>
<li><code>quorum</code>: checkpoint after at least <code>quorum</code> sinks acknowledge.</li>
</ul>
<a class="header" href="print.html#sources" id="sources"><h1>Sources</h1></a>
<p>DeltaForge's only fully supported CDC source today is MySQL binlog ingestion. Each source is configured under <code>spec.source</code> with a <code>type</code> field and a <code>config</code> object. Environment variables are expanded before parsing, so DSNs can be supplied at runtime.</p>
<p>Current built-in source type:</p>
<ul>
<li><a href="mysql.md"><code>mysql</code></a> ‚Äî MySQL binlog CDC.</li>
</ul>
<p>The source interface is pluggable; if you experiment with engines like Postgres logical replication, treat them as variations until they ship with formal support and docs.</p>
<p>All sources honor the pipeline's batching and commit policy once events enter the coordinator.</p>
<a class="header" href="print.html#mysql-source" id="mysql-source"><h1>MySQL source</h1></a>
<p>DeltaForge tails the MySQL binlog to capture row-level changes.</p>
<ul>
<li>üõ°Ô∏è <strong>Replication-safe</strong>: built for MySQL first, using binlog subscriptions with table filters.</li>
</ul>
<a class="header" href="print.html#configuration-1" id="configuration-1"><h2>Configuration</h2></a>
<p>Set <code>spec.source.type</code> to <code>mysql</code> and provide a config object:</p>
<ul>
<li><code>id</code> (string): logical identifier for metrics and logging.</li>
<li><code>dsn</code> (string): MySQL connection string with replication privileges.</li>
<li><code>tables</code> (array<string>): fully qualified tables to subscribe to.</li>
</ul>
<a class="header" href="print.html#example-1" id="example-1"><h3>Example</h3></a>
<pre><code class="language-yaml">source:
  type: mysql
  config:
    id: orders-mysql
    dsn: ${MYSQL_DSN}
    tables:
      - shop.orders
      - shop.order_items
</code></pre>
<a class="header" href="print.html#notes" id="notes"><h2>Notes</h2></a>
<ul>
<li>Table filters are applied before events are handed to processors and sinks.</li>
<li>Batches respect <code>respect_source_tx</code> to avoid splitting a single binlog transaction across multiple commits.</li>
</ul>
<a class="header" href="print.html#sinks" id="sinks"><h1>Sinks</h1></a>
<p>Sinks receive batches from the coordinator after processors run. Each sink lives under <code>spec.sinks</code> and can be marked as required or best-effort via the <code>required</code> flag. Checkpoint behavior is governed by the pipeline's commit policy.</p>
<p>Current built-in sinks:</p>
<ul>
<li><a href="redis.md"><code>redis</code></a> ‚Äî Redis stream sink.</li>
<li><a href="kafka.md"><code>kafka</code></a> ‚Äî Kafka producer sink.</li>
</ul>
<p>You can combine multiple sinks in one pipeline to fan out events.</p>
<a class="header" href="print.html#redis-sink" id="redis-sink"><h1>Redis sink</h1></a>
<p>The Redis sink writes each batch as entries in a Redis stream.</p>
<a class="header" href="print.html#configuration-2" id="configuration-2"><h2>Configuration</h2></a>
<ul>
<li><code>id</code> (string): logical identifier for metrics and logging.</li>
<li><code>uri</code> (string): Redis connection URI.</li>
<li><code>stream</code> (string): Redis stream key to append events to.</li>
<li><code>required</code> (bool, default <code>true</code>): whether acknowledgements from this sink gate checkpoints.</li>
</ul>
<a class="header" href="print.html#example-2" id="example-2"><h3>Example</h3></a>
<pre><code class="language-yaml">sinks:
  - type: redis
    config:
      id: orders-redis
      uri: ${REDIS_URI}
      stream: orders
      required: true
</code></pre>
<a class="header" href="print.html#notes-1" id="notes-1"><h2>Notes</h2></a>
<ul>
<li>Redis streams preserve event order per pipeline and are well-suited for lightweight consumers.</li>
<li>Combine with other sinks (for example, Kafka) by marking only the critical ones as <code>required</code> if you want checkpoints to proceed without waiting for every sink.</li>
</ul>
<a class="header" href="print.html#kafka-sink" id="kafka-sink"><h1>Kafka sink</h1></a>
<p>The Kafka sink publishes batches to a Kafka topic using <code>rdkafka</code>.</p>
<a class="header" href="print.html#configuration-3" id="configuration-3"><h2>Configuration</h2></a>
<ul>
<li><code>id</code> (string): logical identifier for metrics and logging.</li>
<li><code>brokers</code> (string): comma-separated broker list.</li>
<li><code>topic</code> (string): destination topic.</li>
<li><code>required</code> (bool, default <code>true</code>): whether acknowledgements from this sink gate checkpoints.</li>
<li><code>exactly_once</code> (bool, default <code>false</code>): enable EOS semantics when supported by the cluster.</li>
<li><code>client_conf</code> (map, optional): raw <code>librdkafka</code> configuration overrides.</li>
</ul>
<a class="header" href="print.html#example-3" id="example-3"><h3>Example</h3></a>
<pre><code class="language-yaml">sinks:
  - type: kafka
    config:
      id: orders-kafka
      brokers: ${KAFKA_BROKERS}
      topic: orders
      required: true
      exactly_once: false
      client_conf:
        message.timeout.ms: &quot;5000&quot;
</code></pre>
<a class="header" href="print.html#notes-2" id="notes-2"><h2>Notes</h2></a>
<ul>
<li>Combine Kafka with other sinks to fan out data. Use the commit policy to decide whether non-critical sinks should block checkpoints.</li>
<li>Adjust <code>client_conf</code> for durability (for example, <code>acks=all</code>) or performance based on your cluster.</li>
</ul>
<a class="header" href="print.html#checkpoints" id="checkpoints"><h1>Checkpoints</h1></a>
<p>Checkpoints record the progress of each pipeline so ingestion can resume from the last processed point.</p>
<ul>
<li>
<p>üõ∞Ô∏è <strong>Restart/crash-friendly</strong>: checkpoints anchor exactly where each pipeline left off.</p>
</li>
<li>
<p>By default, checkpoints are persisted to <code>./data/df_checkpoints.json</code>.</p>
</li>
<li>
<p>Checkpoint commits occur after sinks acknowledge a batch according to the pipeline's commit policy.</p>
</li>
<li>
<p>Required sinks must succeed before a checkpoint is written; optional sinks are best-effort unless <code>mode: all</code> is configured.</p>
</li>
</ul>
<p>NB: the plan is to add more production grade storage backends for the checkpoints.</p>
<p>Use the REST API to pause or stop pipelines before maintenance to ensure checkpoints are flushed cleanly.</p>
<a class="header" href="print.html#examples" id="examples"><h1>Examples</h1></a>
<a class="header" href="print.html#mysql-to-redis" id="mysql-to-redis"><h1>MySQL to Redis</h1></a>
<p>This example streams MySQL binlog events into a Redis stream with an inline JavaScript transformation.</p>
<pre><code class="language-yaml">metadata:
  name: orders-mysql-to-redis
  tenant: acme
spec:
  source:
    type: mysql
    config:
      id: orders-mysql
      dsn: ${MYSQL_DSN}
      tables:
        - shop.orders
  processors:
    - type: javascript
      id: redact-email
      inline: |
        function process(batch) {
          return batch.map((event) =&gt; {
            if (event.after &amp;&amp; event.after.email) {
              event.after.email = &quot;[redacted]&quot;;
            }
            return event;
          });
        }
      limits:
        timeout_ms: 500
  sinks:
    - type: redis
      config:
        id: orders-redis
        uri: ${REDIS_URI}
        stream: orders
        required: true
  batch:
    max_events: 500
    max_bytes: 1048576
    max_ms: 1000
  commit_policy:
    mode: required
</code></pre>
<p>Feel free to add a Kafka sink alongside Redis. Mark only the critical sink as <code>required</code> if you want checkpoints to proceed when optional sinks are unavailable.</p>
<a class="header" href="print.html#troubleshooting" id="troubleshooting"><h1>Troubleshooting</h1></a>
<p>Common issues and quick checks when running DeltaForge.</p>
<ul>
<li>ü©∫ <strong>Health-first</strong>: start with <code>/healthz</code> and <code>/readyz</code> to pinpoint failing components.</li>
</ul>
<a class="header" href="print.html#runner-fails-to-start" id="runner-fails-to-start"><h2>Runner fails to start</h2></a>
<ul>
<li>Confirm the config path passed to <code>--config</code> exists and is readable.</li>
<li>Validate YAML syntax and that required fields like <code>metadata.name</code> and <code>spec.source</code> are present.</li>
<li>Ensure environment variables referenced in the spec are set (<code>dsn</code>, <code>brokers</code>, <code>uri</code>, etc.).</li>
</ul>
<a class="header" href="print.html#pipelines-remain-unready" id="pipelines-remain-unready"><h2>Pipelines remain unready</h2></a>
<ul>
<li>Check the <code>/readyz</code> endpoint for per-pipeline status and error messages.</li>
<li>Verify upstream credentials allow replication (MySQL binlog). Other engines are experimental unless explicitly documented.</li>
<li>Inspect sink connectivity; a required sink that cannot connect will block checkpoints.</li>
</ul>
<a class="header" href="print.html#slow-throughput" id="slow-throughput"><h2>Slow throughput</h2></a>
<ul>
<li>Increase <code>batch.max_events</code> or <code>batch.max_bytes</code> to reduce flush frequency.</li>
<li>Adjust <code>max_inflight</code> to allow more concurrent batches if sinks can handle parallelism.</li>
<li>Reduce processor work or add guardrails (<code>limits</code>) to prevent slow JavaScript from stalling the pipeline.</li>
</ul>
<a class="header" href="print.html#checkpoints-not-advancing" id="checkpoints-not-advancing"><h2>Checkpoints not advancing</h2></a>
<ul>
<li>Review the commit policy: <code>mode: all</code> or <code>required</code> sinks that are unavailable will block progress.</li>
<li>Look for sink-specific errors (for example, Kafka broker unreachability or Redis backpressure).</li>
<li>Pause and resume the pipeline to force a clean restart after addressing the underlying issue.</li>
</ul>
<a class="header" href="print.html#roadmap" id="roadmap"><h1>Roadmap</h1></a>

                </div>

                <!-- Mobile navigation buttons -->
                

                

            </div>

            

            

        </div>


        <!-- Local fallback for Font Awesome -->
        <script>
            if ($(".fa").css("font-family") !== "FontAwesome") {
                $('<link rel="stylesheet" type="text/css" href="_FontAwesome/css/font-awesome.css">').prependTo('head');
            }
        </script>

        <!-- Livereload script (if served using the cli tool) -->
        
    <script type="text/javascript">
        var socket = new WebSocket("ws://localhost:3001");
        socket.onmessage = function (event) {
            if (event.data === "reload") {
                socket.close();
                location.reload(true); // force reload from server (not from cache)
            }
        };

        window.onbeforeunload = function() {
            socket.close();
        }
    </script>


        

        

        
        <script>
            $(document).ready(function() {
                window.print();
            })
        </script>
        

        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS script -->
        

    </body>
</html>
